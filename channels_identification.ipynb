{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the decline detection better\n",
    "\n",
    "Building on top of our findings in milestone 2, we find a technique to better detect declines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import tqdm\n",
    "import copy\n",
    "import csv\n",
    "\n",
    "from src.data.dataloader_functions import *\n",
    "from src.utils.results_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Run the shell script\n",
    "import subprocess\n",
    "\n",
    "# Path to the shell script\n",
    "script_path = \"./src/scripts/preprocessing_pipeline.sh\"\n",
    "\n",
    "# Run the shell script\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"bash\", script_path],  # Specify the shell script and interpreter\n",
    "        check=True,             # Raise an error if the command fails\n",
    "        text=True,              # Capture output as a string\n",
    "        capture_output=True     # Capture stdout and stderr\n",
    "    )\n",
    "    # Print the output\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error running the script:\")\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original data for the index\n",
    "original_data = load_processed_data(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-compute the rolling growth rate and store it to file\n",
    "\n",
    "/!\\ Note: Running the following cell is very time-consuming and requires around 16GB of memory. Run it only if you don't have the file `df_with_rgr.csv` in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLLING_WINDOW = 20  # Set the rolling window for the growth rate, (5 months by default, could be changed in the future)\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "result = pd.DataFrame()\n",
    "\n",
    "# Iterate over each group with a progress bar\n",
    "for name, group in tqdm(original_data.groupby('channel'), desc=\"Processing channels\"):\n",
    "    group['rolling_growth_rate'] = group['delta_subs'].rolling(ROLLING_WINDOW, min_periods=ROLLING_WINDOW).mean()\n",
    "    result = pd.concat([result, group])\n",
    "\n",
    "result['growth_diff'] = result['delta_subs'] - result['rolling_growth_rate']\n",
    "\n",
    "result.to_csv('data/df_with_rgr_new.tsv', sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data with the rolling growth rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with rolling growth rate\n",
    "df_with_rgr_new = pd.read_csv('data/df_with_rgr_new.tsv', sep='\\t')\n",
    "df_with_rgr_new.set_index(['channel', 'week'], inplace=True)\n",
    "\n",
    "# Detection of period where growth_rate < rolling_growth_rate\n",
    "df_with_rgr_new['decline_event_detected'] = df_with_rgr_new['growth_diff'] < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_rgr_grouped = df_with_rgr_new.reset_index().groupby('channel')\n",
    "\n",
    "print(f'Number of channels : {len(df_with_rgr_grouped.groups.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map that contains for each entry: the id of the channel, and the starting week and end week of each decline event\n",
    "decline_events = {}\n",
    "\n",
    "# For loop to detect the starting and ending week of the decline event\n",
    "for channel in tqdm(df_with_rgr_grouped.groups.keys(), desc=\"Processing channels\"):\n",
    "    channel_data = df_with_rgr_grouped.get_group(channel)\n",
    "\n",
    "    # Identify indices where decline has started and ended\n",
    "    for i in range(1, len(channel_data)):\n",
    "        if channel_data['decline_event_detected'].iloc[i] and not channel_data['decline_event_detected'].iloc[i-1]:\n",
    "            # Add the starting week of the decline event\n",
    "            if channel not in decline_events:\n",
    "                decline_events[channel] = []\n",
    "            if channel == 'UC-lHJZR3Gqxm24_Vd_AJ5Yw':\n",
    "                print(f'Channel {channel} has a decline event that starts.')\n",
    "            decline_events[channel].append((channel_data['week'].iloc[i], None))\n",
    "        if ((not channel_data['decline_event_detected'].iloc[i]) and channel_data['decline_event_detected'].iloc[i-1]):\n",
    "            # Add the ending week of the decline event\n",
    "            decline_events[channel][-1] = (decline_events[channel][-1][0], channel_data['week'].iloc[i])\n",
    "            if channel == 'UC-lHJZR3Gqxm24_Vd_AJ5Yw':\n",
    "                    print(f'Channel {channel} has a decline event that ends.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove only the events (tuples) with None as the second element\n",
    "decline_events_no_None = {k: [x for x in v if x[1] is not None] for k, v in decline_events.items()}\n",
    "\n",
    "# Add the duration of the decline event for each decline event for each channel (created from a deep copy of the original decline events)\n",
    "decline_events_with_duration = copy.deepcopy(decline_events_no_None)\n",
    "\n",
    "for channel in decline_events_with_duration:\n",
    "    for i in range(len(decline_events_with_duration[channel])):\n",
    "        decline_events_with_duration[channel][i] = ((decline_events_with_duration[channel][i][0], decline_events_with_duration[channel][i][1]), decline_events_with_duration[channel][i][1] - decline_events_with_duration[channel][i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the duration of the decline events (mean, median, min, max, plot)\n",
    "duration_list = []\n",
    "for channel in decline_events_with_duration:\n",
    "    for event in decline_events_with_duration[channel]:\n",
    "        duration_list.append(event[1])\n",
    "\n",
    "duration_list = np.array(duration_list)\n",
    "\n",
    "print(f'Mean duration of decline events: {np.mean(duration_list)}')\n",
    "print(f'Median duration of decline events: {np.median(duration_list)}')\n",
    "print(f'Min duration of decline events: {np.min(duration_list)}')\n",
    "print(f'Max duration of decline events: {np.max(duration_list)}')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(duration_list, bins=20, kde=True)\n",
    "plt.title('Distribution of the duration of decline events')\n",
    "plt.xlabel('Duration (weeks)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same plot with log scale to un-scew the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(duration_list, bins=20, kde=True)\n",
    "plt.title('Distribution of the duration of decline events')\n",
    "plt.xlabel('Duration (weeks)')\n",
    "plt.ylabel('Count')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the decline events that are shorter than the minimum duration\n",
    "\n",
    "DECLINE_MIN_DURATION = 8 # Set the minimum duration of a detected decline event to be considered as an effective decline event\n",
    "de_filtered_on_duration = {k: [x for x in v if x[1] >= DECLINE_MIN_DURATION] for k, v in decline_events_with_duration.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {sum([len(v) for v in de_filtered_on_duration.values()])} decline events with duration of at least {DECLINE_MIN_DURATION} weeks.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the duration of the decline events (mean, median, min, max, plot) after filtering out the decline events that are shorter than the minimum duration\n",
    "duration_list_filtered = []\n",
    "for channel in de_filtered_on_duration:\n",
    "    for event in de_filtered_on_duration[channel]:\n",
    "        duration_list_filtered.append(event[1])\n",
    "\n",
    "duration_list_filtered = np.array(duration_list_filtered)\n",
    "\n",
    "print(f'Mean duration of decline events: {np.mean(duration_list_filtered)}')\n",
    "print(f'Median duration of decline events: {np.median(duration_list_filtered)}')\n",
    "print(f'Min duration of decline events: {np.min(duration_list_filtered)}')\n",
    "print(f'Max duration of decline events: {np.max(duration_list_filtered)}')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(duration_list_filtered, bins=20, kde=True)\n",
    "plt.title('Distribution of the duration of decline events')\n",
    "plt.xlabel('Duration (weeks)')\n",
    "plt.ylabel('Count')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the growth diff percentage\n",
    "df_with_rgr_final = df_with_rgr_new\n",
    "df_with_rgr_final['growth_diff_percentage'] = (df_with_rgr_final['growth_diff'] / df_with_rgr_final['rolling_growth_rate']) * 100\n",
    "df_with_rgr_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_rgr_grouped_final = df_with_rgr_final.reset_index().groupby('channel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the decline events filtered out because of a too short duration, we check if the event should still be included in the list of decline events, \n",
    "# because it is too intense (i.e. the minimum growth difference is below a certain threshold)\n",
    "\n",
    "DECLINE_MIN_GROWTH_DIFF_PERCENTAGE = - 80 # Set the minimum magnitude of the growth difference to be considered as an effective decline event\n",
    "\n",
    "# For each channel, for each event:\n",
    "# We keep the event only if the miimum growth difference during the period starting from the first week of the event to the last week of the event is less than the threshold\n",
    "de_filtered_on_growth_diff = {}\n",
    "\n",
    "for channel in tqdm(df_with_rgr_grouped_final.groups.keys(), desc=\"Processing channels\"):\n",
    "    channel_data = df_with_rgr_grouped_final.get_group(channel)\n",
    "    for event in decline_events_with_duration.get(channel, []):\n",
    "        start_week = event[0][0]\n",
    "        end_week = event[0][1]\n",
    "        min_growth_diff = channel_data.loc[(channel_data['week'] >= start_week) & (channel_data['week'] <= end_week)]['growth_diff_percentage'].min()\n",
    "        if min_growth_diff < DECLINE_MIN_GROWTH_DIFF_PERCENTAGE:\n",
    "            if channel not in de_filtered_on_growth_diff:\n",
    "                de_filtered_on_growth_diff[channel] = []\n",
    "            de_filtered_on_growth_diff[channel].append(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the decline events filtered on growth difference, we remove the events that are shorter than 2 weeks to avoid outliers\n",
    "\n",
    "ANTI_OUTLIERS_MIN_DURATION = 8\n",
    "de_filtered_on_growth_diff_no_outliers = {k: [x for x in v if x[1] >= ANTI_OUTLIERS_MIN_DURATION] for k, v in de_filtered_on_growth_diff.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {sum([len(v) for v in de_filtered_on_growth_diff_no_outliers.values()])} decline events (based on growth rate) with duration of at least {ANTI_OUTLIERS_MIN_DURATION} weeks.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the decline events filtered on duration (de_filtered_on_duration) and growth difference (de_filtered_on_growth_diff_no_outliers) in one dictionary\n",
    "decline_events_final = {k: de_filtered_on_duration.get(k, []) + de_filtered_on_growth_diff_no_outliers.get(k, []) for k in set(de_filtered_on_duration) | set(de_filtered_on_growth_diff_no_outliers)}\n",
    "decline_events_final = {k: de_filtered_on_growth_diff_no_outliers.get(k, []) for k in set(de_filtered_on_growth_diff_no_outliers)}\n",
    "\n",
    "print(f'There are {sum([len(v) for v in decline_events_final.values()])} decline events in total.')\n",
    "\n",
    "# Sort the decline events by starting week\n",
    "decline_events_final_sorted = {k: sorted(v, key=lambda x: x[0][0]) for k, v in decline_events_final.items()}\n",
    "print(f'Number of channels with decline events detected: {len(decline_events_final_sorted)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV file\n",
    "with open('data/decline_events_complete.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write header\n",
    "    writer.writerow([\"Channel\", \"Event\", \"Duration\"])\n",
    "    \n",
    "    # Write rows\n",
    "    for channel, events in decline_events_final_sorted.items():\n",
    "        for event, end_week in events:\n",
    "            writer.writerow([channel, event, end_week])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean number of subscribers for each channel\n",
    "mean_subscribers = df_with_rgr_new.reset_index().groupby('channel')['subs'].mean().reset_index()\n",
    "mean_subscribers.columns = ['Channel', 'Mean_Number_of_Subscribers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mean_subscribers DataFrame to a dictionary\n",
    "mean_subscribers_dict = mean_subscribers.set_index('Channel')['Mean_Number_of_Subscribers'].to_dict()\n",
    "\n",
    "# Filter the dictionary to keep only channels with mean subscribers > 1e6\n",
    "filtered_mean_subscribers_dict = {k: v for k, v in mean_subscribers_dict.items() if v > 1e6}\n",
    "\n",
    "decline_events_bb = decline_events_final_sorted.copy()\n",
    "\n",
    "# Iterate over the keys and append mean subscribers\n",
    "for k in list(decline_events_bb.keys()):  # Convert keys to list to avoid runtime error if modifying the dictionary\n",
    "    if k in filtered_mean_subscribers_dict:\n",
    "        decline_events_bb[k].append(filtered_mean_subscribers_dict[k])\n",
    "    else:\n",
    "        del decline_events_bb[k]  # Remove the channel if it doesn't meet the criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if this channel is present is the dictionnary : 'UC-lHJZR3Gqxm24_Vd_AJ5Yw'\n",
    "channel = 'UC-lHJZR3Gqxm24_Vd_AJ5Yw'\n",
    "if channel in decline_events_bb:\n",
    "    nb_events = len(decline_events_bb.get(channel))\n",
    "else:\n",
    "    nb_events = 0\n",
    "print(f'Number of decline events detected for channel {channel}: {nb_events}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many original BB are in this new subset of decline events\n",
    "data = load_bb_timeseries_processed(verbose=True)\n",
    "bb_channels = data.index.get_level_values('channel').unique()\n",
    "\n",
    "nb_channels = 0\n",
    "for channel in bb_channels:\n",
    "    if channel in decline_events_bb:\n",
    "        nb_channels += 1\n",
    "print(f'Number of channels with decline events detected: {nb_channels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV file\n",
    "with open('data/bb_from_declined_events.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write header\n",
    "    writer.writerow([\"Channel\", \"Event\", \"Duration\", \"Mean_Number_of_Subscribers\"])\n",
    "    \n",
    "    # Write rows\n",
    "    for channel, events in decline_events_bb.items():\n",
    "        mean_subscribers = events[-1]  \n",
    "        for event, end_week in events[:-1]:  # Iterate over all events except the last one\n",
    "            writer.writerow([channel, event, end_week, mean_subscribers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot for the datastory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lance_stewart  =  'UC6-NBhOCP8DJqnpZE4TNE-A'\n",
    "\n",
    "print(\"With the past technique, every red area was a decline event.\")\n",
    "plot_rolling_growth_rate2(lance_stewart, df_with_rgr_final)\n",
    "\n",
    "print(\"Now, we obtain less, more meaningful decline events.\")\n",
    "\n",
    "channel_data = df_with_rgr_grouped_final.get_group(lance_stewart )\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Highlight decline events - add these first\n",
    "for event in decline_events_final_sorted.get(lance_stewart , []):\n",
    "    if isinstance(event, tuple) and len(event) == 2:\n",
    "        fig.add_vrect(\n",
    "            x0=event[0][0], \n",
    "            x1=event[0][1], \n",
    "            fillcolor='#DFC5FE', \n",
    "            opacity=0.5, \n",
    "            line_width=0,\n",
    "            layer='below',\n",
    "            name='Decline event'\n",
    "        )\n",
    "    \n",
    "        \n",
    "fig.add_trace(go.Scatter(\n",
    "        x=[None],  # Empty data to not affect the chart\n",
    "        y=[None], \n",
    "        mode='lines', \n",
    "        line=dict(color='#DFC5FE', width=10), \n",
    "        name=f'Decline Event'  # Adjust legend name\n",
    "))\n",
    "\n",
    "# Add growth difference trace with a specific blue color - add these after\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=channel_data['week'], \n",
    "    y=channel_data['delta_subs'], \n",
    "    mode='lines', \n",
    "    name='Growth diff',\n",
    "    line=dict(color='#004AAD')  # Specify the color here\n",
    "))\n",
    "\n",
    "# Add rolling growth rate trace with a specific red color\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=channel_data['week'], \n",
    "    y=channel_data['rolling_growth_rate'], \n",
    "    mode='lines', \n",
    "    name='Rolling growth rate',\n",
    "    line=dict(color='#FF0000')  # Specify the color here\n",
    "))\n",
    "\n",
    "# Update axes, layout, and other styling\n",
    "fig.update_xaxes(\n",
    "    ticks='outside', \n",
    "    tickvals=np.arange(0, 260, 10), \n",
    "    ticktext=[str(i) for i in np.arange(0, 260, 10)]\n",
    ")\n",
    "fig.update_yaxes(ticks='outside')\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Growth diff and rolling growth rate for Lance Stewart\\'s channel',\n",
    "    # center the title\n",
    "    title_x=0.5,\n",
    "    xaxis_title='Week',\n",
    "    yaxis_title='Subscribers',\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=1.02,\n",
    "        xanchor='center',\n",
    "        x=0.5,\n",
    "        bgcolor='rgba(255, 255, 255, 0.5)',\n",
    "        bordercolor='grey',\n",
    "        borderwidth=1.5\n",
    "    ),\n",
    "    template='plotly_white',\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    xaxis_showgrid=False,\n",
    "    yaxis_showgrid=False,\n",
    "    yaxis_zeroline=False,\n",
    "    margin=dict(l=50, r=50, t=100, b=50),\n",
    "    xaxis_tickcolor='black',\n",
    "    yaxis_tickcolor='black',\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "# Add a rectangle shape to create the border\n",
    "fig.add_shape(\n",
    "    type=\"rect\",\n",
    "    x0=0, y0=0, x1=1, y1=1,\n",
    "    xref='paper', yref='paper',\n",
    "    line=dict(color=\"grey\", width=2)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "pio.write_html(fig, file=\"plot_lancet.html\", auto_open=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
