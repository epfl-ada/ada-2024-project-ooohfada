{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from ipywidgets import interact, Dropdown, SelectionSlider, widgets\n",
    "from preprocessing import apply_complete_preprocessing\n",
    "from datasets import load_processed_data, update_processed_data\n",
    "from bbdatasets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data in chunks of 1000: 100%|█████████▉| 18583/18604.825 [00:33<00:00, 549.17it/s]/opt/anaconda3/envs/ada/lib/python3.11/site-packages/tqdm/std.py:636: TqdmWarning: clamping frac to range [0, 1]\n",
      "  full_bar = Bar(frac,\n",
      "Loading data in chunks of 1000: 100%|██████████| 18605/18604.825 [00:33<00:00, 552.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18604824 rows\n"
     ]
    }
   ],
   "source": [
    "# decomment the next line if we need to update the preprocessed data \n",
    "# update_processed_data(verbose=True)\n",
    "original_data = load_processed_data(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data in chunks of 1000: 6it [00:00, 231.92it/s]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5030 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# decomment the next line if we need to update the preprocessed data \n",
    "# update_processed_bb_timeseries(verbose=True)\n",
    "data = load_bb_timeseries_processed(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- i have a dataset that gathers all channels that have suffered at least once of a bad buzz \n",
    "- GOAL : find a general method that can find the bad buzz\n",
    "- HOW CAN I DO IT ? \n",
    "    - i have the list of channels with the date of the bb \n",
    "    - i need to find these dates thanks to a statistical analysis \n",
    "    - i have for each pair of channel and week index : \n",
    "        - category\n",
    "        - views : Total number of views the channel had this week.\n",
    "        - delta_views : Delta views obtained this week.\n",
    "        - subs : Total number of subscribers the channel had this week.\n",
    "        - delta_subs : Delta subscribers obtained this week.\n",
    "        - number of videos : Total number of videos the channel had this week.\n",
    "        - delta_videos : Delta videos obtained this week.\n",
    "        - activity : number of videos posted this week \n",
    "        - view_count => number of views f viedos posted\n",
    "        - like_count => number of likes on pposted video \n",
    "        - dislike_count => number of dislikes on pposted video "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAD BUZZ INDICATORS : \n",
    "    - increase in dislike count \n",
    "    - drop in subscriber growth or loss of subscribers\n",
    "    - decrease in views or view growth \n",
    "    - changes in likes/dislikes ratios : higher dislikes ratio \n",
    "    - changes in number of videos posted, fewer views despite more content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33 channels in the new dataset\n"
     ]
    }
   ],
   "source": [
    "nb_channels = data.reset_index()['channel'].nunique()\n",
    "print(f'There are {nb_channels} channels in the new dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Statistical Analysis : anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Compute rolling averages and standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg_anomaly_detection(data, metric, window_size, bound_size): \n",
    "    data[f'moving_avg_{metric}'] = data.groupby('channel')[metric].transform(lambda x: x.rolling(window_size, min_periods=1).mean())\n",
    "    data[f'moving_std_{metric}'] = data.groupby('channel')[metric].transform(lambda x: x.rolling(window_size, min_periods=1).std())\n",
    "    data[f'upper_bound_{metric}'] = data[f'moving_avg_{metric}'] + bound_size * data[f'moving_std_{metric}']\n",
    "    data[f'lower_bound_{metric}'] = data[f'moving_avg_{metric}'] - bound_size * data[f'moving_std_{metric}']\n",
    "    data[f'is_anomaly_{metric}'] = (data[metric] > data[f'upper_bound_{metric}']) | (data[metric] < data[f'lower_bound_{metric}'])\n",
    "    return data.drop(columns=[f'moving_avg_{metric}', f'moving_std_{metric}', f'upper_bound_{metric}', f'lower_bound_{metric}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels with subs anomalies : 29\n",
      "Number of channels with views anomalies : 20\n",
      "Number of channels with dislikes anomalies : 33\n"
     ]
    }
   ],
   "source": [
    "# TODO : need to play with values \n",
    "WINDOW_SIZE = 8\n",
    "BOUND_SIZE = 2\n",
    "\n",
    "df_moving_avg = data.copy()\n",
    "df_moving_avg = moving_avg_anomaly_detection(df_moving_avg, 'subs', WINDOW_SIZE, BOUND_SIZE)\n",
    "df_moving_avg = moving_avg_anomaly_detection(df_moving_avg, 'views', WINDOW_SIZE, BOUND_SIZE)\n",
    "df_moving_avg = moving_avg_anomaly_detection(df_moving_avg, 'dislike_count', WINDOW_SIZE, BOUND_SIZE)\n",
    "\n",
    "channels_with_subs_anomalies = df_moving_avg.groupby('channel').filter(lambda x : x['is_anomaly_subs'].sum() > 0)\n",
    "channels_with_views_anomalies = df_moving_avg.groupby('channel').filter(lambda x : x['is_anomaly_views'].sum() > 0)\n",
    "channels_with_dislikes_anomalies = df_moving_avg.groupby('channel').filter(lambda x : x['is_anomaly_dislike_count'].sum() > 0)\n",
    "\n",
    "nb_channels_with_subs_anomalies = channels_with_subs_anomalies.reset_index()['channel'].nunique()\n",
    "nb_channels_with_views_anomalies = channels_with_views_anomalies.reset_index()['channel'].nunique()\n",
    "nb_channels_with_dislikes_anomalies = channels_with_dislikes_anomalies.reset_index()['channel'].nunique()\n",
    "\n",
    "print(f'Number of channels with subs anomalies : {nb_channels_with_subs_anomalies}')\n",
    "print(f'Number of channels with views anomalies : {nb_channels_with_views_anomalies}')\n",
    "print(f'Number of channels with dislikes anomalies : {nb_channels_with_dislikes_anomalies}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b6b7d19e2b4d00b0f0dc2435d95cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(description='Channel:', options=('UC-lHJZR3Gqxm24_Vd_AJ5Yw', 'UC0v-tlzsn…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_moving_avg(channel):\n",
    "    df_plot = df_moving_avg.xs(channel, level='channel')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_plot, x='week', y='subs')\n",
    "    plt.scatter(df_plot[df_plot['is_anomaly_subs']].reset_index()['week'], \n",
    "                df_plot[df_plot['is_anomaly_subs']]['subs'],\n",
    "                color='red', label='moving_avg_anomalies', marker='+')\n",
    "    plt.title(f'Channel: {channel}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Create a dropdown widget for selecting channels\n",
    "channel_selector = widgets.SelectionSlider(\n",
    "    options=df_moving_avg.index.get_level_values('channel').unique(),\n",
    "    description='Channel:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Display the widget and update the plot when the selection changes\n",
    "widgets.interactive(plot_moving_avg, channel=channel_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Z-score analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-scores are useful because they indicate how many standard deviations a value is from the mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_anomaly_detection(data, metric, threshold): \n",
    "    data[f'z_score_{metric}'] = data.groupby('channel')[metric].transform(lambda x : stats.zscore(x.dropna()))\n",
    "    data[f'is_anomaly_{metric}'] = data[f'z_score_{metric}'].abs() > threshold\n",
    "    return data.drop(columns=[f'z_score_{metric}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels with subs anomalies : 32\n",
      "Number of channels with views anomalies : 33\n",
      "Number of channels that have both subs and views anomalies : 24\n"
     ]
    }
   ],
   "source": [
    "Z_SCORE_THRESHOLD = 2\n",
    "\n",
    "df_z_score = data.copy()\n",
    "df_z_score = z_score_anomaly_detection(df_z_score, 'delta_subs', Z_SCORE_THRESHOLD)\n",
    "df_z_score = z_score_anomaly_detection(df_z_score, 'delta_views', Z_SCORE_THRESHOLD)\n",
    "\n",
    "channels_with_subs_z_score_anomalies = df_z_score.groupby('channel').filter(lambda x : x['is_anomaly_delta_subs'].sum() > 1)\n",
    "channels_with_views_z_score_anomalies = df_z_score.groupby('channel').filter(lambda x : x['is_anomaly_delta_views'].sum() > 1)\n",
    "\n",
    "nb_channels_with_subs_z_score_anomalies = channels_with_subs_z_score_anomalies.reset_index()['channel'].nunique()\n",
    "nb_channels_with_views_z_score_anomalies = channels_with_views_z_score_anomalies.reset_index()['channel'].nunique()\n",
    "nb_channels_subs_views_anomalies = df_z_score[(df_z_score['is_anomaly_delta_subs']) & \n",
    "                                              (df_z_score['is_anomaly_delta_views'])].reset_index()['channel'].nunique()\n",
    "\n",
    "print(f'Number of channels with subs anomalies : {nb_channels_with_subs_z_score_anomalies}')\n",
    "print(f'Number of channels with views anomalies : {nb_channels_with_views_z_score_anomalies}')\n",
    "print(f'Number of channels that have both subs and views anomalies : {nb_channels_subs_views_anomalies}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "channel\n",
       "UC-lHJZR3Gqxm24_Vd_AJ5Yw    10\n",
       "UC0v-tlzsn0QZwJnkiaUSJVQ     6\n",
       "UC1r4VtVE__5K6c_L_3Vlxxg     5\n",
       "UC2e0bNZ6CzT-Xvr070VaGsw     2\n",
       "UC6-NBhOCP8DJqnpZE4TNE-A     6\n",
       "UCAq9s3QQVCDMvg1iWQBVtxQ     4\n",
       "UCBHu7LsKiwiYViR230RtsCA     9\n",
       "UCDsO-0Yo5zpJk575nKXgMVA     6\n",
       "UCEHf6KUY7Zw7hlXQ7hDemwQ     8\n",
       "UCJZ7f6NQzGKZnFXzFW9y9UQ     6\n",
       "UCKGiTasUqLcZUuUjQiyKotw     8\n",
       "UCKMugoa0uHpjUuq14yOpagw     2\n",
       "UCKlhpmbHGxBE6uw9B_uLeqQ     9\n",
       "UCV9_KinVpV-snHe3C3n1hvA     7\n",
       "UCVJK2AT3ea5RTXNRjX_kz8A     9\n",
       "UCVtFOytbRpEvzLjvqGG5gxQ     8\n",
       "UCWwWOFsW68TqXE-HZLC3WIA     7\n",
       "UCX6OQ3DkcsbYNE6H8uQQuVA    10\n",
       "UCXhSCMRRPyxSoyLSPFxK7VA     9\n",
       "UC_DptbqTndVt_Im3KkuIK5Q     5\n",
       "UCcgVECVN4OKV6DH1jLkqmcA     9\n",
       "UCdJdEguB1F1CiYe7OEi3SBg     6\n",
       "UCdoLeDxfcGwvj_PRl7TLTzQ     1\n",
       "UCiH828EtgQjTyNIMH6YiOSw     4\n",
       "UClWD8su9Sk6GzZDwy9zs3_w     8\n",
       "UCnEn0EUV13IR-_TK7fiIp3g     7\n",
       "UCoiIt_v1D-6z75LmrdIU2aw     5\n",
       "UCtVubfONoPpn4kNuuZ1h6iQ     3\n",
       "UCucot-Zp428OwkyRm2I7v2Q     6\n",
       "UCxJf49T4iTO_jtzWX3rW_jg    12\n",
       "UCy_YiQx1t8oOgz74QIB4Jrw    10\n",
       "UCzJIliq68IHSn-Kwgjeg2AQ     4\n",
       "UCzKc6JrWSt_67UpEYIefrJQ     6\n",
       "Name: is_anomaly_delta_subs, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_score_anomalies_count = df_z_score.groupby('channel')['is_anomaly_delta_subs'].sum()\n",
    "z_score_anomalies_count.columns = ['channel', 'num_anomalies']\n",
    "z_score_anomalies_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I keep a threshold of 2 (meaning that i keeo values that are away of 2 std or more from the mean) then i can get almost all the channels and in each channel i detect more than 1 anomaly \n",
    "\n",
    "It would be now interesting to look at the date of the anomaly to understand effetively is there was a bad buzz\n",
    "\n",
    "I could first plot the number of subs along the time and add a point where an anomaly was detected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0adfdfb7a64483eaebb3a16626b6260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(description='Channel:', options=('UC-lHJZR3Gqxm24_Vd_AJ5Yw', 'UC0v-tlzsn…"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_z_score(channel):\n",
    "    df_plot = df_z_score.xs(channel, level='channel')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=df_plot, x='week', y='subs')\n",
    "    plt.scatter(df_plot[df_plot['is_anomaly_delta_subs']].reset_index()['week'], \n",
    "                df_plot[df_plot['is_anomaly_delta_subs']]['subs'],\n",
    "                color='red', label='z_scores_anomalies', marker='+')\n",
    "    plt.title(f'Channel: {channel}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Create a dropdown widget for selecting channels\n",
    "channel_selector = widgets.SelectionSlider(\n",
    "    options=df_z_score.index.get_level_values('channel').unique(),\n",
    "    description='Channel:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Display the widget and update the plot when the selection changes\n",
    "widgets.interactive(plot_z_score, channel=channel_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
