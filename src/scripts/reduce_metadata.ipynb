{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script reads the very large yt_metadata_en JSONLines file and splits it into smaller CSV files in order to make it more manageable.\n",
    "Each CSV file contains a chunk of the data, with a specified number of records per chunk.\n",
    "The script uses pandas to read the JSONL file, process the data, and save each chunk as a CSV file.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "from preprocessing import map_column_to_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 6 to chunks/chunk_6.csv\n"
     ]
    }
   ],
   "source": [
    "# Set up variables\n",
    "file_path = 'yt_metadata_en.jsonl'\n",
    "chunk_size = 12_924_794\n",
    "columns = ['channel_id', 'upload_date', 'title', 'description']\n",
    "output_folder = 'chunks'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to process each chunk and save it as a CSV\n",
    "def process_and_save_chunk(file_path, start_line, end_line, chunk_index, columns):\n",
    "    processed_data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Skip to the start line\n",
    "        for _ in range(start_line):\n",
    "            file.readline()\n",
    "\n",
    "        # Process the chunk\n",
    "        for i in range(start_line, end_line):\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break  # End of file\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                processed_data.append({col: record[col] for col in columns})\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping malformed line at index {i}\")\n",
    "\n",
    "    # Convert the chunk to a DataFrame\n",
    "    df_chunk = pd.DataFrame(processed_data)\n",
    "\n",
    "    # Write to CSV and clear the DataFrame from memory\n",
    "    output_path = os.path.join(output_folder, f'chunk_{chunk_index}.csv')\n",
    "    df_chunk.to_csv(output_path, index=False)\n",
    "    print(f\"Saved chunk {chunk_index} to {output_path}\")\n",
    "    del df_chunk  # Free up memory\n",
    "\n",
    "# Loop through the file and process each chunk\n",
    "chunk_index = 0\n",
    "start_line = 60_000_001\n",
    "\n",
    "end_line = start_line + chunk_size\n",
    "process_and_save_chunk(file_path, start_line, end_line, 6, columns) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunk 0\n"
     ]
    }
   ],
   "source": [
    "# Test on chunk 0\n",
    "\n",
    "# Load chunk\n",
    "print(\"Loading chunk 0\")\n",
    "chunk = pd.read_csv('../../data/chunks/chunk_0.csv')\n",
    "\n",
    "# Display unique values for upload_date\n",
    "print(\"Unique values for upload_date:\")\n",
    "print(chunk['upload_date'].unique(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunks processing\n",
    "\n",
    "def process_chunk(file_path, chunk_index):\n",
    "    \"\"\"\n",
    "    Process chunks to get first and last week\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): Path to the file\n",
    "    \"\"\"\n",
    "    # Load chunk\n",
    "    print(f\"Loading chunk {chunk_index}\")\n",
    "    chunk = pd.read_csv(file_path)\n",
    "\n",
    "    print(f\"Processing chunk {chunk_index}\")\n",
    "    print(f\"\\n \\Head: {chunk.head(20)}\")\n",
    "    print(f\"\\n \\Tail: {chunk.tail(20)}\")\n",
    "\n",
    "    # Change column name\n",
    "    chunk.rename(columns={'channel': 'channel_id'}, inplace=True)\n",
    "\n",
    "    # Convert date to week\n",
    "    chunk['upload_date'] = pd.to_datetime(chunk['upload_date'])\n",
    "    chunk = map_column_to_week(chunk, 'upload_date')\n",
    "\n",
    "    # Set index to (week, channel)\n",
    "    chunk.set_index(['week', 'channel'], inplace=True)\n",
    "\n",
    "    print(f\"\\n *Head: {chunk.head(20)}\")\n",
    "    print(f\"\\n *Tail: {chunk.tail(20)}\")\n",
    "\n",
    "    # Get first and last week\n",
    "    first_week = chunk['week'].min()\n",
    "    last_week = chunk['week'].max()\n",
    "\n",
    "    # Write first and last week to file name\n",
    "    output_path = file_path.replace('.csv', f'metadata_{first_week}_{last_week}.csv')\n",
    "    chunk.to_csv(output_path, index=False)\n",
    "    print(f\"Saved chunk {chunk_index} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunk 0\n",
      "Processing chunk 0\n",
      "\n",
      " \\Head:                   channel_id          upload_date  \\\n",
      "0   UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-28 00:00:00   \n",
      "1   UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-28 00:00:00   \n",
      "2   UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-28 00:00:00   \n",
      "3   UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-28 00:00:00   \n",
      "4   UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-28 00:00:00   \n",
      "5   UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-27 00:00:00   \n",
      "6   UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-27 00:00:00   \n",
      "7   UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-27 00:00:00   \n",
      "8   UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-27 00:00:00   \n",
      "9   UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-27 00:00:00   \n",
      "10  UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-27 00:00:00   \n",
      "11  UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-26 00:00:00   \n",
      "12  UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-26 00:00:00   \n",
      "13  UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-26 00:00:00   \n",
      "14  UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-26 00:00:00   \n",
      "15  UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-26 00:00:00   \n",
      "16  UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-25 00:00:00   \n",
      "17  UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-25 00:00:00   \n",
      "18  UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-25 00:00:00   \n",
      "19  UCzWrhkg9eK5I8Bm3HfV-unA  2016-09-25 00:00:00   \n",
      "\n",
      "                                                title  \\\n",
      "0   Lego City Police Lego Firetruck Cartoons about...   \n",
      "1   Lego Marvel SuperHeroes Lego Hulk Smash Iron-M...   \n",
      "2   Lego City Police Lego Fireman Cartoons about L...   \n",
      "3   Lego Harry Potter Complete Lego New Movie for ...   \n",
      "4   Lego City Police 1 HOUR LONG VIDEO for kids Le...   \n",
      "5   Lego Marvel SuperHeroes Hulk Smash Iron-Man Le...   \n",
      "6   Lego City Police LONG VIDEO Lego Firetruck Car...   \n",
      "7   Lego City Police Lego Firetruck Long Movie Car...   \n",
      "8   Lego City Police Lego Fireman Firetruck Cartoo...   \n",
      "9   Lego City Police Lego Firetruck Movie Cartoons...   \n",
      "10  Lego City Police Lego Fireman Firetruck Cartoo...   \n",
      "11  Lego City Lego Police1 Hour Long Movie Cartoon...   \n",
      "12  Lego Marvel SuperHeroes Hulk Smash Iron-Man Le...   \n",
      "13  Lego City Police Lego Firetruck 45 Minute Long...   \n",
      "14  Lego City Police Lego Fireman Firetruck Cartoo...   \n",
      "15  Lego City Police Lego Fireman Firetruck Cartoo...   \n",
      "16  Lego Dimensions Movie Lego Cartoons Lego Batma...   \n",
      "17  Lego City Police Lego Cartoons Lego Fire Truck...   \n",
      "18  Lego City Police Lego Cartoons Lego Fire Truck...   \n",
      "19  Lego City Police Lego Cars Police Chase from L...   \n",
      "\n",
      "                                          description  \n",
      "0   Lego City Police Lego Firetruck Cartoons about...  \n",
      "1   Lego Marvel SuperHeroes Lego Hulk Smash Iron-M...  \n",
      "2   Lego City Police Lego Fireman Cartoons about L...  \n",
      "3   Lego Harry Potter Complete Lego New Movie for ...  \n",
      "4   Lego City Police LONG VIDEO for kids Lego Fire...  \n",
      "5   Lego Marvel SuperHeroes Hulk Smash Iron-Man Le...  \n",
      "6   Lego City Fireman Lego Police Firetruck Cartoo...  \n",
      "7   Lego City Police Lego Firetruck Long Movie Car...  \n",
      "8   Lego City Police Lego Fireman Firetruck Cartoo...  \n",
      "9   Lego City Police Lego Firetruck Movie Cartoons...  \n",
      "10  Lego City Police Lego Fireman Firetruck Cartoo...  \n",
      "11  Lego City Lego Police1 Hour Long Movie Cartoon...  \n",
      "12  Lego Marvel SuperHeroes Hulk Smash Iron-Man Le...  \n",
      "13  Lego City Police Lego Firetruck Long Movie Car...  \n",
      "14  Lego City Police Lego Fireman Firetruck Cartoo...  \n",
      "15  Lego City Police Lego Fireman Firetruck Cartoo...  \n",
      "16  Lego Dimensions Movie Lego Cartoons Lego Batma...  \n",
      "17  Lego City Police Lego Cartoons Lego Fire Truck...  \n",
      "18  Lego City Police Lego Cartoons Lego Fire Truck...  \n",
      "19  Lego City Police Lego Cars Police Car Chase fr...  \n",
      "\n",
      " \\Tail:                         channel_id          upload_date  \\\n",
      "10124911  UCjviiML-YkKHSS-T-KV4-qg  2017-04-06 00:00:00   \n",
      "10124912  UCjviiML-YkKHSS-T-KV4-qg  2017-04-05 00:00:00   \n",
      "10124913  UCjviiML-YkKHSS-T-KV4-qg  2017-04-03 00:00:00   \n",
      "10124914  UCjviiML-YkKHSS-T-KV4-qg  2017-04-01 00:00:00   \n",
      "10124915  UCjviiML-YkKHSS-T-KV4-qg  2017-03-31 00:00:00   \n",
      "10124916  UCjviiML-YkKHSS-T-KV4-qg  2017-03-29 00:00:00   \n",
      "10124917  UCjviiML-YkKHSS-T-KV4-qg  2017-03-27 00:00:00   \n",
      "10124918  UCjviiML-YkKHSS-T-KV4-qg  2017-03-26 00:00:00   \n",
      "10124919  UCjviiML-YkKHSS-T-KV4-qg  2017-03-23 00:00:00   \n",
      "10124920  UCjviiML-YkKHSS-T-KV4-qg  2017-03-21 00:00:00   \n",
      "10124921  UCjviiML-YkKHSS-T-KV4-qg  2017-03-20 00:00:00   \n",
      "10124922  UCjviiML-YkKHSS-T-KV4-qg  2017-03-18 00:00:00   \n",
      "10124923  UCjviiML-YkKHSS-T-KV4-qg  2017-03-17 00:00:00   \n",
      "10124924  UCjviiML-YkKHSS-T-KV4-qg  2017-03-13 00:00:00   \n",
      "10124925  UCjviiML-YkKHSS-T-KV4-qg  2017-03-11 00:00:00   \n",
      "10124926  UCjviiML-YkKHSS-T-KV4-qg  2017-03-09 00:00:00   \n",
      "10124927  UCjviiML-YkKHSS-T-KV4-qg  2017-03-06 00:00:00   \n",
      "10124928  UCjviiML-YkKHSS-T-KV4-qg  2017-03-03 00:00:00   \n",
      "10124929  UCjviiML-YkKHSS-T-KV4-qg  2017-03-01 00:00:00   \n",
      "10124930  UCjviiML-YkKHSS-T-KV4-qg  2017-02-27 00:00:00   \n",
      "\n",
      "                                                      title  \\\n",
      "10124911   Louis Lennon - Back Like This (Certified Jackin)   \n",
      "10124912  Jozef K & Winter Son, Lee Walker - Impulse (Ma...   \n",
      "10124913                    Ryan Parmo - Tinteling [DET011]   \n",
      "10124914  Kevin Corral - From The Floor (Rich Pinder, Lu...   \n",
      "10124915  Buurman & Buurman, ENGELHART - Gotta Have it (...   \n",
      "10124916        Buurman & Buurman, ENGELHART - Rouse (FREE)   \n",
      "10124917          Luca Secco & Craftkind - Groovebox (FREE)   \n",
      "10124918  Warehouse Sessions [010] - OVERTRACKED (Tech H...   \n",
      "10124919         Applemoes & Dwayne Pinnock - Swerve (FREE)   \n",
      "10124920                              DJOKO - Sultan (FREE)   \n",
      "10124921                      Avesie - Snake Charmer (FREE)   \n",
      "10124922                    Applemoes & Buzz - Bleep (FREE)   \n",
      "10124923  Martin Hellfritzsch ft. Marck Jamz - Chick Log...   \n",
      "10124924              DJOKO - Stereotypes (ItaloBros Remix)   \n",
      "10124925      Lenny Warn - Trailer Trash (Certified Jackin)   \n",
      "10124926      ILL PHIL & Chris Lorenzo - At Night (FREE DL)   \n",
      "10124927          Jacque Saravante - Button Call (PREMIERE)   \n",
      "10124928           Modèst & Noah Petersen - Basement (Free)   \n",
      "10124929                     TAWÉ - Never Hold Back (Chief)   \n",
      "10124930  Luca Secco & Craftkind, DJOKO - Breakfast Club...   \n",
      "\n",
      "                                                description  \n",
      "10124911  Detection. 2017 House & Techno\\n[DOWNLOAD]http...  \n",
      "10124912  [PREMIERE]\\n(Download)https://goo.gl/AzDko8\\n[...  \n",
      "10124913  Detection. 2017 House & Techno\\n[DOWNLOAD]http...  \n",
      "10124914  [PREMIERE]\\nDetection. 2017 House & Techno\\n[S...  \n",
      "10124915  Detection. 2017 House & Techno\\n[DOWNLOAD]http...  \n",
      "10124916  Detection. 2017 House & Techno\\n[DOWNLOAD]http...  \n",
      "10124917  Detection. 2017 House & Techno\\n[DOWNLOAD]http...  \n",
      "10124918  Detection. 2017 House & Techno\\n[DOWNLOAD]http...  \n",
      "10124919  Detection. 2017 House & Techno\\n[DOWNLOAD]http...  \n",
      "10124920  Detected. 2017 House & Techno\\n[DOWNLOAD]https...  \n",
      "10124921  Detected. 2017 House & Techno\\n[DOWNLOAD]https...  \n",
      "10124922  Detected. 2017 House & Techno\\n[DOWNLOAD]https...  \n",
      "10124923  Detected. 2017 House & Techno\\n[DOWNLOAD]https...  \n",
      "10124924  Detected. 2017 House & Techno\\n[DOWNLOAD]https...  \n",
      "10124925  Detected. 2017 House & Techno\\n[DOWNLOAD]https...  \n",
      "10124926  Detected. 2017 House & Techno\\n[DOWNLOAD]https...  \n",
      "10124927  Detection. 2017 House & Techno\\n[Subscribe] ht...  \n",
      "10124928  Detected. 2017 House & Techno\\n[DOWNLOAD]https...  \n",
      "10124929  Detected. 2017 House & Techno\\n[DOWNLOAD]https...  \n",
      "10124930  Detected. 2017 House & Techno\\n[DOWNLOAD]https...  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "time data \"Natsu (E.N.D) vs Gray Devil Slayer - Fairy Tail Final Season「AMV」- Mirror\n\n\n\n➱ Anime:Fairy Tail Final Season\n\n\n\n\n▶Music: NEFFEX - Mirror\n\n\n▶NEFFEX:\nSpotify: http://bit.ly/NEFFEX_Spotify SoundCloud: http://bit.ly/NEFFEX_SC Facebook: http://bit.ly/NEFFEX_FB Instagram: http://bit.ly/NEFFEX_Insta Twitter: http://bit.ly/NEFFEX_Twitter YouTube: http://bit.ly/NEFFEX_YouTube\n\n\n\n\n➱ Like, Comment & Subscribe!! Thanks for watching! ツ\n\n👉‍ Important All rights reserved to the authors, if you own some material from this video and do not want it displayed here, send an email to me and I will immediately remove it.\n\n\n\"Copyright Disclaimer Under Section 107 of the Copyright Act 1976, allowance is made for \"fair use\" for purposes such as criticism, comment, news reporting, teaching, scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational or personal use tips the balance in favor of fair use.\"\" doesn't match format \"%Y-%m-%d %H:%M:%S\", at position 4876. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1st chunk test\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mprocess_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/chunks/chunk_0.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mprocess_chunk\u001b[0;34m(file_path, chunk_index)\u001b[0m\n\u001b[1;32m     19\u001b[0m chunk\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_id\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Convert date to week\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupload_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mupload_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m chunk \u001b[38;5;241m=\u001b[39m map_column_to_week(chunk, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupload_date\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Set index to (week, channel)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/tools/datetimes.py:1063\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1061\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[0;32m-> 1063\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/tools/datetimes.py:247\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    245\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[0;32m--> 247\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/tools/datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[1;32m    436\u001b[0m     arg,\n\u001b[1;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/tools/datetimes.py:467\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[1;32m    457\u001b[0m     arg,\n\u001b[1;32m    458\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m     result, tz_out \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m         unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32mstrptime.pyx:501\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:451\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:583\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data \"Natsu (E.N.D) vs Gray Devil Slayer - Fairy Tail Final Season「AMV」- Mirror\n\n\n\n➱ Anime:Fairy Tail Final Season\n\n\n\n\n▶Music: NEFFEX - Mirror\n\n\n▶NEFFEX:\nSpotify: http://bit.ly/NEFFEX_Spotify SoundCloud: http://bit.ly/NEFFEX_SC Facebook: http://bit.ly/NEFFEX_FB Instagram: http://bit.ly/NEFFEX_Insta Twitter: http://bit.ly/NEFFEX_Twitter YouTube: http://bit.ly/NEFFEX_YouTube\n\n\n\n\n➱ Like, Comment & Subscribe!! Thanks for watching! ツ\n\n👉‍ Important All rights reserved to the authors, if you own some material from this video and do not want it displayed here, send an email to me and I will immediately remove it.\n\n\n\"Copyright Disclaimer Under Section 107 of the Copyright Act 1976, allowance is made for \"fair use\" for purposes such as criticism, comment, news reporting, teaching, scholarship, and research. Fair use is a use permitted by copyright statute that might otherwise be infringing. Non-profit, educational or personal use tips the balance in favor of fair use.\"\" doesn't match format \"%Y-%m-%d %H:%M:%S\", at position 4876. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "# 1st chunk test\n",
    "process_chunk('../../data/chunks/chunk_0.csv', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
