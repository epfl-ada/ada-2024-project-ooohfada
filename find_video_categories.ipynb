{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import ldamodel\n",
    "from tqdm import tqdm\n",
    "from gensim import corpora\n",
    "from src.utils.recovery_analysis_utils import str_to_list\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "decline_events = pd.read_csv('data/sampled_decline_events_with_videos.csv')\n",
    "videos = pd.read_csv('data/videos_around_declines.csv')\n",
    "\n",
    "decline_events['Videos_before'] = decline_events['Videos_before'].apply(str_to_list)\n",
    "decline_events['Videos_after'] = decline_events['Videos_after'].apply(str_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data_frame with 2 index: the index of the decline and the source (before and after)\n",
    "\n",
    "df_before = decline_events[['Videos_before']].explode('Videos_before')\n",
    "df_before['Source'] = 'Before'\n",
    "df_before = df_before.rename(columns={'Videos_before': 'Video'})\n",
    "\n",
    "df_after = decline_events[['Videos_after']].explode('Videos_after')\n",
    "df_after['Source'] = 'After'\n",
    "df_after = df_after.rename(columns={'Videos_after': 'Video'})\n",
    "\n",
    "df_tags = pd.concat([df_before, df_after], axis=0).reset_index().rename(columns={'index': 'Decline'})\n",
    "df_tags = df_tags.set_index(['Decline', 'Source'])\n",
    "\n",
    "df_tags.sort_values(by = ['Decline', 'Source'])\n",
    "df_tags = df_tags.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Video</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>Before</th>\n",
       "      <td>1684989</td>\n",
       "      <td>MsRosieBea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>1684990</td>\n",
       "      <td>MsRosieBea,primark haul,primark haul august,pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>1684991</td>\n",
       "      <td>MsRosieBea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>1684992</td>\n",
       "      <td>MsRosieBea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>1684993</td>\n",
       "      <td>MsRosieBea,red lip,get ready with me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">36598</th>\n",
       "      <th>After</th>\n",
       "      <td>1889699</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,stra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After</th>\n",
       "      <td>1889700</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,late...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After</th>\n",
       "      <td>1889701</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,lofi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After</th>\n",
       "      <td>1889702</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,mell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After</th>\n",
       "      <td>1889703</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,lofi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2069978 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Video                                               Tags\n",
       "Decline Source                                                            \n",
       "0       Before  1684989                                         MsRosieBea\n",
       "        Before  1684990  MsRosieBea,primark haul,primark haul august,pr...\n",
       "        Before  1684991                                         MsRosieBea\n",
       "        Before  1684992                                         MsRosieBea\n",
       "        Before  1684993               MsRosieBea,red lip,get ready with me\n",
       "...                 ...                                                ...\n",
       "36598   After   1889699  Music,beats,instrumental,right beat radio,stra...\n",
       "        After   1889700  Music,beats,instrumental,right beat radio,late...\n",
       "        After   1889701  Music,beats,instrumental,right beat radio,lofi...\n",
       "        After   1889702  Music,beats,instrumental,right beat radio,mell...\n",
       "        After   1889703  Music,beats,instrumental,right beat radio,lofi...\n",
       "\n",
       "[2069978 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map to obtain the tags of all videos for each video before and after decline\n",
    "df_tags['Tags'] = df_tags['Video'].map(lambda video: videos.loc[video, 'tags'] if video in videos.index else None)\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tags_combined</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>After</th>\n",
       "      <td>MsRosieBea,21st birthday,birthday,ring,jewelle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>MsRosieBea,uni work,studying fashion design,fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>After</th>\n",
       "      <td>hollow,generationhollow,playthrough,blind play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>hollow,generationhollow,playthrough,blind play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>After</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36595</th>\n",
       "      <th>Before</th>\n",
       "      <td>Despacito accordion cover,Fonsi Despacito acco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">36597</th>\n",
       "      <th>After</th>\n",
       "      <td>Shaper,Clapper,Keith Fenner,Fenner,machine sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>Bridgeport,Stainless Steel Placards,Roller Kit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">36598</th>\n",
       "      <th>After</th>\n",
       "      <td>Music,beats,instrumental,right beat radio,minn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>Music,beats,instrumental,right beat radio,lofi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61194 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Tags_combined\n",
       "Decline Source                                                   \n",
       "0       After   MsRosieBea,21st birthday,birthday,ring,jewelle...\n",
       "        Before  MsRosieBea,uni work,studying fashion design,fa...\n",
       "1       After   hollow,generationhollow,playthrough,blind play...\n",
       "        Before  hollow,generationhollow,playthrough,blind play...\n",
       "2       After                                                None\n",
       "...                                                           ...\n",
       "36595   Before  Despacito accordion cover,Fonsi Despacito acco...\n",
       "36597   After   Shaper,Clapper,Keith Fenner,Fenner,machine sho...\n",
       "        Before  Bridgeport,Stainless Steel Placards,Roller Kit...\n",
       "36598   After   Music,beats,instrumental,right beat radio,minn...\n",
       "        Before  Music,beats,instrumental,right beat radio,lofi...\n",
       "\n",
       "[61194 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get for each decline only 2 rows with the tags corresponding to the before and the after, handling NaNs and non-list values\n",
    "df_tags = df_tags.groupby(['Decline', 'Source'])['Tags'].apply(\n",
    "    lambda x: list(set([item for sublist in x.dropna() for item in (sublist if isinstance(sublist, list) else [sublist])]))\n",
    ").reset_index(name='Tags_combined')\n",
    "\n",
    "df_tags.set_index(['Decline', 'Source'], inplace=True)\n",
    "\n",
    "# Map the tags to a string, separating them by new lines\n",
    "df_tags['Tags_combined'] = df_tags['Tags_combined'].map(lambda tags: '\\n'.join(tags) if tags else None)\n",
    "\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CASEFOLD = False\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize and lemmatize the tags, removing stop words and punctuation\n",
    "def preprocess_str(s):\n",
    "    if not s:\n",
    "        return s\n",
    "\n",
    "    tokens = word_tokenize(s.lower() if CASEFOLD else s, language='english', preserve_line=True)\n",
    "    tokens = [[t for t in token_list if t not in stop_words] for token_list in tokens]\n",
    "    tokens = [[lemmatizer.lemmatize(t) for t in token_list] for token_list in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def preprocess_str(s):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return []  # Return an empty list for invalid inputs\n",
    "    tokens = word_tokenize(s.lower() if CASEFOLD else s, preserve_line=True)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and lemmatizing tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61194/61194 [27:20<00:00, 37.30it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary and corpus\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Create a dictionary and a corpus for the LDA model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating dictionary and corpus\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(df_tags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokens\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     25\u001b[0m corpus \u001b[38;5;241m=\u001b[39m [dictionary\u001b[38;5;241m.\u001b[39mdoc2bow(token_list) \u001b[38;5;28;01mfor\u001b[39;00m token_list \u001b[38;5;129;01min\u001b[39;00m df_tags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokens\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining LDA model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\eva\\anaconda3\\envs\\ada\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:78\u001b[0m, in \u001b[0;36mDictionary.__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_nnz \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_documents(documents, prune_at\u001b[38;5;241m=\u001b[39mprune_at)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m         msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     82\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\eva\\anaconda3\\envs\\ada\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:204\u001b[0m, in \u001b[0;36mDictionary.add_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    201\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madding document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, docno, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# update Dictionary with the document\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc2bow(document, allow_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# ignore the result, here we only care about updating token ids\u001b[39;00m\n\u001b[0;32m    206\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos)\n",
      "File \u001b[1;32mc:\\Users\\eva\\anaconda3\\envs\\ada\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:246\u001b[0m, in \u001b[0;36mDictionary.doc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    244\u001b[0m counter \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m document:\n\u001b[1;32m--> 246\u001b[0m     counter[w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    248\u001b[0m token2id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_update \u001b[38;5;129;01mor\u001b[39;00m return_missing:\n",
      "\u001b[1;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, list found"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing and lemmatizing tags\")\n",
    "df_tags['Tokens'] = None\n",
    "for index, row in tqdm(df_tags.iterrows(), total=df_tags.shape[0]):\n",
    "    df_tags.at[index, 'Tokens'] = preprocess_str(row['Tags_combined'])\n",
    "\n",
    "\n",
    "# Create a dictionary and a corpus for the LDA model\n",
    "print(\"Creating dictionary and corpus\")\n",
    "dictionary = corpora.Dictionary(df_tags['Tokens'])\n",
    "corpus = [dictionary.doc2bow(token_list) for token_list in df_tags['Tokens']]\n",
    "\n",
    "print(\"Training LDA model\")\n",
    "lda = ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = lda.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decline  Source\n",
      "0        After     [M, R, e, B, e, ,, 2, 1, b, r, h, ,, b, r, h, ...\n",
      "         Before    [M, R, e, B, e, ,, u, n, w, r, k, ,, u, n, g, ...\n",
      "1        After     [h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...\n",
      "         Before    [h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...\n",
      "2        After                                                    []\n",
      "                                         ...                        \n",
      "36595    Before    [D, e, p, c, c, c, r, n, c, v, e, r, ,, F, n, ...\n",
      "36597    After     [S, h, p, e, r, ,, C, l, p, p, e, r, ,, K, e, ...\n",
      "         Before    [B, r, g, e, p, r, ,, S, n, l, e, S, e, e, l, ...\n",
      "36598    After     [M, u, c, ,, b, e, ,, n, r, u, e, n, l, ,, r, ...\n",
      "         Before    [M, u, c, ,, b, e, ,, n, r, u, e, n, l, ,, r, ...\n",
      "Name: Tokens, Length: 61194, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print(df_tags['Tokens'].head(10))\n",
    "\n",
    "\n",
    "# PROBLEM: The lists are embedded, only need one big list per double index (decline + source)\n",
    "# The flattenig should work but need to remove the rows that have None\n",
    "\n",
    "# Replace None or NaN in Tokens with empty lists\n",
    "df_tags['Tokens'] = df_tags['Tokens'].apply(\n",
    "    lambda x: [] if x is None else x\n",
    ")\n",
    "\n",
    "\n",
    "# Flatten any nested lists in Tokens\n",
    "df_tags['Tokens'] = df_tags['Tokens'].apply(\n",
    "    lambda tokens: [item for sublist in tokens for item in sublist] if any(isinstance(i, list) for i in tokens) else tokens\n",
    "    if isinstance(tokens, list) else []\n",
    ")\n",
    "\n",
    "# Check the cleaned Tokens column\n",
    "print(df_tags['Tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens\n",
       "<class 'list'>        57519\n",
       "<class 'NoneType'>     3675\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tags['Tokens'].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary and corpus\n",
      "Training LDA model\n",
      "(0, '0.071*\",\" + 0.047*\"а\" + 0.043*\"и\" + 0.040*\"о\" + 0.035*\"е\"')\n",
      "(1, '0.117*\",\" + 0.112*\"e\" + 0.069*\"n\" + 0.068*\"r\" + 0.043*\"l\"')\n",
      "(2, '0.132*\"1\" + 0.112*\"2\" + 0.091*\"0\" + 0.068*\",\" + 0.058*\"e\"')\n",
      "(3, '0.097*\"E\" + 0.090*\"A\" + 0.075*\"I\" + 0.070*\"O\" + 0.069*\",\"')\n",
      "(4, '0.165*\"e\" + 0.134*\",\" + 0.107*\"n\" + 0.107*\"r\" + 0.084*\"l\"')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tags_combined</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>After</th>\n",
       "      <td>MsRosieBea,21st birthday,birthday,ring,jewelle...</td>\n",
       "      <td>[M, R, e, B, e, ,, 2, 1, b, r, h, ,, b, r, h, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>MsRosieBea,uni work,studying fashion design,fa...</td>\n",
       "      <td>[M, R, e, B, e, ,, u, n, w, r, k, ,, u, n, g, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>After</th>\n",
       "      <td>hollow,generationhollow,playthrough,blind play...</td>\n",
       "      <td>[h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>hollow,generationhollow,playthrough,blind play...</td>\n",
       "      <td>[h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>After</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36595</th>\n",
       "      <th>Before</th>\n",
       "      <td>Despacito accordion cover,Fonsi Despacito acco...</td>\n",
       "      <td>[D, e, p, c, c, c, r, n, c, v, e, r, ,, F, n, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">36597</th>\n",
       "      <th>After</th>\n",
       "      <td>Shaper,Clapper,Keith Fenner,Fenner,machine sho...</td>\n",
       "      <td>[S, h, p, e, r, ,, C, l, p, p, e, r, ,, K, e, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>Bridgeport,Stainless Steel Placards,Roller Kit...</td>\n",
       "      <td>[B, r, g, e, p, r, ,, S, n, l, e, S, e, e, l, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">36598</th>\n",
       "      <th>After</th>\n",
       "      <td>Music,beats,instrumental,right beat radio,minn...</td>\n",
       "      <td>[M, u, c, ,, b, e, ,, n, r, u, e, n, l, ,, r, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>Music,beats,instrumental,right beat radio,lofi...</td>\n",
       "      <td>[M, u, c, ,, b, e, ,, n, r, u, e, n, l, ,, r, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61194 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Tags_combined  \\\n",
       "Decline Source                                                      \n",
       "0       After   MsRosieBea,21st birthday,birthday,ring,jewelle...   \n",
       "        Before  MsRosieBea,uni work,studying fashion design,fa...   \n",
       "1       After   hollow,generationhollow,playthrough,blind play...   \n",
       "        Before  hollow,generationhollow,playthrough,blind play...   \n",
       "2       After                                                None   \n",
       "...                                                           ...   \n",
       "36595   Before  Despacito accordion cover,Fonsi Despacito acco...   \n",
       "36597   After   Shaper,Clapper,Keith Fenner,Fenner,machine sho...   \n",
       "        Before  Bridgeport,Stainless Steel Placards,Roller Kit...   \n",
       "36598   After   Music,beats,instrumental,right beat radio,minn...   \n",
       "        Before  Music,beats,instrumental,right beat radio,lofi...   \n",
       "\n",
       "                                                           Tokens  \n",
       "Decline Source                                                     \n",
       "0       After   [M, R, e, B, e, ,, 2, 1, b, r, h, ,, b, r, h, ...  \n",
       "        Before  [M, R, e, B, e, ,, u, n, w, r, k, ,, u, n, g, ...  \n",
       "1       After   [h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...  \n",
       "        Before  [h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...  \n",
       "2       After                                                  []  \n",
       "...                                                           ...  \n",
       "36595   Before  [D, e, p, c, c, c, r, n, c, v, e, r, ,, F, n, ...  \n",
       "36597   After   [S, h, p, e, r, ,, C, l, p, p, e, r, ,, K, e, ...  \n",
       "        Before  [B, r, g, e, p, r, ,, S, n, l, e, S, e, e, l, ...  \n",
       "36598   After   [M, u, c, ,, b, e, ,, n, r, u, e, n, l, ,, r, ...  \n",
       "        Before  [M, u, c, ,, b, e, ,, n, r, u, e, n, l, ,, r, ...  \n",
       "\n",
       "[61194 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary and a corpus for the LDA model\n",
    "print(\"Creating dictionary and corpus\")\n",
    "dictionary = corpora.Dictionary(df_tags['Tokens'])\n",
    "corpus = [dictionary.doc2bow(token_list) for token_list in df_tags['Tokens']]\n",
    "\n",
    "print(\"Training LDA model\")\n",
    "lda = ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = lda.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of the preprocessing with a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "                                                    Tags_combined  \\\n",
      "Decline Source                                                      \n",
      "0       After   MsRosieBea,21st birthday,birthday,ring,jewelle...   \n",
      "        Before  MsRosieBea,uni work,studying fashion design,fa...   \n",
      "1       After   hollow,generationhollow,playthrough,blind play...   \n",
      "        Before  hollow,generationhollow,playthrough,blind play...   \n",
      "2       After                                                None   \n",
      "\n",
      "                                                           Tokens  \n",
      "Decline Source                                                     \n",
      "0       After   [M, R, e, B, e, ,, 2, 1, b, r, h, ,, b, r, h, ...  \n",
      "        Before  [M, R, e, B, e, ,, u, n, w, r, k, ,, u, n, g, ...  \n",
      "1       After   [h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...  \n",
      "        Before  [h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...  \n",
      "2       After                                                  []  \n"
     ]
    }
   ],
   "source": [
    "df_small = df_tags.head(100)\n",
    "print(df_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eva\\AppData\\Local\\Temp\\ipykernel_22020\\2423845464.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_small['Tokens'] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and lemmatizing tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 94.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary and corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create a dictionary and a corpus for the LDA model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating dictionary and corpus\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(df_small[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokens\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m corpus \u001b[38;5;241m=\u001b[39m [dictionary\u001b[38;5;241m.\u001b[39mdoc2bow(token_list) \u001b[38;5;28;01mfor\u001b[39;00m token_list \u001b[38;5;129;01min\u001b[39;00m df_small[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokens\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining LDA model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\eva\\anaconda3\\envs\\ada\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:78\u001b[0m, in \u001b[0;36mDictionary.__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_nnz \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_documents(documents, prune_at\u001b[38;5;241m=\u001b[39mprune_at)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m         msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     82\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\eva\\anaconda3\\envs\\ada\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:204\u001b[0m, in \u001b[0;36mDictionary.add_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    201\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madding document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, docno, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# update Dictionary with the document\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc2bow(document, allow_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# ignore the result, here we only care about updating token ids\u001b[39;00m\n\u001b[0;32m    206\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos)\n",
      "File \u001b[1;32mc:\\Users\\eva\\anaconda3\\envs\\ada\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:245\u001b[0m, in \u001b[0;36mDictionary.doc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Construct (word, frequency) mapping.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m counter \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m document:\n\u001b[0;32m    246\u001b[0m     counter[w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    248\u001b[0m token2id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing and lemmatizing tags\")\n",
    "df_small['Tokens'] = None\n",
    "for index, row in tqdm(df_small.iterrows(), total=df_small.shape[0]):\n",
    "    df_small.at[index, 'Tokens'] = preprocess_str(row['Tags_combined'])\n",
    "\n",
    "\n",
    "# Create a dictionary and a corpus for the LDA model\n",
    "print(\"Creating dictionary and corpus\")\n",
    "dictionary = corpora.Dictionary(df_small['Tokens'])\n",
    "corpus = [dictionary.doc2bow(token_list) for token_list in df_small['Tokens']]\n",
    "\n",
    "print(\"Training LDA model\")\n",
    "lda = ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = lda.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens\n",
       "<class 'list'>        86\n",
       "<class 'NoneType'>    14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small['Tokens'].apply(type).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
