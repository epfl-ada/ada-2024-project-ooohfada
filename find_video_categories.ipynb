{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import ldamodel\n",
    "from tqdm import tqdm\n",
    "from gensim import corpora\n",
    "\n",
    "from src.utils.recovery_analysis_utils import str_to_list\n",
    "from src.utils.find_video_categories_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply LDA to annotate the videos before and after the start of declines with categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          channel  week  \\\n",
      "0        UCzWm1-4XF7AHxVUTkHCM1uw   227   \n",
      "1        UCzWm1-4XF7AHxVUTkHCM1uw   226   \n",
      "2        UCzWm1-4XF7AHxVUTkHCM1uw   224   \n",
      "3        UCzWm1-4XF7AHxVUTkHCM1uw   224   \n",
      "4        UCzWm1-4XF7AHxVUTkHCM1uw   223   \n",
      "...                           ...   ...   \n",
      "1905541  UCrwE8kVqtIUVUzKui2WVpuQ   109   \n",
      "1905542  UCrwE8kVqtIUVUzKui2WVpuQ   109   \n",
      "1905543  UCrwE8kVqtIUVUzKui2WVpuQ   109   \n",
      "1905544  UCrwE8kVqtIUVUzKui2WVpuQ   109   \n",
      "1905545  UCrwE8kVqtIUVUzKui2WVpuQ   109   \n",
      "\n",
      "                                                      tags  duration  \n",
      "0        video,games,retrogamer3,ed,findlay,Scam,Steam,...       384  \n",
      "1        video,games,retrogamer3,ed,findlay,Trump,Ameri...       270  \n",
      "2        video,games,retrogamer3,ed,findlay,America's R...       109  \n",
      "3                               MTG Arena War of the Spark      5154  \n",
      "4        video,games,retrogamer3,ed,findlay,Mpow,Headph...       475  \n",
      "...                                                    ...       ...  \n",
      "1905541  BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...       270  \n",
      "1905542  BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...       878  \n",
      "1905543  BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...      1003  \n",
      "1905544  BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...       755  \n",
      "1905545  BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...      1771  \n",
      "\n",
      "[1905546 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "decline_events, videos = load_data()\n",
    "print(videos)\n",
    "decline_events = process_data(decline_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Tags_combined\n",
      "Decline Source                                                   \n",
      "0       After   MsRosieBea,uni,uni life,first year of uni,thir...\n",
      "        Before  MsRosieBea,red lip,get ready with me\\nMsRosieB...\n",
      "1       After   hollow,generationhollow,the surge,surge,robo s...\n",
      "        Before  hollow,generationhollow,paragon,gameplay,alpha...\n",
      "2       After                                                None\n",
      "...                                                           ...\n",
      "36595   Before  Despacito accordion cover,Fonsi Despacito acco...\n",
      "36597   After   #patriotattitude,#whenhellfreezesover,Keith Fe...\n",
      "        Before  Audi,Audi 2.1,Line Bore,Kenax,Line bore Kenax ...\n",
      "36598   After   Music,beats,instrumental,right beat radio,mell...\n",
      "        Before  Music,beats,instrumental,right beat radio,mell...\n",
      "\n",
      "[61194 rows x 1 columns]\n",
      "Tokenizing and lemmatizing tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61194/61194 [03:13<00:00, 316.42it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_tags = create_tags_dataframe(decline_events, videos)\n",
    "\n",
    "print(df_tags)\n",
    "\n",
    "print(\"Tokenizing and lemmatizing tags\")\n",
    "df_tags['Tokens'] = None\n",
    "for index, row in tqdm(df_tags.iterrows(), total=df_tags.shape[0]):\n",
    "    df_tags.at[index, 'Tokens'] = preprocess_str(row['Tags_combined'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the LDA model to get the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary and corpus\n",
      "Training LDA model\n",
      "(0, '0.033*\"makeup\" + 0.019*\"haul\" + 0.015*\"hair\" + 0.013*\"beauty\" + 0.011*\"fashion\" + 0.010*\"tutorial\" + 0.009*\"day\" + 0.008*\"recipe\" + 0.008*\"food\" + 0.007*\"review\" + 0.007*\"style\" + 0.007*\"routine\" + 0.006*\"home\" + 0.006*\"vegan\" + 0.006*\"tip\"')\n",
      "(1, '0.025*\"funny\" + 0.023*\"family\" + 0.022*\"vlog\" + 0.020*\"video\" + 0.012*\"prank\" + 0.011*\"vlogs\" + 0.010*\"life\" + 0.010*\"challenge\" + 0.009*\"girl\" + 0.009*\"daily\" + 0.009*\"comedy\" + 0.009*\"youtube\" + 0.007*\"baby\" + 0.007*\"reaction\" + 0.006*\"vlogger\"')\n",
      "(2, '0.050*\"music\" + 0.042*\"beat\" + 0.030*\"type\" + 0.018*\"dance\" + 0.016*\"free\" + 0.015*\"trap\" + 0.015*\"song\" + 0.013*\"2018\" + 0.012*\"cover\" + 0.012*\"rap\" + 0.010*\"remix\" + 0.010*\"hop\" + 0.010*\"new\" + 0.009*\"hip\" + 0.009*\"lyric\"')\n",
      "(3, '0.061*\"game\" + 0.030*\"gameplay\" + 0.026*\"play\" + 0.021*\"let\" + 0.019*\"minecraft\" + 0.018*\"\\'s\" + 0.013*\"gaming\" + 0.012*\"walkthrough\" + 0.011*\"mod\" + 0.010*\"part\" + 0.010*\"pc\" + 0.009*\"video\" + 0.009*\"ark\" + 0.009*\"2\" + 0.009*\"1\"')\n",
      "(4, '0.055*\"2\" + 0.032*\"gta\" + 0.031*\"5\" + 0.029*\"destiny\" + 0.020*\"online\" + 0.020*\"black\" + 0.019*\"gameplay\" + 0.017*\"ops\" + 0.017*\"4\" + 0.016*\"glitch\" + 0.015*\"call\" + 0.015*\"duty\" + 0.015*\"zombie\" + 0.014*\"3\" + 0.014*\"cod\"')\n",
      "(5, '0.015*\"overwatch\" + 0.010*\"god\" + 0.009*\"time\" + 0.008*\"de\" + 0.008*\"world\" + 0.007*\"anime\" + 0.007*\"angel\" + 0.007*\"hero\" + 0.006*\"jesus\" + 0.006*\"spirit\" + 0.006*\"new\" + 0.006*\"science\" + 0.006*\"dubai\" + 0.006*\"space\" + 0.005*\"fire\"')\n",
      "(6, '0.020*\"android\" + 0.014*\"review\" + 0.014*\"free\" + 0.013*\"best\" + 0.012*\"car\" + 0.012*\"iphone\" + 0.009*\"pro\" + 0.009*\"io\" + 0.008*\"golf\" + 0.008*\"new\" + 0.007*\"x\" + 0.007*\"mobile\" + 0.007*\"phone\" + 0.007*\"2018\" + 0.006*\"galaxy\"')\n",
      "(7, '0.068*\"news\" + 0.028*\"cricket\" + 0.026*\"video\" + 0.025*\"2019\" + 0.024*\"hindi\" + 0.023*\"latest\" + 0.022*\"song\" + 0.019*\"new\" + 0.017*\"indian\" + 0.016*\"india\" + 0.015*\"2018\" + 0.015*\"pakistan\" + 0.014*\"live\" + 0.011*\"tv\" + 0.010*\"channel\"')\n",
      "(8, '0.030*\"news\" + 0.016*\"trump\" + 0.010*\"pgt\" + 0.009*\"show\" + 0.008*\"u\" + 0.007*\"interview\" + 0.007*\"entertainment\" + 0.007*\"state\" + 0.007*\"politics\" + 0.007*\"medium\" + 0.007*\"donald\" + 0.006*\"business\" + 0.006*\"magazine\" + 0.005*\"live\" + 0.005*\"john\"')\n",
      "(9, '0.128*\"fortnite\" + 0.037*\"battle\" + 0.035*\"royale\" + 0.025*\"new\" + 0.019*\"clash\" + 0.016*\"season\" + 0.015*\"best\" + 0.014*\"skin\" + 0.013*\"moment\" + 0.012*\"funny\" + 0.012*\"rocket\" + 0.012*\"update\" + 0.010*\"clan\" + 0.010*\"faze\" + 0.009*\"ninja\"')\n",
      "(10, '0.171*\"v\" + 0.019*\"team\" + 0.018*\"fight\" + 0.018*\"boxing\" + 0.017*\"summit\" + 0.016*\"ufc\" + 0.012*\"fury\" + 0.011*\"injustice\" + 0.011*\"vape\" + 0.011*\"beyond\" + 0.010*\"mma\" + 0.010*\"vega\" + 0.009*\"garcia\" + 0.009*\"match\" + 0.008*\"wilder\"')\n",
      "(11, '0.081*\"pokemon\" + 0.031*\"nintendo\" + 0.026*\"switch\" + 0.023*\"smash\" + 0.022*\"go\" + 0.019*\"super\" + 0.017*\"game\" + 0.016*\"mario\" + 0.015*\"moon\" + 0.014*\"sun\" + 0.013*\"ultimate\" + 0.013*\"shiny\" + 0.013*\"card\" + 0.010*\"bros\" + 0.010*\"sonic\"')\n",
      "(12, '0.034*\"sport\" + 0.032*\"league\" + 0.024*\"cup\" + 0.023*\"world\" + 0.022*\"highlight\" + 0.021*\"football\" + 0.017*\"basketball\" + 0.016*\"v\" + 0.015*\"minor\" + 0.012*\"2018\" + 0.010*\"baseball\" + 0.010*\"2019\" + 0.010*\"goal\" + 0.009*\"nfl\" + 0.009*\"hockey\"')\n",
      "(13, '0.029*\"league\" + 0.029*\"roblox\" + 0.028*\"2\" + 0.026*\"dota\" + 0.022*\"legend\" + 0.017*\"highlight\" + 0.017*\"csgo\" + 0.017*\"game\" + 0.016*\"pubg\" + 0.016*\"best\" + 0.015*\"gaming\" + 0.015*\"moment\" + 0.013*\"gameplay\" + 0.013*\"pro\" + 0.012*\"lol\"')\n",
      "(14, '0.019*\"travel\" + 0.012*\"park\" + 0.012*\"train\" + 0.011*\"six\" + 0.011*\"house\" + 0.010*\"living\" + 0.010*\"rainbow\" + 0.009*\"great\" + 0.009*\"tour\" + 0.009*\"siege\" + 0.008*\"fishing\" + 0.008*\"beach\" + 0.008*\"dane\" + 0.007*\"road\" + 0.007*\"mouse\"')\n",
      "(15, '0.067*\"kid\" + 0.043*\"toy\" + 0.025*\"video\" + 0.021*\"baby\" + 0.015*\"color\" + 0.015*\"slime\" + 0.015*\"fun\" + 0.014*\"learn\" + 0.013*\"tarot\" + 0.013*\"child\" + 0.012*\"animal\" + 0.012*\"play\" + 0.011*\"reading\" + 0.010*\"song\" + 0.010*\"game\"')\n",
      "(16, '0.034*\"star\" + 0.033*\"movie\" + 0.027*\"war\" + 0.020*\"trailer\" + 0.019*\"review\" + 0.018*\"reaction\" + 0.018*\"season\" + 0.018*\"episode\" + 0.013*\"comic\" + 0.011*\"2\" + 0.011*\"film\" + 0.010*\"disney\" + 0.010*\"talent\" + 0.009*\"got\" + 0.009*\"marvel\"')\n",
      "(17, '0.041*\"design\" + 0.024*\"tutorial\" + 0.022*\"art\" + 0.021*\"diy\" + 0.019*\"easy\" + 0.018*\"simple\" + 0.015*\"asmr\" + 0.014*\"dot\" + 0.013*\"make\" + 0.012*\"kpop\" + 0.011*\"pilipinas\" + 0.010*\"hand\" + 0.010*\"craft\" + 0.010*\"bts\" + 0.010*\"nail\"')\n",
      "(18, '0.071*\"wwe\" + 0.060*\"nba\" + 0.023*\"ball\" + 0.018*\"2k19\" + 0.017*\"dragon\" + 0.017*\"2k18\" + 0.010*\"raw\" + 0.010*\"2k17\" + 0.009*\"best\" + 0.009*\"wrestling\" + 0.008*\"celtic\" + 0.008*\"dokkan\" + 0.007*\"naruto\" + 0.007*\"v\" + 0.006*\"myteam\"')\n",
      "(19, '0.063*\"fifa\" + 0.055*\"18\" + 0.050*\"19\" + 0.042*\"madden\" + 0.031*\"team\" + 0.031*\"pack\" + 0.030*\"17\" + 0.021*\"ultimate\" + 0.015*\"career\" + 0.015*\"squad\" + 0.014*\"gameplay\" + 0.013*\"mode\" + 0.013*\"opening\" + 0.012*\"mlb\" + 0.012*\"wow\"')\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary and a corpus for the LDA model\n",
    "print(\"Creating dictionary and corpus\")\n",
    "dictionary, corpus = create_dictionary_and_corpus(df_tags) \n",
    "\n",
    "print(\"Training LDA model\")\n",
    "lda = ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = lda.print_topics(num_words=15)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics saved to lda_topics.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the topics\n",
    "\n",
    "topics = lda.print_topics(num_words=15)\n",
    "\n",
    "# Create a DataFrame from the topics\n",
    "topics_data = []\n",
    "for topic_id, topic in topics:\n",
    "    topics_data.append({\"Topic\": topic_id, \"Words\": topic})\n",
    "\n",
    "topics_df = pd.DataFrame(topics_data)\n",
    "\n",
    "topics_df.to_csv(\"data/lda_topics.csv\", index=False)\n",
    "print(\"Topics saved to lda_topics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the topics to the videos before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Tags_combined  \\\n",
      "Decline Source                                                      \n",
      "0       After   MsRosieBea,uni,uni life,first year of uni,thir...   \n",
      "        Before  MsRosieBea,red lip,get ready with me\\nMsRosieB...   \n",
      "1       After   hollow,generationhollow,the surge,surge,robo s...   \n",
      "        Before  hollow,generationhollow,paragon,gameplay,alpha...   \n",
      "2       After                                                None   \n",
      "\n",
      "                                                           Tokens  \\\n",
      "Decline Source                                                      \n",
      "0       After   [msrosiebea, uni, uni, life, first, year, uni,...   \n",
      "        Before  [msrosiebea, red, lip, get, ready, me\\nmsrosie...   \n",
      "1       After   [hollow, generationhollow, surge, surge, robo,...   \n",
      "        Before  [hollow, generationhollow, paragon, gameplay, ...   \n",
      "2       After                                                  []   \n",
      "\n",
      "                Dominant_Topic  Topic_Probability  \n",
      "Decline Source                                     \n",
      "0       After              0.0           0.712755  \n",
      "        Before             0.0           0.881411  \n",
      "1       After              3.0           0.555881  \n",
      "        Before             3.0           0.287323  \n",
      "2       After              NaN                NaN  \n"
     ]
    }
   ],
   "source": [
    "df_tags['Dominant_Topic'], df_tags['Topic_Probability'] = zip(\n",
    "    *df_tags['Tokens'].apply(lambda tokens: assign_dominant_topic(tokens, lda, dictionary))\n",
    ")\n",
    "\n",
    "print(df_tags.head(5))\n",
    "df_tags.to_csv('data/df_small_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.5815071105454963\n"
     ]
    }
   ],
   "source": [
    "coherence_lda = calculate_coherence(lda, df_tags, dictionary)\n",
    "print(f'Coherence Score: {coherence_lda}')\n",
    "# whole df: 0.6525 with 55 topics, 0.5991 with 50, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
