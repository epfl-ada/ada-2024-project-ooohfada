{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import ldamodel\n",
    "from tqdm import tqdm\n",
    "from gensim import corpora\n",
    "from src.utils.recovery_analysis_utils import str_to_list\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "decline_events = pd.read_csv('data/sampled_decline_events_with_videos.csv')\n",
    "videos = pd.read_csv('data/videos_around_declines.csv')\n",
    "\n",
    "decline_events['Videos_before'] = decline_events['Videos_before'].apply(str_to_list)\n",
    "decline_events['Videos_after'] = decline_events['Videos_after'].apply(str_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data_frame with 2 index: the index of the decline and the source (before and after)\n",
    "\n",
    "df_before = decline_events[['Videos_before']].explode('Videos_before')\n",
    "df_before['Source'] = 'Before'\n",
    "df_before = df_before.rename(columns={'Videos_before': 'Video'})\n",
    "\n",
    "df_after = decline_events[['Videos_after']].explode('Videos_after')\n",
    "df_after['Source'] = 'After'\n",
    "df_after = df_after.rename(columns={'Videos_after': 'Video'})\n",
    "\n",
    "df_tags = pd.concat([df_before, df_after], axis=0).reset_index().rename(columns={'index': 'Decline'})\n",
    "df_tags = df_tags.set_index(['Decline', 'Source'])\n",
    "\n",
    "df_tags.sort_values(by = ['Decline', 'Source'])\n",
    "df_tags = df_tags.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Video</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>Before</th>\n",
       "      <td>1684989</td>\n",
       "      <td>MsRosieBea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>1684990</td>\n",
       "      <td>MsRosieBea,primark haul,primark haul august,pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>1684991</td>\n",
       "      <td>MsRosieBea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>1684992</td>\n",
       "      <td>MsRosieBea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>1684993</td>\n",
       "      <td>MsRosieBea,red lip,get ready with me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">36598</th>\n",
       "      <th>After</th>\n",
       "      <td>1889699</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,stra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After</th>\n",
       "      <td>1889700</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,late...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After</th>\n",
       "      <td>1889701</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,lofi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After</th>\n",
       "      <td>1889702</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,mell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After</th>\n",
       "      <td>1889703</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,lofi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2069978 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Video                                               Tags\n",
       "Decline Source                                                            \n",
       "0       Before  1684989                                         MsRosieBea\n",
       "        Before  1684990  MsRosieBea,primark haul,primark haul august,pr...\n",
       "        Before  1684991                                         MsRosieBea\n",
       "        Before  1684992                                         MsRosieBea\n",
       "        Before  1684993               MsRosieBea,red lip,get ready with me\n",
       "...                 ...                                                ...\n",
       "36598   After   1889699  Music,beats,instrumental,right beat radio,stra...\n",
       "        After   1889700  Music,beats,instrumental,right beat radio,late...\n",
       "        After   1889701  Music,beats,instrumental,right beat radio,lofi...\n",
       "        After   1889702  Music,beats,instrumental,right beat radio,mell...\n",
       "        After   1889703  Music,beats,instrumental,right beat radio,lofi...\n",
       "\n",
       "[2069978 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map to obtain the tags of all videos for each video before and after decline\n",
    "df_tags['Tags'] = df_tags['Video'].map(lambda video: videos.loc[video, 'tags'] if video in videos.index else None)\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tags_combined</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>After</th>\n",
       "      <td>MsRosieBea,21st birthday,birthday,ring,jewelle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>MsRosieBea,uni work,studying fashion design,fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>After</th>\n",
       "      <td>hollow,generationhollow,playthrough,blind play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>hollow,generationhollow,playthrough,blind play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>After</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36595</th>\n",
       "      <th>Before</th>\n",
       "      <td>Despacito accordion cover,Fonsi Despacito acco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">36597</th>\n",
       "      <th>After</th>\n",
       "      <td>Shaper,Clapper,Keith Fenner,Fenner,machine sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>Bridgeport,Stainless Steel Placards,Roller Kit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">36598</th>\n",
       "      <th>After</th>\n",
       "      <td>Music,beats,instrumental,right beat radio,minn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>Music,beats,instrumental,right beat radio,lofi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61194 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Tags_combined\n",
       "Decline Source                                                   \n",
       "0       After   MsRosieBea,21st birthday,birthday,ring,jewelle...\n",
       "        Before  MsRosieBea,uni work,studying fashion design,fa...\n",
       "1       After   hollow,generationhollow,playthrough,blind play...\n",
       "        Before  hollow,generationhollow,playthrough,blind play...\n",
       "2       After                                                None\n",
       "...                                                           ...\n",
       "36595   Before  Despacito accordion cover,Fonsi Despacito acco...\n",
       "36597   After   Shaper,Clapper,Keith Fenner,Fenner,machine sho...\n",
       "        Before  Bridgeport,Stainless Steel Placards,Roller Kit...\n",
       "36598   After   Music,beats,instrumental,right beat radio,minn...\n",
       "        Before  Music,beats,instrumental,right beat radio,lofi...\n",
       "\n",
       "[61194 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get for each decline only 2 rows with the tags corresponding to the before and the after, handling NaNs and non-list values\n",
    "df_tags = df_tags.groupby(['Decline', 'Source'])['Tags'].apply(\n",
    "    lambda x: list(set([item for sublist in x.dropna() for item in (sublist if isinstance(sublist, list) else [sublist])]))\n",
    ").reset_index(name='Tags_combined')\n",
    "\n",
    "df_tags.set_index(['Decline', 'Source'], inplace=True)\n",
    "\n",
    "# Map the tags to a string, separating them by new lines\n",
    "df_tags['Tags_combined'] = df_tags['Tags_combined'].map(lambda tags: '\\n'.join(tags) if tags else None)\n",
    "\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "CASEFOLD = False\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_str(s):\n",
    "    if not isinstance(s, str) or not s.strip(): # Cases where s = None\n",
    "        return []\n",
    "    tokens = word_tokenize(s.lower() if CASEFOLD else s, preserve_line=True)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and lemmatizing tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61194/61194 [27:20<00:00, 37.30it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary and corpus\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Create a dictionary and a corpus for the LDA model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating dictionary and corpus\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(df_tags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokens\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     25\u001b[0m corpus \u001b[38;5;241m=\u001b[39m [dictionary\u001b[38;5;241m.\u001b[39mdoc2bow(token_list) \u001b[38;5;28;01mfor\u001b[39;00m token_list \u001b[38;5;129;01min\u001b[39;00m df_tags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokens\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining LDA model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\eva\\anaconda3\\envs\\ada\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:78\u001b[0m, in \u001b[0;36mDictionary.__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_nnz \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_documents(documents, prune_at\u001b[38;5;241m=\u001b[39mprune_at)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m         msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     82\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\eva\\anaconda3\\envs\\ada\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:204\u001b[0m, in \u001b[0;36mDictionary.add_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    201\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madding document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, docno, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# update Dictionary with the document\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc2bow(document, allow_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# ignore the result, here we only care about updating token ids\u001b[39;00m\n\u001b[0;32m    206\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilt \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m documents (total \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m corpus positions)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_docs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pos)\n",
      "File \u001b[1;32mc:\\Users\\eva\\anaconda3\\envs\\ada\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:246\u001b[0m, in \u001b[0;36mDictionary.doc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    244\u001b[0m counter \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m document:\n\u001b[1;32m--> 246\u001b[0m     counter[w \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    248\u001b[0m token2id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken2id\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_update \u001b[38;5;129;01mor\u001b[39;00m return_missing:\n",
      "\u001b[1;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, list found"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing and lemmatizing tags\")\n",
    "df_tags['Tokens'] = None\n",
    "for index, row in tqdm(df_tags.iterrows(), total=df_tags.shape[0]):\n",
    "    df_tags.at[index, 'Tokens'] = preprocess_str(row['Tags_combined'])\n",
    "\n",
    "\n",
    "# Create a dictionary and a corpus for the LDA model\n",
    "print(\"Creating dictionary and corpus\")\n",
    "dictionary = corpora.Dictionary(df_tags['Tokens'])\n",
    "corpus = [dictionary.doc2bow(token_list) for token_list in df_tags['Tokens']]\n",
    "\n",
    "print(\"Training LDA model\")\n",
    "lda = ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = lda.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decline  Source\n",
      "0        After     [M, R, e, B, e, ,, 2, 1, b, r, h, ,, b, r, h, ...\n",
      "         Before    [M, R, e, B, e, ,, u, n, w, r, k, ,, u, n, g, ...\n",
      "1        After     [h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...\n",
      "         Before    [h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...\n",
      "2        After                                                    []\n",
      "                                         ...                        \n",
      "36595    Before    [D, e, p, c, c, c, r, n, c, v, e, r, ,, F, n, ...\n",
      "36597    After     [S, h, p, e, r, ,, C, l, p, p, e, r, ,, K, e, ...\n",
      "         Before    [B, r, g, e, p, r, ,, S, n, l, e, S, e, e, l, ...\n",
      "36598    After     [M, u, c, ,, b, e, ,, n, r, u, e, n, l, ,, r, ...\n",
      "         Before    [M, u, c, ,, b, e, ,, n, r, u, e, n, l, ,, r, ...\n",
      "Name: Tokens, Length: 61194, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print(df_tags['Tokens'].head(10))\n",
    "# Replace None or NaN in Tokens with empty lists\n",
    "df_tags['Tokens'] = df_tags['Tokens'].apply(\n",
    "    lambda x: [] if x is None else x\n",
    ")\n",
    "\n",
    "\n",
    "# Flatten any nested lists in Tokens\n",
    "df_tags['Tokens'] = df_tags['Tokens'].apply(\n",
    "    lambda tokens: [item for sublist in tokens for item in sublist] if any(isinstance(i, list) for i in tokens) else tokens\n",
    "    if isinstance(tokens, list) else []\n",
    ")\n",
    "\n",
    "# Check the cleaned Tokens column\n",
    "print(df_tags['Tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokens\n",
       "<class 'list'>        57519\n",
       "<class 'NoneType'>     3675\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tags['Tokens'].apply(type).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary and corpus\n",
      "Training LDA model\n",
      "(0, '0.071*\",\" + 0.047*\"а\" + 0.043*\"и\" + 0.040*\"о\" + 0.035*\"е\"')\n",
      "(1, '0.117*\",\" + 0.112*\"e\" + 0.069*\"n\" + 0.068*\"r\" + 0.043*\"l\"')\n",
      "(2, '0.132*\"1\" + 0.112*\"2\" + 0.091*\"0\" + 0.068*\",\" + 0.058*\"e\"')\n",
      "(3, '0.097*\"E\" + 0.090*\"A\" + 0.075*\"I\" + 0.070*\"O\" + 0.069*\",\"')\n",
      "(4, '0.165*\"e\" + 0.134*\",\" + 0.107*\"n\" + 0.107*\"r\" + 0.084*\"l\"')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tags_combined</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>After</th>\n",
       "      <td>MsRosieBea,21st birthday,birthday,ring,jewelle...</td>\n",
       "      <td>[M, R, e, B, e, ,, 2, 1, b, r, h, ,, b, r, h, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>MsRosieBea,uni work,studying fashion design,fa...</td>\n",
       "      <td>[M, R, e, B, e, ,, u, n, w, r, k, ,, u, n, g, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>After</th>\n",
       "      <td>hollow,generationhollow,playthrough,blind play...</td>\n",
       "      <td>[h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>hollow,generationhollow,playthrough,blind play...</td>\n",
       "      <td>[h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>After</th>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36595</th>\n",
       "      <th>Before</th>\n",
       "      <td>Despacito accordion cover,Fonsi Despacito acco...</td>\n",
       "      <td>[D, e, p, c, c, c, r, n, c, v, e, r, ,, F, n, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">36597</th>\n",
       "      <th>After</th>\n",
       "      <td>Shaper,Clapper,Keith Fenner,Fenner,machine sho...</td>\n",
       "      <td>[S, h, p, e, r, ,, C, l, p, p, e, r, ,, K, e, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>Bridgeport,Stainless Steel Placards,Roller Kit...</td>\n",
       "      <td>[B, r, g, e, p, r, ,, S, n, l, e, S, e, e, l, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">36598</th>\n",
       "      <th>After</th>\n",
       "      <td>Music,beats,instrumental,right beat radio,minn...</td>\n",
       "      <td>[M, u, c, ,, b, e, ,, n, r, u, e, n, l, ,, r, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>Music,beats,instrumental,right beat radio,lofi...</td>\n",
       "      <td>[M, u, c, ,, b, e, ,, n, r, u, e, n, l, ,, r, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61194 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Tags_combined  \\\n",
       "Decline Source                                                      \n",
       "0       After   MsRosieBea,21st birthday,birthday,ring,jewelle...   \n",
       "        Before  MsRosieBea,uni work,studying fashion design,fa...   \n",
       "1       After   hollow,generationhollow,playthrough,blind play...   \n",
       "        Before  hollow,generationhollow,playthrough,blind play...   \n",
       "2       After                                                None   \n",
       "...                                                           ...   \n",
       "36595   Before  Despacito accordion cover,Fonsi Despacito acco...   \n",
       "36597   After   Shaper,Clapper,Keith Fenner,Fenner,machine sho...   \n",
       "        Before  Bridgeport,Stainless Steel Placards,Roller Kit...   \n",
       "36598   After   Music,beats,instrumental,right beat radio,minn...   \n",
       "        Before  Music,beats,instrumental,right beat radio,lofi...   \n",
       "\n",
       "                                                           Tokens  \n",
       "Decline Source                                                     \n",
       "0       After   [M, R, e, B, e, ,, 2, 1, b, r, h, ,, b, r, h, ...  \n",
       "        Before  [M, R, e, B, e, ,, u, n, w, r, k, ,, u, n, g, ...  \n",
       "1       After   [h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...  \n",
       "        Before  [h, l, l, w, ,, g, e, n, e, r, n, h, l, l, w, ...  \n",
       "2       After                                                  []  \n",
       "...                                                           ...  \n",
       "36595   Before  [D, e, p, c, c, c, r, n, c, v, e, r, ,, F, n, ...  \n",
       "36597   After   [S, h, p, e, r, ,, C, l, p, p, e, r, ,, K, e, ...  \n",
       "        Before  [B, r, g, e, p, r, ,, S, n, l, e, S, e, e, l, ...  \n",
       "36598   After   [M, u, c, ,, b, e, ,, n, r, u, e, n, l, ,, r, ...  \n",
       "        Before  [M, u, c, ,, b, e, ,, n, r, u, e, n, l, ,, r, ...  \n",
       "\n",
       "[61194 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary and a corpus for the LDA model\n",
    "print(\"Creating dictionary and corpus\")\n",
    "dictionary = corpora.Dictionary(df_tags['Tokens'])\n",
    "corpus = [dictionary.doc2bow(token_list) for token_list in df_tags['Tokens']]\n",
    "\n",
    "print(\"Training LDA model\")\n",
    "lda = ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = lda.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "df_small = df_tags.head(100)\n",
    "print(df_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eva\\AppData\\Local\\Temp\\ipykernel_22020\\1432068147.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_small['Tokens'] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and lemmatizing tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 211.45it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing and lemmatizing tags\")\n",
    "df_small['Tokens'] = None\n",
    "for index, row in tqdm(df_small.iterrows(), total=df_small.shape[0]):\n",
    "    df_small.at[index, 'Tokens'] = preprocess_str(row['Tags_combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary and corpus\n",
      "Training LDA model\n",
      "(17, '0.090*\"Fortnite\" + 0.063*\"fortnite\" + 0.032*\"pokemon\" + 0.029*\"Ninja\" + 0.020*\"voice\" + 0.020*\"real\" + 0.015*\"shiny\" + 0.014*\"montage\" + 0.013*\"challenge\"')\n",
      "(16, '0.000*\"BLACKPINK\" + 0.000*\"DDU\" + 0.000*\"DU\" + 0.000*\"SQUARE\" + 0.000*\"UP\" + 0.000*\"brexit\" + 0.000*\"WW2\" + 0.000*\"블랙핑크\" + 0.000*\"Call\"')\n",
      "(54, '0.034*\"tip\" + 0.026*\"guide\" + 0.024*\"hollow\" + 0.023*\"generationhollow\" + 0.021*\"tutorial\" + 0.020*\"gameplay\" + 0.016*\"trick\" + 0.016*\"playthrough\" + 0.016*\"top\"')\n",
      "(10, '0.049*\"reaction\" + 0.033*\"kpop\" + 0.017*\"BLACKPINK\" + 0.017*\"블랙핑크\" + 0.017*\"blackpink\" + 0.017*\"Shane\" + 0.017*\"shane\" + 0.011*\"house\" + 0.011*\"FANSIGN\"')\n",
      "(32, '0.020*\"impulse\" + 0.007*\"j\" + 0.007*\"cole\" + 0.007*\"drake\" + 0.007*\"prod\" + 0.007*\"uzi\" + 0.007*\"migos\" + 0.007*\"impulsebeats\" + 0.007*\"beatz\"')\n",
      "(33, '0.058*\"brexit\" + 0.033*\"james\" + 0.032*\"o\\'brien\" + 0.021*\"Brexit\" + 0.019*\"uk\" + 0.017*\"post\" + 0.017*\"trump\" + 0.014*\"caller\" + 0.013*\"Johnson\"')\n",
      "(45, '0.016*\"new\" + 0.016*\"model\" + 0.016*\"york\" + 0.016*\"gay\" + 0.011*\"girl\" + 0.011*\"ava\" + 0.011*\"city\" + 0.011*\"modeling\" + 0.011*\"fit\"')\n",
      "(13, '0.031*\"mash\" + 0.029*\"star\" + 0.022*\"spaceballs\" + 0.022*\"deadpool\" + 0.019*\"wayne\" + 0.019*\"trek\" + 0.016*\"war\" + 0.016*\"barf\" + 0.015*\"x\"')\n",
      "(25, '0.007*\"Indivisible\" + 0.007*\"Jon\" + 0.007*\"Manhattan\" + 0.007*\"odette.pavlova\" + 0.007*\"Choice\" + 0.007*\"Bulgari\" + 0.007*\"Rose.\" + 0.007*\"designer\" + 0.007*\"Pure\"')\n",
      "(41, '0.042*\"horse\" + 0.025*\"toy\" + 0.020*\"stormy\" + 0.018*\"strike\" + 0.017*\"model\" + 0.017*\"pony\" + 0.016*\"kid\" + 0.015*\"family\" + 0.013*\"Spirit\"')\n",
      "(12, '0.001*\"fortnite\" + 0.000*\"gameplay\" + 0.000*\"game\" + 0.000*\"skin\" + 0.000*\"android\" + 0.000*\"pc\" + 0.000*\"ps4\" + 0.000*\"samsung\" + 0.000*\"fun\"')\n",
      "(52, '0.073*\"eye\" + 0.072*\"makeup\" + 0.039*\"beauty\" + 0.038*\"cat\" + 0.034*\"tutorial\" + 0.027*\"smokey\" + 0.027*\"ready\" + 0.027*\"get\" + 0.023*\"liner\"')\n",
      "(46, '0.071*\"Games\" + 0.058*\"Video\" + 0.028*\"Game\" + 0.028*\"Gaming\" + 0.028*\"Rerez\" + 0.025*\"Gamers\" + 0.022*\"Bad\" + 0.018*\"Review\" + 0.016*\"Nintendo\"')\n",
      "(31, '0.028*\"whats\" + 0.028*\"inside\" + 0.026*\"hello\" + 0.026*\"tutorial\" + 0.026*\"DIY\" + 0.026*\"beauty\" + 0.026*\"decoden\" + 0.026*\"kawaii\" + 0.026*\"polymer\"')\n",
      "(36, '0.032*\"urdu\" + 0.023*\"youtube\" + 0.022*\"opener\" + 0.020*\"illuminati\" + 0.018*\"hindi\" + 0.018*\"tv\" + 0.017*\"time\" + 0.017*\"space\" + 0.014*\"poem\"')\n",
      "(0, '0.054*\"game\" + 0.051*\"video\" + 0.051*\"Continue\" + 0.048*\"let\" + 0.027*\"episode\" + 0.027*\"gameplay\" + 0.027*\"funny\" + 0.027*\"play\" + 0.026*\"playthrough\"')\n",
      "(18, '0.001*\"van\" + 0.001*\"life\" + 0.001*\"Wander\" + 0.000*\"Corbin\" + 0.000*\"baby\" + 0.000*\"pregnancy\" + 0.000*\"Kelsey\" + 0.000*\"house\" + 0.000*\"new\"')\n",
      "(27, '0.033*\"Sega\" + 0.030*\"Games\" + 0.028*\"Game\" + 0.026*\"Gaming\" + 0.026*\"Review\" + 0.026*\"Boy\" + 0.023*\"Video\" + 0.023*\"Reviews\" + 0.023*\"Rerez\"')\n",
      "(21, '0.050*\"best\" + 0.046*\"moment\" + 0.044*\"funny\" + 0.023*\"base\" + 0.023*\"survival\" + 0.022*\"series\" + 0.021*\"solo\" + 0.020*\"loot\" + 0.019*\"stimpee\"')\n",
      "(42, '0.049*\"Fortnite\" + 0.035*\"fortnite\" + 0.025*\"moment\" + 0.023*\"Ninja\" + 0.022*\"\\'s\" + 0.022*\"real\" + 0.021*\"voice\" + 0.019*\"America\" + 0.019*\"talent\"')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tags_combined</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>After</th>\n",
       "      <td>MsRosieBea,21st birthday,birthday,ring,jewelle...</td>\n",
       "      <td>[MsRosieBea,21st, birthday, birthday, ring, je...</td>\n",
       "      <td>35</td>\n",
       "      <td>0.399864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>MsRosieBea,uni work,studying fashion design,fa...</td>\n",
       "      <td>[MsRosieBea, uni, work, studying, fashion, des...</td>\n",
       "      <td>35</td>\n",
       "      <td>0.986550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>After</th>\n",
       "      <td>hollow,generationhollow,playthrough,blind play...</td>\n",
       "      <td>[hollow, generationhollow, playthrough, blind,...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.998292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>hollow,generationhollow,playthrough,blind play...</td>\n",
       "      <td>[hollow, generationhollow, playthrough, blind,...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.996498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>After</th>\n",
       "      <td>dayz,dayz standalone,.62,update,map,loot,inter...</td>\n",
       "      <td>[dayz, dayz, standalone, .62, update, map, loo...</td>\n",
       "      <td>31</td>\n",
       "      <td>0.899641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">55</th>\n",
       "      <th>After</th>\n",
       "      <td>Brexit,Boris Johnson,PM,Boris,no deal,brexitee...</td>\n",
       "      <td>[Brexit, Boris, Johnson, PM, Boris, deal, brex...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>james o'brien,james o'brien brexit,brexit disa...</td>\n",
       "      <td>[james, o'brien, james, o'brien, brexit, brexi...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.999397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">57</th>\n",
       "      <th>After</th>\n",
       "      <td>base de rap,pista de rap,hip hop instrumental,...</td>\n",
       "      <td>[base, de, rap, pista, de, rap, hip, hop, inst...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.997112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>witch house type beat,A$AP rocky type,Suicide ...</td>\n",
       "      <td>[witch, house, type, beat, A, AP, rocky, type,...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.997988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <th>After</th>\n",
       "      <td>plan with me,erin condren,life planner,2019,20...</td>\n",
       "      <td>[plan, erin, condren, life, planner,2019,2020,...</td>\n",
       "      <td>44</td>\n",
       "      <td>0.988968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Tags_combined  \\\n",
       "Decline Source                                                      \n",
       "0       After   MsRosieBea,21st birthday,birthday,ring,jewelle...   \n",
       "        Before  MsRosieBea,uni work,studying fashion design,fa...   \n",
       "1       After   hollow,generationhollow,playthrough,blind play...   \n",
       "        Before  hollow,generationhollow,playthrough,blind play...   \n",
       "3       After   dayz,dayz standalone,.62,update,map,loot,inter...   \n",
       "...                                                           ...   \n",
       "55      After   Brexit,Boris Johnson,PM,Boris,no deal,brexitee...   \n",
       "        Before  james o'brien,james o'brien brexit,brexit disa...   \n",
       "57      After   base de rap,pista de rap,hip hop instrumental,...   \n",
       "        Before  witch house type beat,A$AP rocky type,Suicide ...   \n",
       "58      After   plan with me,erin condren,life planner,2019,20...   \n",
       "\n",
       "                                                           Tokens  \\\n",
       "Decline Source                                                      \n",
       "0       After   [MsRosieBea,21st, birthday, birthday, ring, je...   \n",
       "        Before  [MsRosieBea, uni, work, studying, fashion, des...   \n",
       "1       After   [hollow, generationhollow, playthrough, blind,...   \n",
       "        Before  [hollow, generationhollow, playthrough, blind,...   \n",
       "3       After   [dayz, dayz, standalone, .62, update, map, loo...   \n",
       "...                                                           ...   \n",
       "55      After   [Brexit, Boris, Johnson, PM, Boris, deal, brex...   \n",
       "        Before  [james, o'brien, james, o'brien, brexit, brexi...   \n",
       "57      After   [base, de, rap, pista, de, rap, hip, hop, inst...   \n",
       "        Before  [witch, house, type, beat, A, AP, rocky, type,...   \n",
       "58      After   [plan, erin, condren, life, planner,2019,2020,...   \n",
       "\n",
       "                Dominant_Topic  Topic_Probability  \n",
       "Decline Source                                     \n",
       "0       After               35           0.399864  \n",
       "        Before              35           0.986550  \n",
       "1       After               17           0.998292  \n",
       "        Before              17           0.996498  \n",
       "3       After               31           0.899641  \n",
       "...                        ...                ...  \n",
       "55      After                6           0.999234  \n",
       "        Before               6           0.999397  \n",
       "57      After               16           0.997112  \n",
       "        Before              16           0.997988  \n",
       "58      After               44           0.988968  \n",
       "\n",
       "[86 rows x 4 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a dictionary and a corpus for the LDA model\n",
    "print(\"Creating dictionary and corpus\")\n",
    "dictionary = corpora.Dictionary(df_small['Tokens'])\n",
    "corpus = [dictionary.doc2bow(token_list) for token_list in df_small['Tokens']]\n",
    "\n",
    "print(\"Training LDA model\")\n",
    "lda = ldamodel.LdaModel(corpus, num_topics=55, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = lda.print_topics(num_words=9)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning topics to each document\n",
      "                                                    Tags_combined  \\\n",
      "Decline Source                                                      \n",
      "0       After   MsRosieBea,21st birthday,birthday,ring,jewelle...   \n",
      "        Before  MsRosieBea,uni work,studying fashion design,fa...   \n",
      "1       After   hollow,generationhollow,playthrough,blind play...   \n",
      "        Before  hollow,generationhollow,playthrough,blind play...   \n",
      "3       After   dayz,dayz standalone,.62,update,map,loot,inter...   \n",
      "        Before  dayz,dayz standalone,.62,update,map,loot,inter...   \n",
      "4       After   Halloween,spooky,scary,creepy,Bogeyman,Breyer,...   \n",
      "        Before  HoneyheartsC,MyFroggyStuff,Infinity Breyers,da...   \n",
      "5       After   Triple Entray,Phora,Drake,Eminem,Justin Bieber...   \n",
      "        Before  Hip hop,Triple Entray,Drake,Eminem,Logic,Phora...   \n",
      "7       After   Yasha,Yasha Jeltuhin,Cyr,Cyr Wheel,Circus,Akro...   \n",
      "        Before  Yasha,Yasha Jeltuhin,Akrosphere,Circus,Jen Mac...   \n",
      "8       After   bruce lee,asian trap beat,chinese trap beat,88...   \n",
      "        Before  impulsebeats,yung impulse,impulse beats,impuls...   \n",
      "10      After   Family vlogs,daily vlogs,toddler,cute couple,n...   \n",
      "        Before  emergency,tornado,driving through a tornado,ca...   \n",
      "11      After   pharmit,pharmit24,malaysia,awesome,gamer,youtu...   \n",
      "        Before  pharmit,pharmit24,malaysia,awesome,gamer,youtu...   \n",
      "12      After   twintalksballet,twin,twins,talk,talks,ballet,b...   \n",
      "        Before  twintalksballet,twin,twins,talk,talks,ballet,b...   \n",
      "\n",
      "                                                           Tokens  \\\n",
      "Decline Source                                                      \n",
      "0       After   [MsRosieBea,21st, birthday, birthday, ring, je...   \n",
      "        Before  [MsRosieBea, uni, work, studying, fashion, des...   \n",
      "1       After   [hollow, generationhollow, playthrough, blind,...   \n",
      "        Before  [hollow, generationhollow, playthrough, blind,...   \n",
      "3       After   [dayz, dayz, standalone, .62, update, map, loo...   \n",
      "        Before  [dayz, dayz, standalone, .62, update, map, loo...   \n",
      "4       After   [Halloween, spooky, scary, creepy, Bogeyman, B...   \n",
      "        Before  [HoneyheartsC, MyFroggyStuff, Infinity, Breyer...   \n",
      "5       After   [Triple, Entray, Phora, Drake, Eminem, Justin,...   \n",
      "        Before  [Hip, hop, Triple, Entray, Drake, Eminem, Logi...   \n",
      "7       After   [Yasha, Yasha, Jeltuhin, Cyr, Cyr, Wheel, Circ...   \n",
      "        Before  [Yasha, Yasha, Jeltuhin, Akrosphere, Circus, J...   \n",
      "8       After   [bruce, lee, asian, trap, beat, chinese, trap,...   \n",
      "        Before  [impulsebeats, yung, impulse, impulse, beat, i...   \n",
      "10      After   [Family, vlogs, daily, vlogs, toddler, cute, c...   \n",
      "        Before  [emergency, tornado, driving, tornado, car, v,...   \n",
      "11      After   [pharmit, pharmit24, malaysia, awesome, gamer,...   \n",
      "        Before  [pharmit, pharmit24, malaysia, awesome, gamer,...   \n",
      "12      After   [twintalksballet, twin, twin, talk, talk, ball...   \n",
      "        Before  [twintalksballet, twin, twin, talk, talk, ball...   \n",
      "\n",
      "                Dominant_Topic  Topic_Probability  \n",
      "Decline Source                                     \n",
      "0       After               40           0.960726  \n",
      "        Before              33           0.758448  \n",
      "1       After               54           0.713057  \n",
      "        Before              54           0.963160  \n",
      "3       After               21           0.999240  \n",
      "        Before              21           0.999116  \n",
      "4       After               35           0.919800  \n",
      "        Before              41           0.999340  \n",
      "5       After               36           0.601828  \n",
      "        Before              36           0.987081  \n",
      "7       After               43           0.968326  \n",
      "        Before              29           0.945451  \n",
      "8       After               19           0.993182  \n",
      "        Before              19           0.542273  \n",
      "10      After               11           0.999820  \n",
      "        Before              11           0.999817  \n",
      "11      After                6           0.974626  \n",
      "        Before               6           0.999218  \n",
      "12      After               53           0.639136  \n",
      "        Before              53           0.997409  \n"
     ]
    }
   ],
   "source": [
    "print(\"Assigning topics to each document\")\n",
    "\n",
    "# Assign the dominant topic to each document\n",
    "def assign_dominant_topic(tokens, lda_model, dictionary):\n",
    "    if not tokens or not isinstance(tokens, list):  # Handle empty or invalid tokens\n",
    "        return None, None\n",
    "    bow = dictionary.doc2bow(tokens)  # Convert tokens to bag-of-words format\n",
    "    topic_probs = lda_model.get_document_topics(bow)  # Get topic distribution\n",
    "    if topic_probs:\n",
    "        dominant_topic, prob = max(topic_probs, key=lambda x: x[1])  # Most probable topic\n",
    "        return dominant_topic, prob\n",
    "    return None, None\n",
    "\n",
    "df_small['Dominant_Topic'], df_small['Topic_Probability'] = zip(\n",
    "    *df_small['Tokens'].apply(lambda tokens: assign_dominant_topic(tokens, lda, dictionary))\n",
    ")\n",
    "\n",
    "print(df_small.head(20))\n",
    "df_small.to_csv('df_small_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create two columns [Topic_change] and [Tokens_change] to determine if there is a difference between the tags before and after a decline. A change in tokens is used for granular analysis while a change is topics is more appropriate for detecting higher-level patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Dominant_Topic</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <th>After</th>\n",
       "      <th>Before</th>\n",
       "      <th>After</th>\n",
       "      <th>Before</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>MsRosieBea,21st birthday birthday ring jewelle...</td>\n",
       "      <td>MsRosieBea uni work studying fashion design fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>hollow generationhollow playthrough blind play...</td>\n",
       "      <td>hollow generationhollow playthrough blind play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>dayz dayz standalone .62 update map loot inter...</td>\n",
       "      <td>dayz dayz standalone .62 update map loot inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Halloween spooky scary creepy Bogeyman Breyer ...</td>\n",
       "      <td>HoneyheartsC MyFroggyStuff Infinity Breyers da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Triple Entray Phora Drake Eminem Justin Bieber...</td>\n",
       "      <td>Hip hop Triple Entray Drake Eminem Logic Phora...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant_Topic         \\\n",
       "Source           After Before   \n",
       "Decline                         \n",
       "0                 40.0   33.0   \n",
       "1                 54.0   54.0   \n",
       "3                 21.0   21.0   \n",
       "4                 35.0   41.0   \n",
       "5                 36.0   36.0   \n",
       "\n",
       "                                                    Tokens  \\\n",
       "Source                                               After   \n",
       "Decline                                                      \n",
       "0        MsRosieBea,21st birthday birthday ring jewelle...   \n",
       "1        hollow generationhollow playthrough blind play...   \n",
       "3        dayz dayz standalone .62 update map loot inter...   \n",
       "4        Halloween spooky scary creepy Bogeyman Breyer ...   \n",
       "5        Triple Entray Phora Drake Eminem Justin Bieber...   \n",
       "\n",
       "                                                            \n",
       "Source                                              Before  \n",
       "Decline                                                     \n",
       "0        MsRosieBea uni work studying fashion design fa...  \n",
       "1        hollow generationhollow playthrough blind play...  \n",
       "3        dayz dayz standalone .62 update map loot inter...  \n",
       "4        HoneyheartsC MyFroggyStuff Infinity Breyers da...  \n",
       "5        Hip hop Triple Entray Drake Eminem Logic Phora...  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small = df_small.dropna(subset=['Tokens', 'Dominant_Topic'])\n",
    "\n",
    "# Pivot the dataset, keeping 'Dominant_topic' in a separate column\n",
    "df_pivot = df_small.pivot_table(\n",
    "    index='Decline',  # The index will be based on the 'Decline'\n",
    "    columns='Source',  # We are splitting by 'Source' (Before and After)\n",
    "    values=['Tokens', 'Dominant_Topic'],  # We want both Tokens and Dominant_topic in the pivoted table\n",
    "    aggfunc={\n",
    "        'Tokens': lambda x: ' '.join([item for sublist in x for item in sublist]),  # Flatten and join the tokens\n",
    "        'Dominant_Topic': lambda x: x.mode()[0]  # Get the most frequent dominant topic (mode)\n",
    "    }\n",
    ")\n",
    "\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Dominant_Topic</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Tokens</th>\n",
       "      <th>Token_Change</th>\n",
       "      <th>Topic_Change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <th>After</th>\n",
       "      <th>Before</th>\n",
       "      <th>After</th>\n",
       "      <th>Before</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>MsRosieBea,21st birthday birthday ring jewelle...</td>\n",
       "      <td>MsRosieBea uni work studying fashion design fa...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>hollow generationhollow playthrough blind play...</td>\n",
       "      <td>hollow generationhollow playthrough blind play...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>dayz dayz standalone .62 update map loot inter...</td>\n",
       "      <td>dayz dayz standalone .62 update map loot inter...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>Halloween spooky scary creepy Bogeyman Breyer ...</td>\n",
       "      <td>HoneyheartsC MyFroggyStuff Infinity Breyers da...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Triple Entray Phora Drake Eminem Justin Bieber...</td>\n",
       "      <td>Hip hop Triple Entray Drake Eminem Logic Phora...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant_Topic         \\\n",
       "Source           After Before   \n",
       "Decline                         \n",
       "0                 40.0   33.0   \n",
       "1                 54.0   54.0   \n",
       "3                 21.0   21.0   \n",
       "4                 35.0   41.0   \n",
       "5                 36.0   36.0   \n",
       "\n",
       "                                                    Tokens  \\\n",
       "Source                                               After   \n",
       "Decline                                                      \n",
       "0        MsRosieBea,21st birthday birthday ring jewelle...   \n",
       "1        hollow generationhollow playthrough blind play...   \n",
       "3        dayz dayz standalone .62 update map loot inter...   \n",
       "4        Halloween spooky scary creepy Bogeyman Breyer ...   \n",
       "5        Triple Entray Phora Drake Eminem Justin Bieber...   \n",
       "\n",
       "                                                           Token_Change  \\\n",
       "Source                                              Before                \n",
       "Decline                                                                   \n",
       "0        MsRosieBea uni work studying fashion design fa...        False   \n",
       "1        hollow generationhollow playthrough blind play...        False   \n",
       "3        dayz dayz standalone .62 update map loot inter...        False   \n",
       "4        HoneyheartsC MyFroggyStuff Infinity Breyers da...        False   \n",
       "5        Hip hop Triple Entray Drake Eminem Logic Phora...        False   \n",
       "\n",
       "        Topic_Change  \n",
       "Source                \n",
       "Decline               \n",
       "0               True  \n",
       "1              False  \n",
       "3              False  \n",
       "4               True  \n",
       "5              False  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_change(tokens_before, tokens_after):\n",
    "    # Ensure tokens are lists and not NaN or float\n",
    "    if not isinstance(tokens_before, list):\n",
    "        tokens_before = []\n",
    "    if not isinstance(tokens_after, list):\n",
    "        tokens_after = []\n",
    "        \n",
    "    # Compare sets of tokens\n",
    "    set_before = set(tokens_before)\n",
    "    set_after = set(tokens_after)\n",
    "    return set_before != set_after  # Change if the sets are not identical\n",
    "\n",
    "# Apply the token change function to compare the tokens before and after for each decline\n",
    "df_pivot['Token_Change'] = df_pivot.apply(\n",
    "    lambda row: token_change(row[('Tokens', 'Before')], row[('Tokens', 'After')]), axis=1)\n",
    "\n",
    "# Assuming 'Dominant_topic' columns are available for 'Before' and 'After'\n",
    "df_pivot['Topic_Change'] = df_pivot.apply(\n",
    "    lambda row: row[('Dominant_Topic', 'Before')] != row[('Dominant_Topic', 'After')], axis=1)\n",
    "\n",
    "# Verify the results\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.7474957968694171\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=df_small['Tokens'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score: {coherence_lda}') # 0.7475 with 55 topics, numwords = 9\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
