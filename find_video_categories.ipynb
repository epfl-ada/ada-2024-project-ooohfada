{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          video,games,retrogamer3,ed,findlay,Scam,Steam,...\n",
       "1          video,games,retrogamer3,ed,findlay,Trump,Ameri...\n",
       "2          video,games,retrogamer3,ed,findlay,America's R...\n",
       "3                                 MTG Arena War of the Spark\n",
       "4          video,games,retrogamer3,ed,findlay,Mpow,Headph...\n",
       "                                 ...                        \n",
       "1905541    BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...\n",
       "1905542    BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...\n",
       "1905543    BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...\n",
       "1905544    BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...\n",
       "1905545    BJP,Bharatiya Janata Party,BJP videos,Yuva TV,...\n",
       "Name: tags, Length: 1905546, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import decomposition\n",
    "import re\n",
    "import string\n",
    "\n",
    "videos_data = pd.read_csv('data/videos_around_declines.csv')\n",
    "\n",
    "video_tags = videos_data['tags'].astype(str)\n",
    "\n",
    "video_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nathangromb/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '00',\n",
       " '00000000000',\n",
       " '000001',\n",
       " '00000000',\n",
       " '000000000',\n",
       " '00000',\n",
       " '0000000',\n",
       " '000000',\n",
       " '0000']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def clean_tag(s):\n",
    "    without_punctuation = re.sub(f'[{re.escape(string.punctuation)}]', '', s.lower().replace(',', ' '))\n",
    "    without_stopwords = \" \".join([word for word in without_punctuation.split() if word not in english_stopwords])\n",
    "    lemmatized = lemmatizer.lemmatize(without_stopwords)\n",
    "    return lemmatized\n",
    "\n",
    "unique_tags = video_tags.apply(lambda x: clean_tag(x)).explode().unique()\n",
    "\n",
    "# Vectorize the tags\n",
    "tfv = TfidfVectorizer()\n",
    "vectorized_tags = tfv.fit_transform(unique_tags)\n",
    "\n",
    "# Perform SVD on the tags to get the topics\n",
    "svd = decomposition.TruncatedSVD(n_components=10)\n",
    "tags_svd = svd.fit_transform(vectorized_tags)\n",
    "\n",
    "# Get the topic scores\n",
    "topic_scores = dict(\n",
    "    zip(\n",
    "        tfv.get_feature_names_out(),\n",
    "        tags_svd[0]\n",
    "    )\n",
    ")\n",
    "\n",
    "topic_output = sorted(\n",
    "    topic_scores, key=topic_scores.get, reverse=True\n",
    ")\n",
    "\n",
    "# Print the topics in order of importance\n",
    "topic_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
