{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import ldamodel\n",
    "from tqdm import tqdm\n",
    "from gensim import corpora\n",
    "from src.utils.recovery_analysis_utils import str_to_list\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "decline_events = pd.read_csv('data/sampled_decline_events_with_videos.csv')\n",
    "videos = pd.read_csv('data/videos_around_declines.csv')\n",
    "\n",
    "decline_events['Videos_before'] = decline_events['Videos_before'].apply(str_to_list)\n",
    "decline_events['Videos_after'] = decline_events['Videos_after'].apply(str_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data_frame with 2 index: the index of the decline and the source (before and after)\n",
    "\n",
    "df_before = decline_events[['Videos_before']].explode('Videos_before')\n",
    "df_before['Source'] = 'Before'\n",
    "df_before = df_before.rename(columns={'Videos_before': 'Video'})\n",
    "\n",
    "df_after = decline_events[['Videos_after']].explode('Videos_after')\n",
    "df_after['Source'] = 'After'\n",
    "df_after = df_after.rename(columns={'Videos_after': 'Video'})\n",
    "\n",
    "df_tags = pd.concat([df_before, df_after], axis=0).reset_index().rename(columns={'index': 'Decline'})\n",
    "df_tags = df_tags.set_index(['Decline', 'Source'])\n",
    "\n",
    "df_tags.sort_values(by = ['Decline', 'Source'])\n",
    "df_tags = df_tags.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Video</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>Before</th>\n",
       "      <td>1684989</td>\n",
       "      <td>MsRosieBea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>1684990</td>\n",
       "      <td>MsRosieBea,primark haul,primark haul august,pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>1684991</td>\n",
       "      <td>MsRosieBea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>1684992</td>\n",
       "      <td>MsRosieBea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>1684993</td>\n",
       "      <td>MsRosieBea,red lip,get ready with me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">36598</th>\n",
       "      <th>After</th>\n",
       "      <td>1889699</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,stra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After</th>\n",
       "      <td>1889700</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,late...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After</th>\n",
       "      <td>1889701</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,lofi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After</th>\n",
       "      <td>1889702</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,mell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>After</th>\n",
       "      <td>1889703</td>\n",
       "      <td>Music,beats,instrumental,right beat radio,lofi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2069978 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Video                                               Tags\n",
       "Decline Source                                                            \n",
       "0       Before  1684989                                         MsRosieBea\n",
       "        Before  1684990  MsRosieBea,primark haul,primark haul august,pr...\n",
       "        Before  1684991                                         MsRosieBea\n",
       "        Before  1684992                                         MsRosieBea\n",
       "        Before  1684993               MsRosieBea,red lip,get ready with me\n",
       "...                 ...                                                ...\n",
       "36598   After   1889699  Music,beats,instrumental,right beat radio,stra...\n",
       "        After   1889700  Music,beats,instrumental,right beat radio,late...\n",
       "        After   1889701  Music,beats,instrumental,right beat radio,lofi...\n",
       "        After   1889702  Music,beats,instrumental,right beat radio,mell...\n",
       "        After   1889703  Music,beats,instrumental,right beat radio,lofi...\n",
       "\n",
       "[2069978 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map to obtain the tags of all videos for each video before and after decline\n",
    "df_tags['Tags'] = df_tags['Video'].map(lambda video: videos.loc[video, 'tags'] if video in videos.index else None)\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tags_combined</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>After</th>\n",
       "      <td>MsRosieBea,uni,uni life,first year of uni,thir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>MsRosieBea,red lip,get ready with me\\nMsRosieB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>After</th>\n",
       "      <td>hollow,generationhollow,tea,questions,qna,answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>hollow,generationhollow,playthrough,blind play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>After</th>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36595</th>\n",
       "      <th>Before</th>\n",
       "      <td>Despacito accordion cover,Fonsi Despacito acco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">36597</th>\n",
       "      <th>After</th>\n",
       "      <td>Babbitt,Babbitt pouring,Keith Fenner,Fenner,ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>Boat Lift,fork lift,Keith Fenner,Fenner,machin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">36598</th>\n",
       "      <th>After</th>\n",
       "      <td>Music,beats,instrumental,right beat radio,acou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>Music,beats,instrumental,right beat radio,acou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61194 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Tags_combined\n",
       "Decline Source                                                   \n",
       "0       After   MsRosieBea,uni,uni life,first year of uni,thir...\n",
       "        Before  MsRosieBea,red lip,get ready with me\\nMsRosieB...\n",
       "1       After   hollow,generationhollow,tea,questions,qna,answ...\n",
       "        Before  hollow,generationhollow,playthrough,blind play...\n",
       "2       After                                                None\n",
       "...                                                           ...\n",
       "36595   Before  Despacito accordion cover,Fonsi Despacito acco...\n",
       "36597   After   Babbitt,Babbitt pouring,Keith Fenner,Fenner,ma...\n",
       "        Before  Boat Lift,fork lift,Keith Fenner,Fenner,machin...\n",
       "36598   After   Music,beats,instrumental,right beat radio,acou...\n",
       "        Before  Music,beats,instrumental,right beat radio,acou...\n",
       "\n",
       "[61194 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get for each decline only 2 rows with the tags corresponding to the before and the after, handling NaNs and non-list values\n",
    "df_tags = df_tags.groupby(['Decline', 'Source'])['Tags'].apply(\n",
    "    lambda x: list(set([item for sublist in x.dropna() for item in (sublist if isinstance(sublist, list) else [sublist])]))\n",
    ").reset_index(name='Tags_combined')\n",
    "\n",
    "df_tags.set_index(['Decline', 'Source'], inplace=True)\n",
    "\n",
    "# Map the tags to a string, separating them by new lines\n",
    "df_tags['Tags_combined'] = df_tags['Tags_combined'].map(lambda tags: '\\n'.join(tags) if tags else None)\n",
    "\n",
    "df_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "CASEFOLD = False\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_str(s):\n",
    "    s = s.lower()\n",
    "    if not isinstance(s, str) or not s.strip(): # Cases where s = None\n",
    "        return []\n",
    "    tokens = word_tokenize(s.lower() if CASEFOLD else s, preserve_line=True)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "df_small = df_tags.head(100)\n",
    "print(df_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eva\\AppData\\Local\\Temp\\ipykernel_42928\\1432068147.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_small['Tokens'] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and lemmatizing tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [00:01<00:00, 53.03it/s] \n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing and lemmatizing tags\")\n",
    "df_small['Tokens'] = None\n",
    "for index, row in tqdm(df_small.iterrows(), total=df_small.shape[0]):\n",
    "    df_small.at[index, 'Tokens'] = preprocess_str(row['Tags_combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary and corpus\n",
      "Training LDA model\n",
      "(14, '0.001*\"fortnite\" + 0.001*\"game\" + 0.001*\"wander\" + 0.001*\"video\" + 0.001*\"continue\" + 0.001*\"let\" + 0.000*\"kelsey\" + 0.000*\"van\" + 0.000*\"new\"')\n",
      "(32, '0.050*\"best\" + 0.046*\"moment\" + 0.045*\"funny\" + 0.023*\"base\" + 0.023*\"survival\" + 0.022*\"series\" + 0.021*\"solo\" + 0.021*\"loot\" + 0.019*\"rust\"')\n",
      "(33, '0.023*\"v\" + 0.019*\"flash\" + 0.015*\"superman\" + 0.015*\"new\" + 0.014*\"quicksilver\" + 0.012*\"holiday\" + 0.012*\"goku\" + 0.012*\"52\" + 0.012*\"mcu\"')\n",
      "(45, '0.016*\"zoe\" + 0.009*\"raven\" + 0.008*\"mythology\" + 0.008*\"dead\" + 0.008*\"complete\" + 0.001*\"horse\" + 0.000*\"stormy\" + 0.000*\"toy\" + 0.000*\"fortnite\"')\n",
      "(27, '0.216*\"fortnite\" + 0.098*\"skin\" + 0.043*\"free\" + 0.038*\"new\" + 0.034*\"season\" + 0.027*\"battle\" + 0.021*\"gifting\" + 0.017*\"trooper\" + 0.016*\"7\"')\n",
      "(22, '0.001*\"wander\" + 0.001*\"kelsey\" + 0.001*\"brexit\" + 0.000*\"corbin\" + 0.000*\"o\\'brien\" + 0.000*\"van\" + 0.000*\"life\" + 0.000*\"james\" + 0.000*\"step\"')\n",
      "(18, '0.045*\"make-up\" + 0.033*\"skin\" + 0.033*\"beauty\" + 0.033*\"simple\" + 0.033*\"sensitive\" + 0.020*\"cleansing\" + 0.013*\"remover\" + 0.013*\"eye\" + 0.013*\"oil\"')\n",
      "(51, '0.051*\"twin\" + 0.050*\"dancer\" + 0.048*\"talk\" + 0.047*\"ballerina\" + 0.028*\"ballet\" + 0.026*\"twintalksballet\" + 0.025*\"dance\" + 0.025*\"kirsten\" + 0.019*\"tip\"')\n",
      "(28, '0.018*\"fortnite\" + 0.018*\"fortnitevbucks\" + 0.016*\"fortniteglitch\" + 0.014*\"4\" + 0.013*\"tfue\" + 0.012*\"ps4live\" + 0.012*\"entertainment\" + 0.012*\"interactive\" + 0.012*\"sony\"')\n",
      "(12, '0.050*\"gameplay\" + 0.046*\"android\" + 0.037*\"game\" + 0.036*\"pc\" + 0.029*\"samsung\" + 0.029*\"pharmit24\" + 0.026*\"surfer\" + 0.026*\"s8+\" + 0.026*\"subway\"')\n",
      "(54, '0.002*\"fortnite\" + 0.001*\"skin\" + 0.001*\"new\" + 0.000*\"free\" + 0.000*\"battle\" + 0.000*\"season\" + 0.000*\"gifting\" + 0.000*\"best\" + 0.000*\"funny\"')\n",
      "(46, '0.024*\"dylangous\" + 0.019*\"book\" + 0.019*\"asmr\" + 0.019*\"existentialdelight\" + 0.019*\"philosophy\" + 0.019*\"esoteric\" + 0.014*\"music\" + 0.014*\"synthreading\" + 0.014*\"synthwave\"')\n",
      "(37, '0.063*\"yasha\" + 0.038*\"daniela\" + 0.031*\"cyr\" + 0.025*\"viva\" + 0.025*\"jeltuhin\" + 0.025*\"akrosphere\" + 0.019*\"circus\" + 0.019*\"wheel\" + 0.013*\"fest\"')\n",
      "(2, '0.086*\"fortnite\" + 0.029*\"moment\" + 0.028*\"ninja\" + 0.026*\"voice\" + 0.026*\"funny\" + 0.025*\"real\" + 0.021*\"\\'s\" + 0.019*\"got\" + 0.016*\"america\"')\n",
      "(39, '0.047*\"watch\" + 0.041*\"bvlgari\" + 0.037*\"luxury\" + 0.037*\"bulgari\" + 0.031*\"jewellery\" + 0.029*\"jewel\" + 0.029*\"jewelry\" + 0.029*\"gioielli\" + 0.027*\"best\"')\n",
      "(43, '0.001*\"wander\" + 0.001*\"kelsey\" + 0.001*\"corbin\" + 0.001*\"van\" + 0.001*\"life\" + 0.001*\"step\" + 0.001*\"baby\" + 0.001*\"family\" + 0.001*\"new\"')\n",
      "(8, '0.059*\"blackpink\" + 0.041*\"du\" + 0.041*\"ddu\" + 0.023*\"reaction\" + 0.022*\"블랙핑크\" + 0.022*\"square\" + 0.021*\"comic\" + 0.016*\"halloween\" + 0.014*\"kpop\"')\n",
      "(42, '0.089*\"jjmacedo\" + 0.089*\"art\" + 0.049*\"ballad\" + 0.027*\"relax\" + 0.026*\"rock\" + 0.021*\"baladas\" + 0.020*\"bliss\" + 0.018*\"braga\" + 0.015*\"portugal\"')\n",
      "(15, '0.150*\"beat\" + 0.083*\"type\" + 0.055*\"instrumental\" + 0.053*\"rap\" + 0.053*\"trap\" + 0.034*\"free\" + 0.024*\"de\" + 0.024*\"dark\" + 0.023*\"hip\"')\n",
      "(19, '0.104*\"teacher\" + 0.048*\"classroom\" + 0.033*\"first\" + 0.029*\"fridge\" + 0.026*\"tip\" + 0.025*\"year\" + 0.020*\"vlog\" + 0.019*\"instagram\" + 0.016*\"book\"')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Tags_combined</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th>Source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>After</th>\n",
       "      <td>MsRosieBea,uni,uni life,first year of uni,thir...</td>\n",
       "      <td>[msrosiebea, uni, uni, life, first, year, uni,...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.960726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>MsRosieBea,red lip,get ready with me\\nMsRosieB...</td>\n",
       "      <td>[msrosiebea, red, lip, get, ready, msrosiebea,...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.986549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>After</th>\n",
       "      <td>hollow,generationhollow,tea,questions,qna,answ...</td>\n",
       "      <td>[hollow, generationhollow, tea, question, qna,...</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.987197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>hollow,generationhollow,playthrough,blind play...</td>\n",
       "      <td>[hollow, generationhollow, playthrough, blind,...</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.971890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>After</th>\n",
       "      <td>dayz,loot,betrayal,solo,friends,survive,surviv...</td>\n",
       "      <td>[dayz, loot, betrayal, solo, friend, survive, ...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.574609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">55</th>\n",
       "      <th>After</th>\n",
       "      <td>dhs,thomas homan,donald Trump,president trump,...</td>\n",
       "      <td>[dhs, thomas, homan, donald, trump, president,...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.998405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>james o'brien,theresa may,pm may,james o'brien...</td>\n",
       "      <td>[james, o'brien, theresa, may, pm, may, james,...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.999397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">57</th>\n",
       "      <th>After</th>\n",
       "      <td>SUICIDE BOYS TYPE BEAT,$uicide boy$ type,night...</td>\n",
       "      <td>[suicide, boy, type, beat, uicide, boy, type, ...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.637635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Before</th>\n",
       "      <td>sad trap beat,sad trap instrumental,goodbye,sa...</td>\n",
       "      <td>[sad, trap, beat, sad, trap, instrumental, goo...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.993759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <th>After</th>\n",
       "      <td>planner,planning,plan with me,2018-2019,2018,2...</td>\n",
       "      <td>[planner, planning, plan, me,2018-2019,2018,20...</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.988967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Tags_combined  \\\n",
       "Decline Source                                                      \n",
       "0       After   MsRosieBea,uni,uni life,first year of uni,thir...   \n",
       "        Before  MsRosieBea,red lip,get ready with me\\nMsRosieB...   \n",
       "1       After   hollow,generationhollow,tea,questions,qna,answ...   \n",
       "        Before  hollow,generationhollow,playthrough,blind play...   \n",
       "3       After   dayz,loot,betrayal,solo,friends,survive,surviv...   \n",
       "...                                                           ...   \n",
       "55      After   dhs,thomas homan,donald Trump,president trump,...   \n",
       "        Before  james o'brien,theresa may,pm may,james o'brien...   \n",
       "57      After   SUICIDE BOYS TYPE BEAT,$uicide boy$ type,night...   \n",
       "        Before  sad trap beat,sad trap instrumental,goodbye,sa...   \n",
       "58      After   planner,planning,plan with me,2018-2019,2018,2...   \n",
       "\n",
       "                                                           Tokens  \\\n",
       "Decline Source                                                      \n",
       "0       After   [msrosiebea, uni, uni, life, first, year, uni,...   \n",
       "        Before  [msrosiebea, red, lip, get, ready, msrosiebea,...   \n",
       "1       After   [hollow, generationhollow, tea, question, qna,...   \n",
       "        Before  [hollow, generationhollow, playthrough, blind,...   \n",
       "3       After   [dayz, loot, betrayal, solo, friend, survive, ...   \n",
       "...                                                           ...   \n",
       "55      After   [dhs, thomas, homan, donald, trump, president,...   \n",
       "        Before  [james, o'brien, theresa, may, pm, may, james,...   \n",
       "57      After   [suicide, boy, type, beat, uicide, boy, type, ...   \n",
       "        Before  [sad, trap, beat, sad, trap, instrumental, goo...   \n",
       "58      After   [planner, planning, plan, me,2018-2019,2018,20...   \n",
       "\n",
       "                Dominant_Topic  Topic_Probability  \n",
       "Decline Source                                     \n",
       "0       After             42.0           0.960726  \n",
       "        Before            23.0           0.986549  \n",
       "1       After             36.0           0.987197  \n",
       "        Before            36.0           0.971890  \n",
       "3       After             23.0           0.574609  \n",
       "...                        ...                ...  \n",
       "55      After             33.0           0.998405  \n",
       "        Before            33.0           0.999397  \n",
       "57      After             31.0           0.637635  \n",
       "        Before            31.0           0.993759  \n",
       "58      After             41.0           0.988967  \n",
       "\n",
       "[86 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a dictionary and a corpus for the LDA model\n",
    "print(\"Creating dictionary and corpus\")\n",
    "dictionary = corpora.Dictionary(df_small['Tokens'])\n",
    "corpus = [dictionary.doc2bow(token_list) for token_list in df_small['Tokens']]\n",
    "\n",
    "print(\"Training LDA model\")\n",
    "lda = ldamodel.LdaModel(corpus, num_topics=55, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = lda.print_topics(num_words=9)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning topics to each document\n",
      "                                                    Tags_combined  \\\n",
      "Decline Source                                                      \n",
      "0       After   MsRosieBea,uni,uni life,first year of uni,thir...   \n",
      "        Before  MsRosieBea,red lip,get ready with me\\nMsRosieB...   \n",
      "1       After   hollow,generationhollow,tea,questions,qna,answ...   \n",
      "        Before  hollow,generationhollow,playthrough,blind play...   \n",
      "3       After   dayz,loot,betrayal,solo,friends,survive,surviv...   \n",
      "        Before  dayz,dayz standalone,.62,update,map,loot,inter...   \n",
      "4       After   stormystrike,stormy strikes channel,stormystik...   \n",
      "        Before  Spirit,Spirit Riding Free,Spirit Stallion of t...   \n",
      "5       After   Triple Entray,Phora,Drake,Eminem,Justin Bieber...   \n",
      "        Before  Triple Entray,Hip Hop,Eminem,Phora,Drake,Logic...   \n",
      "7       After   Yasha,Yasha Jeltuhin,Akrosphere,Trapeze,Circus...   \n",
      "        Before  Yasha,Yasha Jeltuhin,Akrosphere,Circus,Jen Mac...   \n",
      "8       After   free type beats,free untagged beats,playboi ca...   \n",
      "        Before  impulsebeats,yung impulse,impulse beats,impuls...   \n",
      "10      After   the wander steps to wander,honeymoon,photoshoo...   \n",
      "        Before  comedy,dream house,new house,new home,buying a...   \n",
      "11      After   pharmit,pharmit24,malaysia,awesome,gamer,youtu...   \n",
      "        Before  pharmit,pharmit24,malaysia,awesome,gamer,youtu...   \n",
      "12      After   twintalksballet,twin,twins,talk,talks,ballet,b...   \n",
      "        Before  twintalksballet,twin,twins,talk,talks,ballet,b...   \n",
      "\n",
      "                                                           Tokens  \\\n",
      "Decline Source                                                      \n",
      "0       After   [msrosiebea, uni, uni, life, first, year, uni,...   \n",
      "        Before  [msrosiebea, red, lip, get, ready, msrosiebea,...   \n",
      "1       After   [hollow, generationhollow, tea, question, qna,...   \n",
      "        Before  [hollow, generationhollow, playthrough, blind,...   \n",
      "3       After   [dayz, loot, betrayal, solo, friend, survive, ...   \n",
      "        Before  [dayz, dayz, standalone, .62, update, map, loo...   \n",
      "4       After   [stormystrike, stormy, strike, channel, stormy...   \n",
      "        Before  [spirit, spirit, riding, free, spirit, stallio...   \n",
      "5       After   [triple, entray, phora, drake, eminem, justin,...   \n",
      "        Before  [triple, entray, hip, hop, eminem, phora, drak...   \n",
      "7       After   [yasha, yasha, jeltuhin, akrosphere, trapeze, ...   \n",
      "        Before  [yasha, yasha, jeltuhin, akrosphere, circus, j...   \n",
      "8       After   [free, type, beat, free, untagged, beat, playb...   \n",
      "        Before  [impulsebeats, yung, impulse, impulse, beat, i...   \n",
      "10      After   [wander, step, wander, honeymoon, photoshoot, ...   \n",
      "        Before  [comedy, dream, house, new, house, new, home, ...   \n",
      "11      After   [pharmit, pharmit24, malaysia, awesome, gamer,...   \n",
      "        Before  [pharmit, pharmit24, malaysia, awesome, gamer,...   \n",
      "12      After   [twintalksballet, twin, twin, talk, talk, ball...   \n",
      "        Before  [twintalksballet, twin, twin, talk, talk, ball...   \n",
      "\n",
      "                Dominant_Topic  Topic_Probability  \n",
      "Decline Source                                     \n",
      "0       After               19           0.520721  \n",
      "        Before              35           0.986550  \n",
      "1       After               36           0.970825  \n",
      "        Before              36           0.992272  \n",
      "3       After               32           0.999236  \n",
      "        Before              32           0.999112  \n",
      "4       After               47           0.992577  \n",
      "        Before              47           0.987484  \n",
      "5       After               39           0.901814  \n",
      "        Before              39           0.986732  \n",
      "7       After               37           0.968328  \n",
      "        Before              37           0.778886  \n",
      "8       After               15           0.525632  \n",
      "        Before              15           0.491459  \n",
      "10      After               34           0.999816  \n",
      "        Before              34           0.999107  \n",
      "11      After               12           0.976786  \n",
      "        Before              12           0.999218  \n",
      "12      After               51           0.993366  \n",
      "        Before              51           0.997409  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eva\\AppData\\Local\\Temp\\ipykernel_42928\\3727712663.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_small['Dominant_Topic'], df_small['Topic_Probability'] = zip(\n"
     ]
    }
   ],
   "source": [
    "print(\"Assigning topics to each document\")\n",
    "\n",
    "# Assign the dominant topic to each document\n",
    "def assign_dominant_topic(tokens, lda_model, dictionary):\n",
    "    if not tokens or not isinstance(tokens, list):  # Handle empty or invalid tokens\n",
    "        return None, None\n",
    "    bow = dictionary.doc2bow(tokens)  # Convert tokens to bag-of-words format\n",
    "    topic_probs = lda_model.get_document_topics(bow)  # Get topic distribution\n",
    "    if topic_probs:\n",
    "        dominant_topic, prob = max(topic_probs, key=lambda x: x[1])  # Most probable topic\n",
    "        return dominant_topic, prob\n",
    "    return None, None\n",
    "\n",
    "df_small['Dominant_Topic'], df_small['Topic_Probability'] = zip(\n",
    "    *df_small['Tokens'].apply(lambda tokens: assign_dominant_topic(tokens, lda, dictionary))\n",
    ")\n",
    "\n",
    "print(df_small.head(20))\n",
    "df_small.to_csv('data/df_small_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create two columns [Topic_change] and [Tokens_change] to determine if there is a difference between the tags before and after a decline. A change in tokens is used for granular analysis while a change is topics is more appropriate for detecting higher-level patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Dominant_Topic</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <th>After</th>\n",
       "      <th>Before</th>\n",
       "      <th>After</th>\n",
       "      <th>Before</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>msrosiebea uni uni life first year uni third y...</td>\n",
       "      <td>msrosiebea red lip get ready msrosiebea bikini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>hollow generationhollow tea question qna answe...</td>\n",
       "      <td>hollow generationhollow playthrough blind play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>dayz loot betrayal solo friend survive surviva...</td>\n",
       "      <td>dayz dayz standalone .62 update map loot inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>stormystrike stormy strike channel stormystike...</td>\n",
       "      <td>spirit spirit riding free spirit stallion cima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>triple entray phora drake eminem justin bieber...</td>\n",
       "      <td>triple entray hip hop eminem phora drake logic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant_Topic         \\\n",
       "Source           After Before   \n",
       "Decline                         \n",
       "0                 19.0   35.0   \n",
       "1                 36.0   36.0   \n",
       "3                 32.0   32.0   \n",
       "4                 47.0   47.0   \n",
       "5                 39.0   39.0   \n",
       "\n",
       "                                                    Tokens  \\\n",
       "Source                                               After   \n",
       "Decline                                                      \n",
       "0        msrosiebea uni uni life first year uni third y...   \n",
       "1        hollow generationhollow tea question qna answe...   \n",
       "3        dayz loot betrayal solo friend survive surviva...   \n",
       "4        stormystrike stormy strike channel stormystike...   \n",
       "5        triple entray phora drake eminem justin bieber...   \n",
       "\n",
       "                                                            \n",
       "Source                                              Before  \n",
       "Decline                                                     \n",
       "0        msrosiebea red lip get ready msrosiebea bikini...  \n",
       "1        hollow generationhollow playthrough blind play...  \n",
       "3        dayz dayz standalone .62 update map loot inter...  \n",
       "4        spirit spirit riding free spirit stallion cima...  \n",
       "5        triple entray hip hop eminem phora drake logic...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small = df_small.dropna(subset=['Tokens', 'Dominant_Topic'])\n",
    "\n",
    "# Pivot the dataset, keeping 'Dominant_topic' in a separate column\n",
    "df_pivot = df_small.pivot_table(\n",
    "    index='Decline',  # The index will be based on the 'Decline'\n",
    "    columns='Source',  # We are splitting by 'Source' (Before and After)\n",
    "    values=['Tokens', 'Dominant_Topic'],  # We want both Tokens and Dominant_topic in the pivoted table\n",
    "    aggfunc={\n",
    "        'Tokens': lambda x: ' '.join([item for sublist in x for item in sublist]),  # Flatten and join the tokens\n",
    "        'Dominant_Topic': lambda x: x.mode()[0]  # Get the most frequent dominant topic (mode)\n",
    "    }\n",
    ")\n",
    "\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Dominant_Topic</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Tokens</th>\n",
       "      <th>Token_Change</th>\n",
       "      <th>Topic_Change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <th>After</th>\n",
       "      <th>Before</th>\n",
       "      <th>After</th>\n",
       "      <th>Before</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>msrosiebea uni uni life first year uni third y...</td>\n",
       "      <td>msrosiebea red lip get ready msrosiebea bikini...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>hollow generationhollow tea question qna answe...</td>\n",
       "      <td>hollow generationhollow playthrough blind play...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>dayz loot betrayal solo friend survive surviva...</td>\n",
       "      <td>dayz dayz standalone .62 update map loot inter...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>stormystrike stormy strike channel stormystike...</td>\n",
       "      <td>spirit spirit riding free spirit stallion cima...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>triple entray phora drake eminem justin bieber...</td>\n",
       "      <td>triple entray hip hop eminem phora drake logic...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant_Topic         \\\n",
       "Source           After Before   \n",
       "Decline                         \n",
       "0                 19.0   35.0   \n",
       "1                 36.0   36.0   \n",
       "3                 32.0   32.0   \n",
       "4                 47.0   47.0   \n",
       "5                 39.0   39.0   \n",
       "\n",
       "                                                    Tokens  \\\n",
       "Source                                               After   \n",
       "Decline                                                      \n",
       "0        msrosiebea uni uni life first year uni third y...   \n",
       "1        hollow generationhollow tea question qna answe...   \n",
       "3        dayz loot betrayal solo friend survive surviva...   \n",
       "4        stormystrike stormy strike channel stormystike...   \n",
       "5        triple entray phora drake eminem justin bieber...   \n",
       "\n",
       "                                                           Token_Change  \\\n",
       "Source                                              Before                \n",
       "Decline                                                                   \n",
       "0        msrosiebea red lip get ready msrosiebea bikini...        False   \n",
       "1        hollow generationhollow playthrough blind play...        False   \n",
       "3        dayz dayz standalone .62 update map loot inter...        False   \n",
       "4        spirit spirit riding free spirit stallion cima...        False   \n",
       "5        triple entray hip hop eminem phora drake logic...        False   \n",
       "\n",
       "        Topic_Change  \n",
       "Source                \n",
       "Decline               \n",
       "0               True  \n",
       "1              False  \n",
       "3              False  \n",
       "4              False  \n",
       "5              False  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_change(tokens_before, tokens_after):\n",
    "    # Ensure tokens are lists and not NaN or float\n",
    "    if not isinstance(tokens_before, list):\n",
    "        tokens_before = []\n",
    "    if not isinstance(tokens_after, list):\n",
    "        tokens_after = []\n",
    "        \n",
    "    # Compare sets of tokens\n",
    "    set_before = set(tokens_before)\n",
    "    set_after = set(tokens_after)\n",
    "    return set_before != set_after  # Change if the sets are not identical\n",
    "\n",
    "# Apply the token change function to compare the tokens before and after for each decline\n",
    "df_pivot['Token_Change'] = df_pivot.apply(\n",
    "    lambda row: token_change(row[('Tokens', 'Before')], row[('Tokens', 'After')]), axis=1)\n",
    "\n",
    "# Assuming 'Dominant_topic' columns are available for 'Before' and 'After'\n",
    "df_pivot['Topic_Change'] = df_pivot.apply(\n",
    "    lambda row: row[('Dominant_Topic', 'Before')] != row[('Dominant_Topic', 'After')], axis=1)\n",
    "\n",
    "# Verify the results\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.6721154057246735\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=df_small['Tokens'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score: {coherence_lda}') # 0.7475 with 55 topics, numwords = 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and lemmatizing tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57517/57517 [12:53<00:00, 74.34it/s] \n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing and lemmatizing tags\")\n",
    "df_tags['Tokens'] = None\n",
    "for index, row in tqdm(df_tags.iterrows(), total=df_tags.shape[0]):\n",
    "    df_tags.at[index, 'Tokens'] = preprocess_str(row['Tags_combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary and corpus\n",
      "Training LDA model\n",
      "(0, '0.057*\"beat\" + 0.041*\"type\" + 0.036*\"nintendo\" + 0.029*\"smash\" + 0.028*\"switch\" + 0.023*\"free\" + 0.022*\"trap\" + 0.018*\"super\" + 0.018*\"mario\" + 0.016*\"rap\" + 0.015*\"2018\" + 0.013*\"hop\" + 0.013*\"ultimate\" + 0.013*\"tennis\" + 0.013*\"hip\"')\n",
      "(1, '0.049*\"minecraft\" + 0.039*\"dota\" + 0.034*\"black\" + 0.028*\"ops\" + 0.028*\"call\" + 0.028*\"zombie\" + 0.025*\"duty\" + 0.022*\"cod\" + 0.019*\"ark\" + 0.019*\"ww2\" + 0.017*\"3\" + 0.015*\"mod\" + 0.012*\"gameplay\" + 0.012*\"4\" + 0.012*\"warfare\"')\n",
      "(2, '0.094*\"fifa\" + 0.075*\"18\" + 0.067*\"19\" + 0.045*\"team\" + 0.042*\"pack\" + 0.039*\"17\" + 0.029*\"ultimate\" + 0.022*\"opening\" + 0.020*\"career\" + 0.019*\"mode\" + 0.019*\"squad\" + 0.016*\"fut\" + 0.015*\"player\" + 0.013*\"sbc\" + 0.012*\"gameplay\"')\n",
      "(3, '0.016*\"news\" + 0.015*\"trump\" + 0.009*\"house\" + 0.007*\"show\" + 0.006*\"entertainment\" + 0.006*\"politics\" + 0.006*\"u\" + 0.006*\"interview\" + 0.006*\"medium\" + 0.006*\"donald\" + 0.006*\"state\" + 0.005*\"live\" + 0.005*\"mix\" + 0.005*\"tmz\" + 0.005*\"dubstep\"')\n",
      "(4, '0.067*\"gta\" + 0.055*\"5\" + 0.046*\"online\" + 0.034*\"car\" + 0.023*\"glitch\" + 0.018*\"red\" + 0.018*\"v\" + 0.015*\"money\" + 0.014*\"dead\" + 0.011*\"auto\" + 0.010*\"dlc\" + 0.010*\"grand\" + 0.009*\"racing\" + 0.009*\"crash\" + 0.009*\"forza\"')\n",
      "(5, '0.042*\"art\" + 0.033*\"2017\" + 0.029*\"tarot\" + 0.027*\"golf\" + 0.026*\"reading\" + 0.021*\"love\" + 0.019*\"2018\" + 0.018*\"angel\" + 0.014*\"f1\" + 0.011*\"drawing\" + 0.010*\"card\" + 0.010*\"april\" + 0.009*\"march\" + 0.009*\"associated\" + 0.009*\"spiritual\"')\n",
      "(6, '0.045*\"funny\" + 0.023*\"video\" + 0.023*\"roblox\" + 0.022*\"star\" + 0.019*\"prank\" + 0.017*\"war\" + 0.014*\"comedy\" + 0.012*\"challenge\" + 0.011*\"meme\" + 0.010*\"story\" + 0.010*\"youtube\" + 0.007*\"paul\" + 0.006*\"scary\" + 0.006*\"girl\" + 0.006*\"vine\"')\n",
      "(7, '0.025*\"vlog\" + 0.025*\"makeup\" + 0.014*\"haul\" + 0.011*\"hair\" + 0.010*\"beauty\" + 0.009*\"tutorial\" + 0.009*\"day\" + 0.009*\"vlogs\" + 0.009*\"fashion\" + 0.008*\"life\" + 0.008*\"daily\" + 0.007*\"vlogger\" + 0.006*\"family\" + 0.006*\"new\" + 0.005*\"review\"')\n",
      "(8, '0.022*\"design\" + 0.016*\"make\" + 0.014*\"bitcoin\" + 0.012*\"recipe\" + 0.012*\"easy\" + 0.011*\"food\" + 0.010*\"diy\" + 0.010*\"simple\" + 0.009*\"money\" + 0.008*\"fiba\" + 0.007*\"free\" + 0.007*\"cryptocurrency\" + 0.007*\"crypto\" + 0.007*\"dot\" + 0.006*\"life\"')\n",
      "(9, '0.134*\"fortnite\" + 0.037*\"battle\" + 0.036*\"royale\" + 0.025*\"moment\" + 0.025*\"new\" + 0.022*\"best\" + 0.017*\"pubg\" + 0.017*\"skin\" + 0.016*\"season\" + 0.015*\"funny\" + 0.014*\"rocket\" + 0.011*\"update\" + 0.011*\"league\" + 0.010*\"gameplay\" + 0.009*\"ninja\"')\n",
      "(10, '0.058*\"v\" + 0.045*\"wwe\" + 0.024*\"sport\" + 0.019*\"cup\" + 0.019*\"world\" + 0.016*\"league\" + 0.015*\"highlight\" + 0.014*\"football\" + 0.010*\"2018\" + 0.010*\"2019\" + 0.009*\"match\" + 0.008*\"goal\" + 0.007*\"asmr\" + 0.007*\"de\" + 0.007*\"raw\"')\n",
      "(11, '0.013*\"movie\" + 0.013*\"review\" + 0.013*\"season\" + 0.012*\"trailer\" + 0.010*\"episode\" + 0.009*\"dragon\" + 0.009*\"comic\" + 0.009*\"ball\" + 0.007*\"film\" + 0.006*\"marvel\" + 0.006*\"anime\" + 0.006*\"book\" + 0.005*\"reaction\" + 0.005*\"2\" + 0.005*\"dead\"')\n",
      "(12, '0.083*\"news\" + 0.026*\"video\" + 0.024*\"movie\" + 0.021*\"cricket\" + 0.020*\"latest\" + 0.020*\"hindi\" + 0.017*\"2019\" + 0.016*\"new\" + 0.015*\"indian\" + 0.015*\"india\" + 0.012*\"2018\" + 0.012*\"tv\" + 0.011*\"today\" + 0.011*\"pakistan\" + 0.010*\"channel\"')\n",
      "(13, '0.030*\"league\" + 0.023*\"legend\" + 0.020*\"lol\" + 0.012*\"god\" + 0.011*\"time\" + 0.010*\"world\" + 0.009*\"magic\" + 0.009*\"jesus\" + 0.008*\"science\" + 0.007*\"earth\" + 0.007*\"space\" + 0.007*\"rv\" + 0.007*\"bible\" + 0.006*\"new\" + 0.006*\"hearthstone\"')\n",
      "(14, '0.056*\"nba\" + 0.042*\"madden\" + 0.022*\"basketball\" + 0.020*\"minor\" + 0.017*\"baseball\" + 0.017*\"highlight\" + 0.015*\"2k19\" + 0.015*\"2k18\" + 0.014*\"sport\" + 0.014*\"mut\" + 0.014*\"nfl\" + 0.013*\"mlb\" + 0.013*\"hockey\" + 0.010*\"diamond\" + 0.009*\"raven\"')\n",
      "(15, '0.070*\"music\" + 0.044*\"reaction\" + 0.041*\"song\" + 0.031*\"dance\" + 0.019*\"cover\" + 0.017*\"video\" + 0.015*\"bts\" + 0.015*\"remix\" + 0.014*\"talent\" + 0.014*\"new\" + 0.013*\"love\" + 0.013*\"lyric\" + 0.013*\"pgt\" + 0.012*\"got\" + 0.012*\"pop\"')\n",
      "(16, '0.064*\"2\" + 0.031*\"destiny\" + 0.019*\"clash\" + 0.017*\"overwatch\" + 0.015*\"csgo\" + 0.013*\"guide\" + 0.012*\"gameplay\" + 0.011*\"best\" + 0.010*\"highlight\" + 0.010*\"division\" + 0.010*\"new\" + 0.010*\"pro\" + 0.010*\"gaming\" + 0.010*\"hero\" + 0.009*\"c\"')\n",
      "(17, '0.066*\"pokemon\" + 0.025*\"android\" + 0.019*\"mobile\" + 0.019*\"go\" + 0.016*\"best\" + 0.015*\"free\" + 0.015*\"iphone\" + 0.011*\"moon\" + 0.011*\"review\" + 0.011*\"sun\" + 0.011*\"new\" + 0.011*\"io\" + 0.010*\"shiny\" + 0.010*\"card\" + 0.009*\"pro\"')\n",
      "(18, '0.058*\"kid\" + 0.038*\"family\" + 0.033*\"toy\" + 0.028*\"baby\" + 0.023*\"video\" + 0.015*\"fun\" + 0.013*\"disney\" + 0.013*\"animal\" + 0.012*\"slime\" + 0.012*\"color\" + 0.011*\"learn\" + 0.011*\"child\" + 0.011*\"cat\" + 0.010*\"play\" + 0.010*\"dog\"')\n",
      "(19, '0.070*\"game\" + 0.039*\"gameplay\" + 0.027*\"play\" + 0.021*\"let\" + 0.019*\"\\'s\" + 0.019*\"2\" + 0.016*\"gaming\" + 0.014*\"4\" + 0.012*\"walkthrough\" + 0.011*\"1\" + 0.011*\"ps4\" + 0.011*\"pc\" + 0.010*\"part\" + 0.010*\"video\" + 0.010*\"xbox\"')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a dictionary and a corpus for the LDA model\n",
    "print(\"Creating dictionary and corpus\")\n",
    "dictionary = corpora.Dictionary(df_tags['Tokens'])\n",
    "corpus = [dictionary.doc2bow(token_list) for token_list in df_tags['Tokens']]\n",
    "\n",
    "print(\"Training LDA model\")\n",
    "lda = ldamodel.LdaModel(corpus, num_topics=20, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = lda.print_topics(num_words=15)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics saved to lda_topics.csv\n"
     ]
    }
   ],
   "source": [
    "topics = lda.print_topics(num_words=15)\n",
    "\n",
    "# Create a DataFrame from the topics\n",
    "topics_data = []\n",
    "for topic_id, topic in topics:\n",
    "    topics_data.append({\"Topic\": topic_id, \"Words\": topic})\n",
    "\n",
    "topics_df = pd.DataFrame(topics_data)\n",
    "\n",
    "topics_df.to_csv(\"data/lda_topics.csv\", index=False)\n",
    "print(\"Topics saved to lda_topics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning topics to each document\n",
      "                                                    Tags_combined  \\\n",
      "Decline Source                                                      \n",
      "0       After   MsRosieBea,uni,uni life,first year of uni,thir...   \n",
      "        Before  MsRosieBea,red lip,get ready with me\\nMsRosieB...   \n",
      "1       After   hollow,generationhollow,tea,questions,qna,answ...   \n",
      "        Before  hollow,generationhollow,playthrough,blind play...   \n",
      "3       After   dayz,loot,betrayal,solo,friends,survive,surviv...   \n",
      "        Before  dayz,dayz standalone,.62,update,map,loot,inter...   \n",
      "4       After   stormystrike,stormy strikes channel,stormystik...   \n",
      "        Before  Spirit,Spirit Riding Free,Spirit Stallion of t...   \n",
      "5       After   Triple Entray,Phora,Drake,Eminem,Justin Bieber...   \n",
      "        Before  Triple Entray,Hip Hop,Eminem,Phora,Drake,Logic...   \n",
      "7       After   Yasha,Yasha Jeltuhin,Akrosphere,Trapeze,Circus...   \n",
      "        Before  Yasha,Yasha Jeltuhin,Akrosphere,Circus,Jen Mac...   \n",
      "8       After   free type beats,free untagged beats,playboi ca...   \n",
      "        Before  impulsebeats,yung impulse,impulse beats,impuls...   \n",
      "10      After   the wander steps to wander,honeymoon,photoshoo...   \n",
      "        Before  comedy,dream house,new house,new home,buying a...   \n",
      "11      After   pharmit,pharmit24,malaysia,awesome,gamer,youtu...   \n",
      "        Before  pharmit,pharmit24,malaysia,awesome,gamer,youtu...   \n",
      "12      After   twintalksballet,twin,twins,talk,talks,ballet,b...   \n",
      "        Before  twintalksballet,twin,twins,talk,talks,ballet,b...   \n",
      "\n",
      "                                                           Tokens  \\\n",
      "Decline Source                                                      \n",
      "0       After   [msrosiebea, uni, uni, life, first, year, uni,...   \n",
      "        Before  [msrosiebea, red, lip, get, ready, msrosiebea,...   \n",
      "1       After   [hollow, generationhollow, tea, question, qna,...   \n",
      "        Before  [hollow, generationhollow, playthrough, blind,...   \n",
      "3       After   [dayz, loot, betrayal, solo, friend, survive, ...   \n",
      "        Before  [dayz, dayz, standalone, .62, update, map, loo...   \n",
      "4       After   [stormystrike, stormy, strike, channel, stormy...   \n",
      "        Before  [spirit, spirit, riding, free, spirit, stallio...   \n",
      "5       After   [triple, entray, phora, drake, eminem, justin,...   \n",
      "        Before  [triple, entray, hip, hop, eminem, phora, drak...   \n",
      "7       After   [yasha, yasha, jeltuhin, akrosphere, trapeze, ...   \n",
      "        Before  [yasha, yasha, jeltuhin, akrosphere, circus, j...   \n",
      "8       After   [free, type, beat, free, untagged, beat, playb...   \n",
      "        Before  [impulsebeats, yung, impulse, impulse, beat, i...   \n",
      "10      After   [wander, step, wander, honeymoon, photoshoot, ...   \n",
      "        Before  [comedy, dream, house, new, house, new, home, ...   \n",
      "11      After   [pharmit, pharmit24, malaysia, awesome, gamer,...   \n",
      "        Before  [pharmit, pharmit24, malaysia, awesome, gamer,...   \n",
      "12      After   [twintalksballet, twin, twin, talk, talk, ball...   \n",
      "        Before  [twintalksballet, twin, twin, talk, talk, ball...   \n",
      "\n",
      "                Dominant_Topic  Topic_Probability  \n",
      "Decline Source                                     \n",
      "0       After                7           0.879984  \n",
      "        Before               7           0.888036  \n",
      "1       After               19           0.619076  \n",
      "        Before              19           0.422962  \n",
      "3       After                9           0.301571  \n",
      "        Before               9           0.272670  \n",
      "4       After               18           0.378290  \n",
      "        Before              18           0.412376  \n",
      "5       After                0           0.440172  \n",
      "        Before               0           0.780147  \n",
      "7       After               11           0.588445  \n",
      "        Before              13           0.595551  \n",
      "8       After                0           0.785160  \n",
      "        Before               0           0.817391  \n",
      "10      After                7           0.533964  \n",
      "        Before              13           0.376729  \n",
      "11      After               19           0.486818  \n",
      "        Before              19           0.520971  \n",
      "12      After                7           0.422796  \n",
      "        Before               7           0.329311  \n"
     ]
    }
   ],
   "source": [
    "print(\"Assigning topics to each document\")\n",
    "\n",
    "def assign_dominant_topic(tokens, lda_model, dictionary):\n",
    "    if not tokens or not isinstance(tokens, list):  # Handle empty or invalid tokens\n",
    "        return None, None\n",
    "    bow = dictionary.doc2bow(tokens)  # Convert tokens to bag-of-words format\n",
    "    topic_probs = lda_model.get_document_topics(bow)  # Get topic distribution\n",
    "    if topic_probs:\n",
    "        dominant_topic, prob = max(topic_probs, key=lambda x: x[1])  # Most probable topic\n",
    "        return dominant_topic, prob\n",
    "    return None, None\n",
    "\n",
    "df_tags['Dominant_Topic'], df_tags['Topic_Probability'] = zip(\n",
    "    *df_tags['Tokens'].apply(lambda tokens: assign_dominant_topic(tokens, lda, dictionary))\n",
    ")\n",
    "\n",
    "print(df_tags.head(20))\n",
    "df_tags.to_csv('df_small_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Dominant_Topic</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <th>After</th>\n",
       "      <th>Before</th>\n",
       "      <th>After</th>\n",
       "      <th>Before</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>msrosiebea uni uni life first year uni third y...</td>\n",
       "      <td>msrosiebea red lip get ready msrosiebea bikini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>hollow generationhollow tea question qna answe...</td>\n",
       "      <td>hollow generationhollow playthrough blind play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dayz loot betrayal solo friend survive surviva...</td>\n",
       "      <td>dayz dayz standalone .62 update map loot inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>stormystrike stormy strike channel stormystike...</td>\n",
       "      <td>spirit spirit riding free spirit stallion cima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>triple entray phora drake eminem justin bieber...</td>\n",
       "      <td>triple entray hip hop eminem phora drake logic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>yasha yasha jeltuhin akrosphere trapeze circus...</td>\n",
       "      <td>yasha yasha jeltuhin akrosphere circus jen mac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>free type beat free untagged beat playboi cart...</td>\n",
       "      <td>impulsebeats yung impulse impulse beat impulse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>wander step wander honeymoon photoshoot best h...</td>\n",
       "      <td>comedy dream house new house new home buying h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>pharmit pharmit24 malaysia awesome gamer youtu...</td>\n",
       "      <td>pharmit pharmit24 malaysia awesome gamer youtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>twintalksballet twin twin talk talk ballet bal...</td>\n",
       "      <td>twintalksballet twin twin talk talk ballet bal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>style diy hairstyle hair natural hair ponytail...</td>\n",
       "      <td>style hairstyle hair natural hair dutch braid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>baladas ballad prog rock art jjmacedo braga po...</td>\n",
       "      <td>yiruma baladas piano ballad love amor art arte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hero strike moba,3v3 game mobile brawl shooting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>simple eye make-up remover hydrating cleansing...</td>\n",
       "      <td>simple sensitive skin eye make-up remover beau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>kawaii mstrinity143 japan japanese candy kit s...</td>\n",
       "      <td>kawaii mstrinity143 japan japanese candy kit s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>enduro world championship ewc wec endurogp end...</td>\n",
       "      <td>enduro world championship ewc wec ktm husqvarn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>instagram instagram control life instagram con...</td>\n",
       "      <td>teacher vlog last day school end school year t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alex clare three heart alex clare three day gr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>humanism v robot ai transhumanism steve fuller...</td>\n",
       "      <td>aging vitality rudi westendorp growing old fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>haul fashion haul sarojini sarojini haul saroj...</td>\n",
       "      <td>ootw ootd outfit week,2016 outfit fashion yout...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant_Topic         \\\n",
       "Source           After Before   \n",
       "Decline                         \n",
       "0                  7.0    7.0   \n",
       "1                 19.0   19.0   \n",
       "3                  9.0    9.0   \n",
       "4                 18.0   18.0   \n",
       "5                  0.0    0.0   \n",
       "7                 11.0   13.0   \n",
       "8                  0.0    0.0   \n",
       "10                 7.0   13.0   \n",
       "11                19.0   19.0   \n",
       "12                 7.0    7.0   \n",
       "13                 7.0    7.0   \n",
       "14                11.0    5.0   \n",
       "15                 NaN   16.0   \n",
       "16                 7.0    7.0   \n",
       "17                18.0   18.0   \n",
       "18                10.0    4.0   \n",
       "19                 7.0    7.0   \n",
       "20                 3.0    NaN   \n",
       "21                13.0    7.0   \n",
       "22                 7.0    7.0   \n",
       "\n",
       "                                                    Tokens  \\\n",
       "Source                                               After   \n",
       "Decline                                                      \n",
       "0        msrosiebea uni uni life first year uni third y...   \n",
       "1        hollow generationhollow tea question qna answe...   \n",
       "3        dayz loot betrayal solo friend survive surviva...   \n",
       "4        stormystrike stormy strike channel stormystike...   \n",
       "5        triple entray phora drake eminem justin bieber...   \n",
       "7        yasha yasha jeltuhin akrosphere trapeze circus...   \n",
       "8        free type beat free untagged beat playboi cart...   \n",
       "10       wander step wander honeymoon photoshoot best h...   \n",
       "11       pharmit pharmit24 malaysia awesome gamer youtu...   \n",
       "12       twintalksballet twin twin talk talk ballet bal...   \n",
       "13       style diy hairstyle hair natural hair ponytail...   \n",
       "14       baladas ballad prog rock art jjmacedo braga po...   \n",
       "15                                                     NaN   \n",
       "16       simple eye make-up remover hydrating cleansing...   \n",
       "17       kawaii mstrinity143 japan japanese candy kit s...   \n",
       "18       enduro world championship ewc wec endurogp end...   \n",
       "19       instagram instagram control life instagram con...   \n",
       "20       alex clare three heart alex clare three day gr...   \n",
       "21       humanism v robot ai transhumanism steve fuller...   \n",
       "22       haul fashion haul sarojini sarojini haul saroj...   \n",
       "\n",
       "                                                            \n",
       "Source                                              Before  \n",
       "Decline                                                     \n",
       "0        msrosiebea red lip get ready msrosiebea bikini...  \n",
       "1        hollow generationhollow playthrough blind play...  \n",
       "3        dayz dayz standalone .62 update map loot inter...  \n",
       "4        spirit spirit riding free spirit stallion cima...  \n",
       "5        triple entray hip hop eminem phora drake logic...  \n",
       "7        yasha yasha jeltuhin akrosphere circus jen mac...  \n",
       "8        impulsebeats yung impulse impulse beat impulse...  \n",
       "10       comedy dream house new house new home buying h...  \n",
       "11       pharmit pharmit24 malaysia awesome gamer youtu...  \n",
       "12       twintalksballet twin twin talk talk ballet bal...  \n",
       "13       style hairstyle hair natural hair dutch braid ...  \n",
       "14       yiruma baladas piano ballad love amor art arte...  \n",
       "15         hero strike moba,3v3 game mobile brawl shooting  \n",
       "16       simple sensitive skin eye make-up remover beau...  \n",
       "17       kawaii mstrinity143 japan japanese candy kit s...  \n",
       "18       enduro world championship ewc wec ktm husqvarn...  \n",
       "19       teacher vlog last day school end school year t...  \n",
       "20                                                     NaN  \n",
       "21       aging vitality rudi westendorp growing old fre...  \n",
       "22       ootw ootd outfit week,2016 outfit fashion yout...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tags = df_tags.dropna(subset=['Tokens', 'Dominant_Topic'])\n",
    "\n",
    "# Pivot the dataset\n",
    "df_pivot = df_tags.pivot_table(\n",
    "    index='Decline',  \n",
    "    columns='Source',  \n",
    "    values=['Tokens', 'Dominant_Topic'],  \n",
    "    aggfunc={\n",
    "        'Tokens': lambda x: ' '.join([item for sublist in x for item in sublist]),  # Flatten and join the tokens\n",
    "        'Dominant_Topic': lambda x: x.mode()[0]  # Get the most frequent dominant topic\n",
    "    }\n",
    ")\n",
    "\n",
    "df_pivot.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Dominant_Topic</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Tokens</th>\n",
       "      <th>Token_Change</th>\n",
       "      <th>Topic_Change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Source</th>\n",
       "      <th>After</th>\n",
       "      <th>Before</th>\n",
       "      <th>After</th>\n",
       "      <th>Before</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>msrosiebea uni uni life first year uni third y...</td>\n",
       "      <td>msrosiebea red lip get ready msrosiebea bikini...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>hollow generationhollow tea question qna answe...</td>\n",
       "      <td>hollow generationhollow playthrough blind play...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>dayz loot betrayal solo friend survive surviva...</td>\n",
       "      <td>dayz dayz standalone .62 update map loot inter...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>stormystrike stormy strike channel stormystike...</td>\n",
       "      <td>spirit spirit riding free spirit stallion cima...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>triple entray phora drake eminem justin bieber...</td>\n",
       "      <td>triple entray hip hop eminem phora drake logic...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant_Topic         \\\n",
       "Source           After Before   \n",
       "Decline                         \n",
       "0                  7.0    7.0   \n",
       "1                 19.0   19.0   \n",
       "3                  9.0    9.0   \n",
       "4                 18.0   18.0   \n",
       "5                  0.0    0.0   \n",
       "\n",
       "                                                    Tokens  \\\n",
       "Source                                               After   \n",
       "Decline                                                      \n",
       "0        msrosiebea uni uni life first year uni third y...   \n",
       "1        hollow generationhollow tea question qna answe...   \n",
       "3        dayz loot betrayal solo friend survive surviva...   \n",
       "4        stormystrike stormy strike channel stormystike...   \n",
       "5        triple entray phora drake eminem justin bieber...   \n",
       "\n",
       "                                                           Token_Change  \\\n",
       "Source                                              Before                \n",
       "Decline                                                                   \n",
       "0        msrosiebea red lip get ready msrosiebea bikini...        False   \n",
       "1        hollow generationhollow playthrough blind play...        False   \n",
       "3        dayz dayz standalone .62 update map loot inter...        False   \n",
       "4        spirit spirit riding free spirit stallion cima...        False   \n",
       "5        triple entray hip hop eminem phora drake logic...        False   \n",
       "\n",
       "        Topic_Change  \n",
       "Source                \n",
       "Decline               \n",
       "0              False  \n",
       "1              False  \n",
       "3              False  \n",
       "4              False  \n",
       "5              False  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def token_change(tokens_before, tokens_after):\n",
    "    # Ensure tokens are lists and not NaN or float\n",
    "    if not isinstance(tokens_before, list):\n",
    "        tokens_before = []\n",
    "    if not isinstance(tokens_after, list):\n",
    "        tokens_after = []\n",
    "        \n",
    "    set_before = set(tokens_before)\n",
    "    set_after = set(tokens_after)\n",
    "    return set_before != set_after \n",
    "\n",
    "df_pivot['Token_Change'] = df_pivot.apply(\n",
    "    lambda row: token_change(row[('Tokens', 'Before')], row[('Tokens', 'After')]), axis=1)\n",
    "\n",
    "df_pivot['Topic_Change'] = df_pivot.apply(\n",
    "    lambda row: row[('Dominant_Topic', 'Before')] != row[('Dominant_Topic', 'After')], axis=1)\n",
    "\n",
    "df_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a new csv file \n",
    "df_topic_change = df_pivot.reset_index()\n",
    "df_topic_change = df_topic_change[['Decline', 'Topic_Change', 'Dominant_Topic']]\n",
    "df_topic_change.to_csv('df_topic_change_20_15w.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.41% of the channels changed the topic of the videos after the start of the decline.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{df_topic_change['Topic_Change'].mean() * 100:.2f}% of the channels changed the topic of the videos after the start of the decline.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.5810251989073743\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=df_tags['Tokens'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score: {coherence_lda}')\n",
    "# small df: optimum at 0.7475 with 55 topics, numwords = 9 \n",
    "# whole df: 0.6525 with 55 topics, 0.5991 with 50, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
