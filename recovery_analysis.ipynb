{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from src.data.dataloader_functions import *\n",
    "from src.utils.results_utils import *\n",
    "from src.utils.recovery_analysis_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "# Make the code reproducible\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_declines_original = pd.read_csv('data/decline_events_complete_-80pc.csv')\n",
    "df_channels = pd.read_csv('data/df_channels_en.tsv', sep='\\t', usecols=['channel', 'category_cc'], index_col='channel')\n",
    "df_data_processed = load_processed_data(usecols=['channel', 'week', 'subs', 'delta_videos', 'activity', 'views'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the outcome\n",
    "\n",
    "Using the duration of the decline, determine whether the YouTuber recovered or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_declines = df_all_declines_original.copy()\n",
    "\n",
    "# If the decline is longer than 4 months without recovery, we consider the YouTuber was not successful in handling it.\n",
    "# Our aim is to find strategies that lead to quick recoveries, therefore taking more than 4 months would be considered unsuccessful.\n",
    "RECOVERY_THRESHOLD = 4 * 4\n",
    "\n",
    "# Add the decline outcome\n",
    "df_all_declines['Recovered'] = df_all_declines['Duration'] < RECOVERY_THRESHOLD\n",
    "\n",
    "# Split the tuple (decline start, decline end) into two separate columns\n",
    "df_all_declines['Event'] = df_all_declines['Event'].apply(lambda s: [int(week_id) for week_id in s[1:-1].split(', ')])\n",
    "df_all_declines['Start'] = df_all_declines['Event'].apply(lambda e: e[0])\n",
    "df_all_declines['End'] = df_all_declines['Event'].apply(lambda e: e[1])\n",
    "df_all_declines.drop('Event', axis=1, inplace=True)\n",
    "\n",
    "# Add the channel category\n",
    "df_all_declines['Category'] = df_all_declines['Channel'].apply(lambda c: df_channels.loc[c]['category_cc'])\n",
    "\n",
    "# Add the channel's subs at the start of the decline\n",
    "decline_index = list(zip(df_all_declines['Channel'], df_all_declines['Start']))\n",
    "df_all_declines['Subs_start'] = df_data_processed.loc[decline_index, 'subs'].values\n",
    "\n",
    "# Add the activity at the start of the decline\n",
    "df_all_declines['Activity_start'] = df_data_processed.loc[decline_index, 'activity'].values\n",
    "\n",
    "# Add the delta videos at the start of the decline\n",
    "df_all_declines['Delta_videos'] = df_data_processed.loc[decline_index, 'delta_videos'].values\n",
    "\n",
    "# Add the channel's subs at the start of the decline\n",
    "df_all_declines['Views_start'] = df_data_processed.loc[decline_index, 'views'].values\n",
    "\n",
    "df_all_declines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is the recovery distributed?\n",
    "\n",
    "To get a first idea of what factors come into play when a YouTuber tries to recover from a decline, we make the dataset balanced by using a matched observational study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_groups_by_categories(df_all_declines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_group_distributions(df_all_declines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can observe that some features are not balanced between the treatment and control groups, especially views and subscribers at the start of the decline, we will perform matching between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_declines['Recovered'].value_counts())\n",
    "print(f\"\\nTotal number of declines: {len(df_all_declines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the size of the dataset, we use random sampling to ease the matching's computation.\n",
    "\n",
    "To check that sampling does not mess with the distribution of recoveries, we plot them depending on the sampling proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sampling_rates(df_all_declines, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to sample 30% of the data since the distribution of recoveries is left mostly unchanged, and it allows to keep a representative sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df_all_declines.sample(frac=0.3, replace=False, random_state=SEED)\n",
    "\n",
    "print(df_sampled['Recovered'].value_counts())\n",
    "print(f\"\\nTotal number of declines after sampling: {len(df_sampled)}\")\n",
    "\n",
    "df_sampled.to_csv('data/SAMPLE.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propensity score matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the matches from the file, otherwise compute them\n",
    "try:\n",
    "    with open('data/matches.pkl', 'rb') as f:\n",
    "        matches = pickle.load(f)\n",
    "        print(\"Matches loaded from file.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found, computing the matches...\")\n",
    "    matches = match_declines(df_sampled)\n",
    "    with open('data/matches.pkl', 'wb') as f:\n",
    "        pickle.dump(matches, f)\n",
    "        print(\"Matches saved to file.\")\n",
    "\n",
    "# Flatten\n",
    "matches = [index for match in matches for index in match]\n",
    "\n",
    "# Get the matched declines\n",
    "df_matched = df_sampled.loc[matches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the propensity score matching, we can compare the distributions to the ones that we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_groups_by_categories(df_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_group_distributions(df_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTuber reactions\n",
    "\n",
    "As we want to find the best ways to deal with a decline depending on the situation, we then take a look at how the YouTubers reacted to the decline, and what methods proved effective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
