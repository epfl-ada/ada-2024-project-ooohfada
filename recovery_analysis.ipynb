{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.data.dataloader_functions import *\n",
    "from src.utils.results_utils import *\n",
    "from src.utils.recovery_analysis_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "# Make the code reproducible\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_declines_original = pd.read_csv('data/decline_events_complete.csv')\n",
    "df_channels = pd.read_csv('data/df_channels_en.tsv', sep='\\t', usecols=['channel', 'category_cc'], index_col='channel')\n",
    "df_data_processed = load_processed_data(usecols=['channel', 'week', 'subs', 'delta_videos', 'activity', 'views'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the outcome\n",
    "\n",
    "Using the duration of the decline, determine whether the YouTuber recovered or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_declines = df_all_declines_original.copy()\n",
    "\n",
    "# If the decline is longer than 4 months without recovery, we consider the YouTuber was not successful in handling it.\n",
    "# Our aim is to find strategies that lead to quick recoveries, therefore taking more than 4 months would be considered unsuccessful.\n",
    "RECOVERY_THRESHOLD = 4 * 4\n",
    "\n",
    "# Add the decline outcome\n",
    "df_all_declines['Recovered'] = df_all_declines['Duration'] < RECOVERY_THRESHOLD\n",
    "\n",
    "# Split the tuple (decline start, decline end) into two separate columns\n",
    "df_all_declines['Event'] = df_all_declines['Event'].apply(lambda s: [int(week_id) for week_id in s[1:-1].split(', ')])\n",
    "df_all_declines['Start'] = df_all_declines['Event'].apply(lambda e: e[0])\n",
    "df_all_declines['End'] = df_all_declines['Event'].apply(lambda e: e[1])\n",
    "df_all_declines.drop('Event', axis=1, inplace=True)\n",
    "\n",
    "# Add the channel category\n",
    "df_all_declines['Category'] = df_all_declines['Channel'].apply(lambda c: df_channels.loc[c]['category_cc'])\n",
    "\n",
    "# Add the channel's subs at the start of the decline\n",
    "decline_index = list(zip(df_all_declines['Channel'], df_all_declines['Start']))\n",
    "df_all_declines['Subs_start'] = df_data_processed.loc[decline_index, 'subs'].values\n",
    "\n",
    "# Add the activity at the start of the decline\n",
    "df_all_declines['Activity_start'] = df_data_processed.loc[decline_index, 'activity'].values\n",
    "\n",
    "# Add the delta videos at the start of the decline\n",
    "df_all_declines['Delta_videos'] = df_data_processed.loc[decline_index, 'delta_videos'].values\n",
    "\n",
    "# Add the channel's subs at the start of the decline\n",
    "df_all_declines['Views_start'] = df_data_processed.loc[decline_index, 'views'].values\n",
    "\n",
    "print(f\"Overall recovery rate: {df_all_declines['Recovered'].mean():.2f}\")\n",
    "\n",
    "df_all_declines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is the recovery distributed?\n",
    "\n",
    "To get a first idea of what factors come into play when a YouTuber tries to recover from a decline, we make the dataset balanced by using a matched observational study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recovered_by_categories(df_all_declines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_group_distributions(df_all_declines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can observe that some features are not balanced between the treatment and control groups, especially views and subscribers at the start of the decline, we will perform matching between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_declines['Recovered'].value_counts())\n",
    "print(f\"\\nTotal number of declines: {len(df_all_declines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the size of the dataset, we use random sampling to ease the matching's computation.\n",
    "\n",
    "To check that sampling does not mess with the distribution of recoveries, we plot them depending on the sampling proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sampling_rates(df_all_declines, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to sample 30% of the data since the distribution of recoveries is left mostly unchanged, and it allows to keep a representative sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df_all_declines.sample(frac=0.3, replace=False, random_state=SEED)\n",
    "\n",
    "print(df_sampled['Recovered'].value_counts())\n",
    "print(f\"\\nTotal number of declines after sampling: {len(df_sampled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTuber reactions\n",
    "\n",
    "As we want to find the best ways to deal with a decline depending on the situation, we then take a look at how the YouTubers reacted to the decline, and what methods proved effective.\n",
    "\n",
    "### Adding the reaction metrics whose impact we want to measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the videos around the declines, from week (decline start - decline duration) to week (decline end)\n",
    "videos_around_declines = pd.read_csv('data/videos_around_declines.csv')\n",
    "\n",
    "# Add the declines with the indices of the corresponding videos\n",
    "df_sampled = get_sampled_declines_with_videos(df_sampled, videos_around_declines)\n",
    "\n",
    "# Augment the data with the video stats : videos per week and mean video duration, before and during the declines\n",
    "df_sampled = add_video_stats(df_sampled, videos_around_declines)\n",
    "\n",
    "# Drop the indices of the videos, they are not needed anymore\n",
    "df_sampled = df_sampled.drop(['Videos_before', 'Videos_after'], axis=1)\n",
    "\n",
    "DIV_ZERO_TOLERANCE = 1e-6\n",
    "\n",
    "# Indicate whether the channel changed video duration after the start of the decline. We include a tolerance of 50% change.\n",
    "df_sampled['Changed_duration'] = df_sampled.apply(lambda row: np.abs(row['Mean_duration_after'] - row['Mean_duration_before']) / np.max([row['Mean_duration_before'], DIV_ZERO_TOLERANCE]) > 0.5, axis=1)\n",
    "df_sampled = df_sampled.drop(['Mean_duration_before', 'Mean_duration_after'], axis=1) # The actual mean durations are not needed anymore\n",
    "print(f\"{df_sampled['Changed_duration'].mean() * 100:.2f}% of the channels changed their video duration after the start of the decline.\")\n",
    "\n",
    "# Indicate whether the channel changed publishing frequency after the start of the decline. We include a tolerance of 100% change.\n",
    "df_sampled['Increased_post_frequency'] = df_sampled.apply(lambda row: row['Videos_per_week_after'] - row['Videos_per_week_before'] / np.max([row['Videos_per_week_before'], DIV_ZERO_TOLERANCE]) > 1, axis=1)\n",
    "df_sampled = df_sampled.drop(['Videos_per_week_before', 'Videos_per_week_after'], axis=1) # The actual frequencies are not needed anymore\n",
    "print(f\"{df_sampled['Increased_post_frequency'].mean() * 100:.2f}% of the channels increased their publishing frequency after the start of the decline.\")\n",
    "\n",
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propensity score matching\n",
    "\n",
    "Seeing that the declines do not have the same distribution on their features, we perform propensity score matchings to balance the treatment and control groups looking at the effect that changing publication frequency,video duration and video category after the start of the decline have on the recovery.\n",
    "\n",
    "After the propensity score matching, we can observe the effect of the different treatments on the recovery :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TREATMENTS = ['Changed_duration', 'Increased_post_frequency']\n",
    "\n",
    "for treatment in TREATMENTS:\n",
    "    # Try to load the matches from the file, otherwise compute them\n",
    "    matches = get_matches(treatment=treatment, declines=df_sampled, verbose=False)\n",
    "\n",
    "    print(f\"{treatment} matches :\", matches)\n",
    "\n",
    "    # Flatten\n",
    "    matches = [index for match in matches for index in match]\n",
    "\n",
    "    # Get the matched declines\n",
    "    df_matched = df_sampled.loc[matches]\n",
    "\n",
    "    plot_treatment_effect(df_matched, treatment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
