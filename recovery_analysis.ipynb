{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from src.data.dataloader_functions import *\n",
    "from src.utils.results_utils import *\n",
    "from src.utils.recovery_analysis_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "# Make the code reproducible\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_declines_original = pd.read_csv('data/decline_events_complete.csv')\n",
    "df_channels = pd.read_csv('data/df_channels_en.tsv', sep='\\t', usecols=['channel', 'category_cc'], index_col='channel')\n",
    "df_data_processed = load_processed_data(usecols=['channel', 'week', 'subs', 'activity', 'views'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the outcome\n",
    "\n",
    "Using the duration of the decline, determine whether the YouTuber recovered or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_declines = df_all_declines_original.copy()\n",
    "\n",
    "# If the decline is longer than 4 months without recovery, we consider the YouTuber was not successful in handling it.\n",
    "# Our aim is to find strategies that lead to quick recoveries, therefore taking more than 4 months would be considered unsuccessful.\n",
    "RECOVERY_THRESHOLD = 4 * 4\n",
    "\n",
    "# Add the decline outcome\n",
    "df_all_declines['Recovered'] = df_all_declines['Duration'] < RECOVERY_THRESHOLD\n",
    "\n",
    "# Split the tuple (decline start, decline end) into two separate columns\n",
    "df_all_declines['Event'] = df_all_declines['Event'].apply(lambda s: [int(week_id) for week_id in s[1:-1].split(', ')])\n",
    "df_all_declines['Start'] = df_all_declines['Event'].apply(lambda e: e[0])\n",
    "df_all_declines['End'] = df_all_declines['Event'].apply(lambda e: e[1])\n",
    "df_all_declines.drop('Event', axis=1, inplace=True)\n",
    "\n",
    "# Add the channel category\n",
    "df_all_declines['Category'] = df_all_declines['Channel'].apply(lambda c: df_channels.loc[c]['category_cc'])\n",
    "\n",
    "# Add the channel's subs at the start of the decline\n",
    "decline_index = list(zip(df_all_declines['Channel'], df_all_declines['Start']))\n",
    "df_all_declines['Subs_start'] = df_data_processed.loc[decline_index, 'subs'].values\n",
    "\n",
    "# Add the activity at the start of the decline\n",
    "df_all_declines['Activity_start'] = df_data_processed.loc[decline_index, 'activity'].values\n",
    "\n",
    "# Add the channel's subs at the start of the decline\n",
    "df_all_declines['Views_start'] = df_data_processed.loc[decline_index, 'views'].values\n",
    "\n",
    "print(f\"Overall recovery rate: {df_all_declines['Recovered'].mean():.2f}\")\n",
    "\n",
    "df_all_declines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is the recovery distributed?\n",
    "\n",
    "To get a first idea of what factors come into play when a YouTuber tries to recover from a decline, we plot some distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recovered_by_categories(df_all_declines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_group_distributions(df_all_declines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that some features are not balanced between the declines that recovered and those that did not, especially views and subscribers at the start of the decline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_declines['Recovered'].value_counts())\n",
    "print(f\"\\nTotal number of declines: {len(df_all_declines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTuber reactions\n",
    "\n",
    "Aiming at finding the best ways to deal with a decline depending on the situation, we take a look at how the YouTubers reacted to the decline, and what methods proved effective.\\\n",
    "In order to observe the reaction's impact, we will conduct a matched observational study on the dataset by using propensity score matching.\n",
    "\n",
    "Considering the size of the dataset, we use random sampling to ease the matching's computation.\\\n",
    "To check that sampling does not mess with the distribution of recoveries, we plot them depending on the sampling proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sampling_rates(df_all_declines, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to sample 30% of the data since it considerably reduces the size of the dataset, and allows to keep a representative sample of the data without perturbing the recovery distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df_all_declines.sample(frac=0.3, replace=False, random_state=SEED)\n",
    "\n",
    "print(df_sampled['Recovered'].value_counts())\n",
    "print(f\"\\nTotal number of declines after sampling: {len(df_sampled)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the reaction metrics whose impact we want to measure\n",
    "\n",
    "The reactions that we are able to observe here are the following:\n",
    "- Did the YouTuber change video publication frequency?\n",
    "- Did the YouTuber change video length?\n",
    "- Did the YouTuber change video category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the videos around the declines, from week (decline start - decline duration) to week (decline end)\n",
    "videos_around_declines = pd.read_csv('data/videos_around_declines.csv')\n",
    "\n",
    "# Add the declines with the indices of the corresponding videos\n",
    "df_sampled = get_sampled_declines_with_videos(df_sampled, videos_around_declines)\n",
    "\n",
    "# Augment the data with the video stats : videos per week and mean video duration, before and during the declines\n",
    "df_sampled = add_video_stats(df_sampled, videos_around_declines)\n",
    "\n",
    "DIV_BY_ZERO_TOLERANCE = 1e-6\n",
    "\n",
    "# Indicate whether the channel increased mean video duration after the start of the decline. We include a tolerance of 50% change.\n",
    "df_sampled['Mean_duration_difference'] = df_sampled.apply(lambda row: row['Mean_duration_after'] - row['Mean_duration_before'], axis=1)\n",
    "df_sampled['Posted_longer_videos'] = df_sampled.apply(lambda row: (row['Mean_duration_difference']) / np.max([row['Mean_duration_before'], DIV_BY_ZERO_TOLERANCE]) > 0.5, axis=1)\n",
    "df_sampled['Posted_shorter_videos'] = df_sampled.apply(lambda row: (row['Mean_duration_difference']) / np.max([row['Mean_duration_before'], DIV_BY_ZERO_TOLERANCE]) < -0.5, axis=1)\n",
    "print(f\"\\n{df_sampled['Posted_longer_videos'].mean() * 100:.2f}% of the channels posted longer videos after the start of the decline.\")\n",
    "print(f\"{df_sampled['Posted_shorter_videos'].mean() * 100:.2f}% of the channels posted shorter videos after the start of the decline.\\n\")\n",
    "\n",
    "# Indicate whether the channel changed publishing frequency after the start of the decline. We include a tolerance of 100% change (doubling the frequency).\n",
    "df_sampled['Mean_frequency_difference'] = df_sampled.apply(lambda row: row['Videos_per_week_after'] - row['Videos_per_week_before'], axis=1)\n",
    "df_sampled['Posted_more'] = df_sampled.apply(lambda row: (row['Mean_frequency_difference']) / np.max([row['Videos_per_week_before'], DIV_BY_ZERO_TOLERANCE]) > 0.5, axis=1)\n",
    "df_sampled['Posted_less'] = df_sampled.apply(lambda row: (row['Mean_frequency_difference']) / np.max([row['Videos_per_week_before'], DIV_BY_ZERO_TOLERANCE]) < -0.5, axis=1)\n",
    "print(f\"{df_sampled['Posted_more'].mean() * 100:.2f}% of the channels posted more videos after the start of the decline.\")\n",
    "print(f\"{df_sampled['Posted_less'].mean() * 100:.2f}% of the channels posted less videos after the start of the decline.\")\n",
    "\n",
    "# Adding three columns for the topic of the videos before and after the decline, and whether the topic changed or not\n",
    "topic_change_data = pd.read_csv('data/df_topic_change_20_15w.csv')\n",
    "topic_change_data.columns = ['Decline', 'Topic_Change', 'Topic_Before', 'Topic_After']\n",
    "df_sampled = pd.merge(df_sampled, topic_change_data, left_index=True, right_on='Decline', how='left')\n",
    "print(f\"{df_sampled['Topic_Change'].mean() * 100:.2f}% of the channels changed of topic after the start of the decline.\")\n",
    "df_sampled = df_sampled.drop(columns=['Decline', 'Topic_Before', 'Topic_After'])\n",
    "\n",
    "\n",
    "# Drop the declines with missing data (usually due to having no video before or after the decline, or no category)\n",
    "df_sampled = df_sampled.dropna()\n",
    "\n",
    "# Put the differences aside, to be used later but not in the models\n",
    "df_videos_per_week = df_sampled[['Videos_per_week_before', 'Videos_per_week_after']]\n",
    "df_video_duration = df_sampled[['Mean_duration_before', 'Mean_duration_after']] \n",
    "df_sampled = df_sampled.drop(['Mean_duration_before', 'Mean_duration_after'], axis=1)\n",
    "df_sampled = df_sampled.drop(['Videos_per_week_before', 'Videos_per_week_after'], axis=1)\n",
    "\n",
    "# Keep a copy of the declines that have videos before and after the decline\n",
    "df_sampled_without_zero_videos = df_sampled[(df_sampled['Videos_before'].apply(len) > 0) & (df_sampled['Videos_after'].apply(len) > 0)]\n",
    "\n",
    "# Drop the indices of the videos, they are not needed anymore\n",
    "df_sampled = df_sampled.drop(['Videos_before', 'Videos_after'], axis=1)\n",
    "df_sampled_without_zero_videos = df_sampled_without_zero_videos.drop(['Videos_before', 'Videos_after'], axis=1)\n",
    "\n",
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propensity score matching\n",
    "\n",
    "Seeing that the declines do not have the same distribution on their features, we perform propensity score matchings to balance the treatment and control groups looking at the effect that changing publication frequency,video duration and video category after the start of the decline have on the recovery.\n",
    "\n",
    "After the propensity score matching, we can observe the effect of the different treatments on the recovery :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variables to be used for the matching\n",
    "# Treatments in the same array will be plotted together\n",
    "TREATMENTS = [\n",
    "    ['Posted_more',\n",
    "    'Posted_less'],\n",
    "    ['Posted_longer_videos',\n",
    "    'Posted_shorter_videos'],\n",
    "    ['Topic_Change']\n",
    "]\n",
    "\n",
    "# The variables to be dropped for each treatment (to avoid multicollinearity or strong correlation)\n",
    "# Use the same order as the TREATMENTS array\n",
    "to_drop = [\n",
    "    [['Mean_frequency_difference', 'Posted_less'],\n",
    "    ['Mean_frequency_difference', 'Posted_more']],\n",
    "    [['Mean_duration_difference', 'Posted_shorter_videos'],\n",
    "    ['Mean_duration_difference', 'Posted_longer_videos']],\n",
    "    [[]]\n",
    "]\n",
    "\n",
    "plot_df = pd.DataFrame(columns=['Strategy', 'Adopted the strategy', 'Did not adopt'])\n",
    "\n",
    "matched_dfs = {}\n",
    "for plot_treatments, plot_dropped in zip(TREATMENTS, to_drop):\n",
    "    fig, axes = plt.subplots(1, len(plot_treatments), figsize=(5*len(plot_treatments), 4))\n",
    "    for subplot_id, (treatment, dropped) in enumerate(zip(plot_treatments, plot_dropped)):\n",
    "\n",
    "        # Try to load the matches from the file, otherwise compute them\n",
    "        matches = get_matches(treatment=treatment, declines=df_sampled.drop(dropped, axis=1), verbose=False)\n",
    "\n",
    "        print(f\"{treatment} matches :\", matches)\n",
    "\n",
    "        # Flatten\n",
    "        matches = [index for match in matches for index in match]\n",
    "\n",
    "        # Get the matched declines\n",
    "        matched_dfs[treatment] = df_sampled.loc[matches]\n",
    "\n",
    "        counts = matched_dfs[treatment].groupby(treatment)['Recovered'].mean() * 100\n",
    "        plot_df.loc[len(plot_df)] = [treatment, counts[True], counts[False]]\n",
    "\n",
    "        plot_treatment_effect(matched_dfs[treatment], treatment, ax=axes[subplot_id] if len(plot_treatments) > 1 else axes)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_df.to_csv('plot_data/matches_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_decision_tree(X, y):\n",
    "    # Create the decision tree model\n",
    "    tree = DecisionTreeClassifier(max_depth=3, random_state=SEED)\n",
    "\n",
    "    # Evaluate the model using cross-validation\n",
    "    cv_results = cross_validate(tree, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Fit the model on the whole dataset\n",
    "    tree.fit(X, y)\n",
    "\n",
    "    return tree, cv_results\n",
    "\n",
    "drop_cols = ['Channel', 'Start', 'End', 'Duration', 'Recovered', 'Mean_duration_difference', 'Mean_frequency_difference']\n",
    "\n",
    "tree_X = df_sampled.drop(drop_cols, axis=1)\n",
    "tree_X = pd.get_dummies(tree_X, columns=['Category'], drop_first=True)\n",
    "\n",
    "tree_y = df_sampled['Recovered']\n",
    "\n",
    "tree, cv_results = perform_decision_tree(tree_X, tree_y)\n",
    "\n",
    "print(f\"Decision tree accuracy: {cv_results['test_score'].mean():.2f}\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree, filled=True, feature_names=tree_X.columns, class_names=['Not recovered', 'Recovered'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the categorical variable into dummies\n",
    "df_sampled_processed = pd.get_dummies(df_sampled, columns=['Category'], drop_first=True)\n",
    "\n",
    "# Transform the boolean variables into integers\n",
    "df_sampled_processed['Recovered'] = df_sampled_processed['Recovered'].astype(int)\n",
    "df_sampled_processed['Posted_more'] = df_sampled_processed['Posted_more'].astype(int)\n",
    "df_sampled_processed['Posted_less'] = df_sampled_processed['Posted_less'].astype(int)\n",
    "df_sampled_processed['Posted_longer_videos'] = df_sampled_processed['Posted_longer_videos'].astype(int)\n",
    "df_sampled_processed['Posted_shorter_videos'] = df_sampled_processed['Posted_shorter_videos'].astype(int)\n",
    "df_sampled_processed['Topic_Change'] = df_sampled_processed['Topic_Change'].astype(int)\n",
    "\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_sampled_processed.drop(columns=['Channel']).corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix['Recovered'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**:\n",
    "\n",
    "* **Positive Correlations** : Posting more videos, changes in upload frequency, and certain categories (e.g., Gaming, News & Politics) are slightly associated with recovery.\n",
    "\n",
    "* **Negative Correlations** : Longer decline durations and posting fewer videos are more strongly associated with lower chances of recovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot to visualize relationships\n",
    "sns.pairplot(df_sampled_processed, vars=['Recovered', 'Duration', 'Subs_start', 'Mean_duration_difference', 'Mean_frequency_difference', 'Topic_Change'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of the correlation matrix\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts :** This visual analysis confirm the correlation analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, do the regression on all the declines that we kept until now.\n",
    "\n",
    "As a sanity check, we run the same regression, removing the declines that have no videos either before, during or both to check that the presence of zeros does not have a significant impact on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that are not included in the regression\n",
    "drop_cols = ['Channel', 'Start', 'End', 'Duration', 'Recovered', 'Mean_duration_difference', 'Mean_frequency_difference', 'Category']\n",
    "\n",
    "# Prepare the data for the logistic regression\n",
    "logit_X = df_sampled.drop(drop_cols, axis=1)\n",
    "logit_y = df_sampled['Recovered']\n",
    "\n",
    "# Perform the regression\n",
    "logit_result = perform_logistic_regression(logit_X, logit_y)\n",
    "\n",
    "plt.figure(figsize=(6, 5), dpi=400)\n",
    "plot_logit_coefficients(logit_result, title='Logistic regression Coefficients', filename='logit_coefficients.png')\n",
    "\n",
    "# Save the coefficients, p-values and variable names to a file for plotting\n",
    "with open('plot_data/logit_results.csv', 'w') as f:\n",
    "    res = pd.DataFrame({'coef': logit_result.params, 'p-value': logit_result.pvalues}).reset_index()\n",
    "    res.columns = ['Variable', 'Coefficient', 'p-value']\n",
    "    res.to_csv(f, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights:**\n",
    "\n",
    "* **Posted_more**:  Posting more videos during the decline period significantly increases the chances of recovery.\n",
    "\n",
    "* **Posted_shorter_videos** : Posting shorter videos has a marginally significant positive effect on recovery.\n",
    "\n",
    "* **Subs_start** : The number of subscribers at the start of the decline has a marginally significant positive effect on recovery.\n",
    "\n",
    "* **Posted_less** : Posting fewer videos significantly decreases the chances of recovery.\n",
    "\n",
    "* **Category Impact**: Categories with positive coefficients (e.g., Gaming, News & Politics) are more likely to recover. Focus on content that fits these categories.\n",
    "\n",
    "\n",
    "**Actionable Advice:**\n",
    "\n",
    "* **Increase Video Uploads** : Consistently post more videos during the decline period to engage your audience and increase the chances of recovery.\n",
    "\n",
    "* **Avoid Reducing Uploads** : Avoid posting fewer videos, as this significantly decreases the chances of recovery.\n",
    "\n",
    "* **Consider Video Length** : Posting shorter videos may have a positive impact on recovery.\n",
    "\n",
    "* **Focus on Content Categories** : If possible, avoid focusing solely on Education and Music categories, as these are associated with lower recovery rates.\n",
    "\n",
    "* **Leverage Subscriber Base** : Engage with your existing subscribers to maximize their support during the decline period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at how we could answer to Youtuber's questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some potential questions that a Youtube cerator could ask us, let's understand which one we could answer : \n",
    "\n",
    "1. How often should I post new videos?\n",
    "\n",
    "2. Should I focus on shorter or longer videos?\n",
    "**Answer**: This can depend on your audience's preferences. Analyze your video performance data to see if shorter or longer videos perform better. You can also experiment with different lengths to find the optimal duration for your content.\n",
    "\n",
    "3. What type of content should I focus on?\n",
    "**Answer**: Focus on content that has historically performed well on your channel. Additionally, consider creating content that fits into popular categories like Gaming or News & Politics, as these have shown higher recovery rates.\n",
    "\n",
    "4. How can I maintain quality while increasing quantity?\n",
    "**Answer**: Plan your content in advance and create a content calendar. Batch filming and editing can also help you maintain quality while increasing the number of videos you post.\n",
    "\n",
    "5. Will posting more videos affect my channel's overall quality?\n",
    "**Answer**: It's important to strike a balance between quantity and quality. Ensure that each video provides value to your audience. If necessary, consider outsourcing tasks like editing to maintain quality.\n",
    "\n",
    "6. How can I keep my audience engaged with more frequent uploads?\n",
    "**Answer**: Engage with your audience through comments, community posts, and live streams. Ask for their feedback and involve them in your content creation process to keep them interested.\n",
    "\n",
    "7. What if I don't see immediate results from posting more videos?\n",
    "**Answer**: Recovery can take time, so be patient and consistent. Monitor your analytics to track progress and make adjustments as needed. Consistency is key to building and maintaining audience engagement.\n",
    "\n",
    "8. How can I come up with more video ideas?\n",
    "**Answer**: Use tools like YouTube Analytics, Google Trends, and social media to identify trending topics and popular content in your niche. Engage with your audience to get ideas and feedback on what they want to see.\n",
    "\n",
    "9. What other strategies can complement posting more videos?\n",
    "**Answer**: In addition to posting more videos, focus on optimizing your video titles, descriptions, and tags for SEO. Collaborate with other creators, promote your videos on social media, and engage with your audience to boost visibility and engagement.\n",
    "\n",
    "10. How do I measure the success of posting more videos?\n",
    "**Answer**: Use YouTube Analytics to track key metrics such as views, watch time, subscriber growth, and engagement. Compare these metrics before and after increasing your upload frequency to measure the impact.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 : How often should I post new videos?\n",
    "We saw that **uploading more videos** should help to maximize the chances of recovery. Hence we should look at how many videos we should advise him to post each week "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `Posted_more` has a positive impact on recovery, we want to look into what situations benefit the most from posting videos more often.\n",
    "\n",
    "We therefore look at the characteristics of decline which reacted that way, and when it worked best :\n",
    "\n",
    "- Did channels who increased publication frequency already post often, or did they post few videos before the\n",
    "- Is posting more often associated with a lower average video duration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a new dataframe useful for reaction analysis\n",
    "kept_cols = ['Channel', 'Duration', 'Start', 'End', 'Posted_more', 'Posted_less', 'Posted_longer_videos', 'Posted_shorter_videos', 'Recovered', 'Mean_duration_difference', 'Mean_frequency_difference']\n",
    "df_reactions = pd.concat([df_sampled[kept_cols], df_videos_per_week, df_video_duration], axis=1)\n",
    "\n",
    "df_reactions['No_change'] = ~df_reactions['Posted_more'] & ~df_reactions['Posted_less']\n",
    "df_reactions['Frequency_reaction'] = pd.from_dummies(df_reactions[['Posted_more', 'Posted_less', 'No_change']])\n",
    "df_reactions = df_reactions.drop(['Posted_more', 'Posted_less', 'No_change'], axis=1)\n",
    "\n",
    "df_reactions['No_change'] = ~df_reactions['Posted_longer_videos'] & ~df_reactions['Posted_shorter_videos']\n",
    "df_reactions['Video_duration_reaction'] = pd.from_dummies(df_reactions[['Posted_longer_videos', 'Posted_shorter_videos', 'No_change']])\n",
    "df_reactions = df_reactions.drop(['Posted_longer_videos', 'Posted_shorter_videos', 'No_change'], axis=1)\n",
    "\n",
    "df_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_by_frequency_reaction(df_reactions, 'Videos_per_week_before', 'Distribution of videos per week, before the decline')\n",
    "plot_distribution_by_frequency_reaction(df_reactions, 'Videos_per_week_after', 'Distribution of videos per week, after the decline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot description :**\n",
    "\n",
    "* In red are the channels that during their decline didn't change the frequency of their publications, we plot the ditribution of videos published before and after their decline.\n",
    "\n",
    "* In light blue are the channels that during they declined decreased the frequency of their publications, we plot the distribution of videos published before and after their decline.\n",
    "\n",
    "* In dark blue are the channels that during they declined increased the frequency of their publications, we plot the distribution of videos published before and after their decline.\n",
    "\n",
    "* The red line represents the average number of videos published before/after the decline.\n",
    "\n",
    "It is interesting to note that the channels that increased video frequency after the start of the decline used to post less than average before the decline, while the ones that reduced video frequency used to post approximately as much as the average. The two groups almost switch places in terms of video frequency.\n",
    "\n",
    "These observations are good because their decline and recovery can be caused because they posted less videos, and they could have recovered by posting more videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next :** \n",
    "\n",
    "It would be interesting to give an indicator of the **number of videos to post per week** in order to have more chance to recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reaction_posted_more = df_reactions[df_reactions['Frequency_reaction'] == 'Posted_more']\n",
    "\n",
    "stats_before = df_reaction_posted_more[df_reaction_posted_more['Recovered']==1]['Videos_per_week_before'].describe()\n",
    "stats_after = df_reaction_posted_more[df_reaction_posted_more['Recovered']==1]['Videos_per_week_after'].describe()\n",
    "\n",
    "print(\"Statistics for Videos per Week Before Decline:\")\n",
    "print(stats_before)\n",
    "\n",
    "print(\"\\nStatistics for Videos per Week After Decline:\")\n",
    "print(stats_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think that the mean is really meaningful because we can notice quite some outliers, we should therefore preferably look at the median. Here we can observe that before a decline 0.41 videos were posted and after a decline 1.2 videos were posted. Meaning that we could advice to post at least one video per week in order to increase the chance of recovering from the decline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Perform t-test for number of videos per week before the decline\n",
    "t_stat_before, p_value_before = ttest_ind(df_reaction_posted_more[df_reaction_posted_more['Recovered'] == 1]['Videos_per_week_before'],\n",
    "                                          df_reaction_posted_more[df_reaction_posted_more['Recovered'] == 0]['Videos_per_week_before'], equal_var=False)\n",
    "\n",
    "# Perform t-test for number of videos per week after the decline\n",
    "t_stat_after, p_value_after = ttest_ind(df_reaction_posted_more[df_reaction_posted_more['Recovered'] == 1]['Videos_per_week_after'],\n",
    "                                        df_reaction_posted_more[df_reaction_posted_more['Recovered'] == 0]['Videos_per_week_after'], equal_var=False)\n",
    "\n",
    "print(f'T-test for Videos per Week Before Decline: t-statistic = {t_stat_before}, p-value = {p_value_before}')\n",
    "print(f'T-test for Videos per Week After Decline: t-statistic = {t_stat_after}, p-value = {p_value_after}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both t-tests suggest that the number of videos per week (both before and after the decline) is significantly different between channels that recovered and those that did not. This implies that the frequency of video uploads may play a role in a channel's recovery from a decline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Prepare the data for logistic regression\n",
    "X = df_reaction_posted_more[['Mean_frequency_difference', 'Mean_duration_before', 'Mean_duration_after']]\n",
    "y = df_reaction_posted_more['Recovered'].astype(int)  # Ensure the target variable is integer\n",
    "\n",
    "# Add a constant to the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "print(X.dtypes)\n",
    "# Fit the logistic regression model\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "* **Intercept (const)**: `Statistically significant` (p-value = 0.022), indicating a baseline log-odds of recovery.\n",
    "* **Mean_frequency_difference**: `Statistically significant` (p-value = 0.016), suggesting that the mean frequency difference has a significant positive effect on the recovery rate.\n",
    "* **Mean_duration_before**: `Statistically significant` (p-value = 0.020), suggesting that the mean duration of videos before the decline has a significant positive effect on the recovery rate.\n",
    "* **Mean_duration_after**: `Not statistically significant` (p-value = 0.703), suggesting that the mean duration of videos after the decline does not have a significant effect on the recovery rate.\n",
    "\n",
    "The only variable that migth be interesting to look into is the mean frequency difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse relationship between mean frequency difference and recovery\n",
    "correlation = df_sampled_processed['Mean_frequency_difference'].corr(df_sampled_processed['Recovered'])\n",
    "print(f'Correlation between Mean Frequency Difference and Recovery: {correlation}')\n",
    "\n",
    "# Plot the relationship between mean frequency difference and recovery\n",
    "sns.scatterplot(x='Mean_frequency_difference', y='Recovered', data=df_sampled_processed)\n",
    "plt.title('Upload Frequency vs. Recovery')\n",
    "plt.xlabel('Upload Frequency (videos per week)')\n",
    "plt.ylabel('Recovery')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO supprimer si pas pertinent\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.kdeplot(df_sampled[df_sampled['Recovered'] == True]['Mean_frequency_difference'], label='Recovered', fill=True, alpha=0.5)\n",
    "sns.kdeplot(df_sampled[df_sampled['Recovered'] == False]['Mean_frequency_difference'], label='Not Recovered', fill=True, alpha=0.5)\n",
    "plt.xlim(-20, 20)\n",
    "plt.title('Distribution of Video Frequency by Recovery Status')\n",
    "plt.xlabel('Upload Frequency (videos per week)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to advise a creator of the number of videos to upload, we should look at the probability to recover of each different frequency of upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 0.5, 1, 2, 3, 4, 5, 10]\n",
    "labels = ['<0.5', '0.5-1', '1-2', '2-3', '3-4', '4-5', '>5']\n",
    "\n",
    "df = df_sampled_processed.copy()\n",
    "\n",
    "# Bin the upload frequencies\n",
    "df['Frequency_bin'] = pd.cut(df['Mean_frequency_difference'], bins=bins, labels=labels)\n",
    "\n",
    "# Calculate the average recovery rate for each bin\n",
    "recovery_by_frequency = df.groupby('Frequency_bin')['Recovered'].mean().reset_index()\n",
    "\n",
    "# Plot the recovery rates by upload frequency\n",
    "sns.barplot(x='Frequency_bin', y='Recovered', data=recovery_by_frequency)\n",
    "plt.title('Average Recovery Rate by Upload Frequency')\n",
    "plt.xlabel('Upload Frequency (videos per week)')\n",
    "plt.ylabel('Average Recovery Rate')\n",
    "plt.ylim(0.4, 0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_upload_frequency = df_sampled_processed['Mean_frequency_difference'].mean()\n",
    "\n",
    "# Example recommendation based on the barplot\n",
    "if current_upload_frequency < 0.5:\n",
    "    recommendation = \"You should increase your upload frequency to at least 1 video per week to improve your chances of recovery.\"\n",
    "elif 0.5 <= current_upload_frequency < 1:\n",
    "    recommendation = \"You are currently uploading less than 1 video per week. Aim to increase your upload frequency to 1-2 videos per week.\"\n",
    "elif 1 <= current_upload_frequency < 2:\n",
    "    recommendation = \"You are in the optimal range of 1-2 videos per week. Maintain this frequency to maximize your chances of recovery.\"\n",
    "elif 2 <= current_upload_frequency < 3:\n",
    "    recommendation = \"You might want to upload less frequently to 1-2 videos per week to improve your chances of recovery or to upload more frequently to 3-4 videos per week\"\n",
    "else:\n",
    "    recommendation = \"You are uploading more than 3 videos per week. Ensure that you maintain the quality of your content while keeping up with this frequency.\"\n",
    "\n",
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 : Should I focus on shorter or longer videos? \n",
    "\n",
    "Now I know that I should upload more frequently, but what about the duration of my videos \n",
    "\n",
    "From the regression, we observed that changing the duration of a video doesn't change much. But let's still be a bit curious and see if we can extract something out of it \n",
    "\n",
    "Here we want to visualize if changing the duration of the videos uploaded change the rate of recovery. We observe that if there is no change in the duration of the videos uploaded then the creator has 50% chance of recovery. If the creator posts shorter videos, he has 50% chance of recovery as well and if he posts longer videos then he has 47% chance of recovery. \n",
    "\n",
    "So by just visualizing the data, we may want to conclude that during a decline changing the duration of the videos uploaded doesn't change the recovery. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between the video duration reaction and recovery\n",
    "df_reaction_posted_more['Recovered'] = df_reaction_posted_more['Recovered'].astype(int)\n",
    "sns.barplot(data=df_reaction_posted_more, x='Video_duration_reaction', y='Recovered', errorbar=None)\n",
    "plt.title('Recovery vs Video Duration Reaction')\n",
    "plt.xlabel('Video Duration Reaction')\n",
    "plt.ylabel('Recovery Rate')\n",
    "plt.ylim(0.4, 0.55)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the column name of Mean_duration_difference to mean_video_duration\n",
    "df.rename(columns={'Mean_duration_difference': 'Mean_video_duration'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation\n",
    "correlation = df['Mean_video_duration'].corr(df['Recovered'])\n",
    "print(f'Correlation between mean video duration and recovery: {correlation}')\n",
    "\n",
    "# Plot the relationship\n",
    "sns.scatterplot(x='Mean_video_duration', y='Recovered', data=df)\n",
    "plt.title('Mean Video Duration vs. Recovery')\n",
    "plt.xlabel('Mean Video Duration (minutes)')\n",
    "plt.ylabel('Recovery')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO supprimer si pas pertinent\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.kdeplot(df[df['Recovered'] == True]['Mean_video_duration'], label='Recovered', fill=True, alpha=0.5)\n",
    "sns.kdeplot(df[df['Recovered'] == False]['Mean_video_duration'], label='Not Recovered', fill=True, alpha=0.5)\n",
    "plt.xlim(-2000, 2000)\n",
    "plt.title('Distribution of Video Durations by Recovery Status')\n",
    "plt.xlabel('Mean video duration')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define duration bins\n",
    "duration_bins = [0*60, 5*60, 10*60, 15*60, 20*60, 30*60, 60*60, 120*60]\n",
    "duration_labels = ['<5', '5-10', '10-15', '15-20', '20-30', '30-60', '>60']\n",
    "\n",
    "# Bin the video durations\n",
    "df['Duration_bin'] = pd.cut(df['Mean_video_duration'], bins=duration_bins, labels=duration_labels)\n",
    "\n",
    "# Calculate the average recovery rate for each bin\n",
    "recovery_by_duration = df.groupby('Duration_bin')['Recovered'].mean().reset_index()\n",
    "\n",
    "# Plot the recovery rates by video duration\n",
    "sns.barplot(x='Duration_bin', y='Recovered', data=recovery_by_duration)\n",
    "plt.title('Average Recovery Rate by Video Duration')\n",
    "plt.xlabel('Mean Video Duration (minutes)')\n",
    "plt.ylabel('Average Recovery Rate')\n",
    "plt.ylim(0.35, 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 : What type of content should i focus on ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sampled.copy()\n",
    "\n",
    "# Calculate the average recovery rate for each content category\n",
    "recovery_by_category = df.groupby('Category')['Recovered'].mean().reset_index()\n",
    "\n",
    "# Sort the categories by recovery rate\n",
    "recovery_by_category = recovery_by_category.sort_values(by='Recovered', ascending=False)\n",
    "\n",
    "# Display the recovery rates by category\n",
    "print(recovery_by_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the recovery rates by content category\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Recovered', y='Category', data=recovery_by_category, palette='viridis')\n",
    "plt.title('Average Recovery Rate by Content Category')\n",
    "plt.xlabel('Average Recovery Rate')\n",
    "plt.ylabel('Content Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example recommendation based on the barplot\n",
    "high_recovery_categories = recovery_by_category[recovery_by_category['Recovered'] > 0.45]['Category'].tolist()\n",
    "low_recovery_categories = recovery_by_category[recovery_by_category['Recovered'] <= 0.45]['Category'].tolist()\n",
    "\n",
    "recommendation = f\"To maximize your chances of recovery, focus on creating content in the following categories: {', '.join(high_recovery_categories)}. These categories have shown higher recovery rates. Consider avoiding or minimizing content in the following categories: {', '.join(low_recovery_categories)}, as they have shown lower recovery rates.\"\n",
    "\n",
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q? : Should I change the topic of my videos? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does a change of topic influence the recovery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions_topics = pd.merge(df_reactions, topic_change_data, left_index=True, right_on='Decline', how='left')\n",
    "df_reactions_topics = df_reactions_topics.dropna()\n",
    "\n",
    "change_vs_unchanged = df_reactions_topics.groupby('Topic_Change')['Recovered'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Topic_Change', y='Recovered', data=change_vs_unchanged, palette='viridis')\n",
    "plt.title('Impact of Topic Change on Recovery Rate')\n",
    "plt.xlabel('Topic Changed')\n",
    "plt.ylabel('Recovery Rate')\n",
    "plt.xticks([0, 1], ['No', 'Yes'])\n",
    "plt.ylim(0.4, 0.45)\n",
    "plt.show()\n",
    "\n",
    "print(change_vs_unchanged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very small variation, not statistically significant \\\n",
    "BUT maybe different topic transitions have different correlations with the recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of different toopic changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions_changed_topic = df_reactions_topics[df_reactions_topics['Topic_Change'] == True]\n",
    "df_reactions_changed_topic = df_reactions_changed_topic.drop(columns=['Decline', 'Topic_Change'])\n",
    "df_reactions_changed_topic = df_reactions_changed_topic.dropna()\n",
    "df_reactions_changed_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Topic_Before and Topic_After to calculate recovery rates\n",
    "topic_transitions = df_reactions_changed_topic.groupby(['Topic_Before', 'Topic_After']).agg(\n",
    "    recovery_rate=('Recovered', 'mean'),\n",
    "    count=('Recovered', 'size')\n",
    ").reset_index()\n",
    "\n",
    "# Filter for meaningful transitions, with more than 10 cases\n",
    "topic_transitions = topic_transitions[topic_transitions['count'] > 10]\n",
    "topic_transitions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_data = topic_transitions.pivot(\n",
    "    index='Topic_Before', \n",
    "    columns='Topic_After', \n",
    "    values='recovery_rate'\n",
    ")\n",
    "\n",
    "# Heatmap of the recovery rates by topic transitions\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(pivot_data, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={'label': 'Recovery Rate'})\n",
    "plt.title('Recovery Rate by Topic Transition')\n",
    "plt.xlabel('Topic After')\n",
    "plt.ylabel('Topic Before')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort transitions by recovery rate\n",
    "sorted_transitions = topic_transitions.sort_values(by='recovery_rate', ascending=False)\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    data=sorted_transitions,\n",
    "    x='recovery_rate',\n",
    "    y=sorted_transitions.apply(lambda row: f\"{row['Topic_Before']} -> {row['Topic_After']}\", axis=1),\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Recovery Rate by Topic Transition')\n",
    "plt.xlabel('Recovery Rate')\n",
    "plt.ylabel('Topic Transition')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(\n",
    "    sorted_transitions,\n",
    "    x='recovery_rate',\n",
    "    y=sorted_transitions.apply(lambda row: f\"{row['Topic_Before']} -> {row['Topic_After']} (n={row['count']})\", axis=1),\n",
    "    orientation='h',\n",
    "    title='Recovery Rate by Topic Transition',\n",
    "    labels={'x': 'Recovery Rate', 'y': 'Topic Transition'},\n",
    "    hover_data=['count']\n",
    ")\n",
    "fig.update_layout(height=1000) \n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
