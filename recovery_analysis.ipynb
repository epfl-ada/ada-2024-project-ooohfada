{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from src.data.dataloader_functions import *\n",
    "from src.utils.results_utils import *\n",
    "from src.utils.recovery_analysis_utils import *\n",
    "from src.utils.plots_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "# Make the code reproducible\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_declines_original = pd.read_csv('data/decline_events_complete.csv')\n",
    "df_channels = pd.read_csv('data/df_channels_en.tsv', sep='\\t', usecols=['channel', 'category_cc'], index_col='channel')\n",
    "df_data_processed = load_processed_data(usecols=['channel', 'week', 'subs', 'activity', 'views'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_declines_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the outcome\n",
    "\n",
    "Using the duration of the decline, determine whether the YouTuber recovered or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_declines = add_declines_to_db(df_all_declines_original, df_channels, df_data_processed)\n",
    "print(f\"Overall recovery rate: {df_all_declines['Recovered'].mean():.2f}\")\n",
    "df_all_declines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is the recovery distributed?\n",
    "\n",
    "To get a first idea of what factors come into play when a YouTuber tries to recover from a decline, we plot some distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_recovered_by_categories(df_all_declines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_group_distributions(df_all_declines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that some features are not balanced between the declines that recovered and those that did not, especially views and subscribers at the start of the decline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_declines['Recovered'].value_counts())\n",
    "print(f\"\\nTotal number of declines: {len(df_all_declines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTuber reactions\n",
    "\n",
    "Aiming at finding the best ways to deal with a decline depending on the situation, we take a look at how the YouTubers reacted to the decline, and what methods proved effective.\\\n",
    "In order to observe the reaction's impact, we will conduct a matched observational study on the dataset by using propensity score matching.\n",
    "\n",
    "Considering the size of the dataset, we use random sampling to ease the matching's computation.\\\n",
    "To check that sampling does not mess with the distribution of recoveries, we plot them depending on the sampling proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sampling_rates(df_all_declines, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to sample 30% of the data since it considerably reduces the size of the dataset, and allows to keep a representative sample of the data without perturbing the recovery distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df_all_declines.sample(frac=0.3, replace=False, random_state=SEED)\n",
    "\n",
    "print(df_sampled['Recovered'].value_counts())\n",
    "print(f\"\\nTotal number of declines after sampling: {len(df_sampled)}\")\n",
    "\n",
    "# Save the data for the plots\n",
    "df_sampled.to_csv('plot_data/decline_events.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the reaction metrics whose impact we want to measure\n",
    "\n",
    "The reactions that we are able to observe here are the following:\n",
    "- Did the YouTuber change video publication frequency?\n",
    "- Did the YouTuber change video length?\n",
    "- Did the YouTuber change video category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the videos around the declines, from week (decline start - decline duration) to week (decline end)\n",
    "videos_around_declines = pd.read_csv('data/videos_around_declines.csv')\n",
    "\n",
    "# Add the declines with the indices of the corresponding videos\n",
    "df_sampled = get_sampled_declines_with_videos(df_sampled, videos_around_declines)\n",
    "\n",
    "# Augment the data with the video stats : videos per week and mean video duration, before and during the declines\n",
    "df_sampled = add_video_stats(df_sampled, videos_around_declines)\n",
    "\n",
    "DIV_BY_ZERO_TOLERANCE = 1e-6\n",
    "\n",
    "\n",
    "# Calculate mean duration differences\n",
    "df_sampled = calculate_difference(df_sampled, 'Mean_duration_after', 'Mean_duration_before', 'Mean_duration_difference')\n",
    "df_sampled = add_change_columns(df_sampled, 'Mean_duration_difference', 'Mean_duration_before', 'Posted_longer_videos', 'Posted_shorter_videos', DIV_BY_ZERO_TOLERANCE, threshold=0.5)\n",
    "print_stats(df_sampled, 'Posted_longer_videos', 'Posted_shorter_videos', 'mean video duration')\n",
    "\n",
    "# Calculate mean frequency differences\n",
    "df_sampled = calculate_difference(df_sampled, 'Videos_per_week_after', 'Videos_per_week_before', 'Mean_frequency_difference')\n",
    "df_sampled = add_change_columns(df_sampled, 'Mean_frequency_difference', 'Videos_per_week_before', 'Posted_more', 'Posted_less', DIV_BY_ZERO_TOLERANCE, threshold=0.5)\n",
    "print_stats(df_sampled, 'Posted_more', 'Posted_less', 'publishing frequency')\n",
    "\n",
    "# Merge and analyze topic changes\n",
    "df_sampled = merge_and_report_topic_changes(df_sampled, 'data/df_topic_change_20_15w.csv')\n",
    "df_sampled = df_sampled.dropna()# Drop unnecessary columns\n",
    "\n",
    "# Save the dataframes for later and drop unnecessary columns\n",
    "df_videos_per_week = df_sampled[['Videos_per_week_before', 'Videos_per_week_after']]\n",
    "df_video_duration = df_sampled[['Mean_duration_before', 'Mean_duration_after']]\n",
    "df_sampled = df_sampled.drop(columns=['Mean_duration_before', 'Mean_duration_after', 'Videos_per_week_before', 'Videos_per_week_after'])\n",
    "\n",
    "# Drop videos indices\n",
    "df_sampled = df_sampled.drop(columns=['Videos_before', 'Videos_after'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise mean video duration at the start of the decline as it has just been added (for the datastory)\n",
    "plt.figure(figsize=(7, 3))\n",
    "\n",
    "sns.histplot(data=df_video_duration / 60, x=\"Mean_duration_before\", hue=df_sampled['Recovered'], log_scale=True, element=\"step\", palette=[RED, GREEN],)\n",
    "plt.title('Distribution of channels by mean video duration \\nat the start of the decline')\n",
    "plt.xlabel('Mean video duration (minutes)')\n",
    "plt.ylabel('Number of channels')\n",
    "\n",
    "plt.savefig('plot_data/mean_video_duration_start_decline.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the categorical variable into dummies\n",
    "df_correlation = df_sampled[['Recovered', 'Posted_more', 'Posted_less', 'Posted_longer_videos', 'Posted_shorter_videos', 'Topic_change']].copy()\n",
    "\n",
    "for col in df_correlation.columns:\n",
    "    df_correlation[col] = df_correlation[col].astype(int)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_correlation.corr()\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix['Recovered'].sort_values(ascending=False))\n",
    "\n",
    "# Save the correlation matrix as a CSV file for the plots\n",
    "correlation_matrix.to_csv('plot_data/correlation_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**:\n",
    "\n",
    "* **Positive Correlations** : Posting more videos (higher upload frequency) is slightly associated with recovery.\n",
    "\n",
    "* **Negative Correlations** : Posting fewer videos are more strongly associated with lower chances of recovery.\n",
    "\n",
    "* **No Correlations** : Other factors are nearly not correlated with recovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of the correlation matrix\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts :** This visual analysis confirm the correlation analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, do the regression on all the declines that we kept until now.\n",
    "\n",
    "As a sanity check, we run the same regression, removing the declines that have no videos either before, during or both to check that the presence of zeros does not have a significant impact on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that are not included in the regression\n",
    "drop_cols = ['Channel', 'Start', 'End', 'Duration', 'Recovered', 'Mean_duration_difference', 'Mean_frequency_difference', 'Category', 'Subs_start', 'Views_start', 'Activity_start']\n",
    "\n",
    "# Prepare the data for the logistic regression\n",
    "logit_X = df_sampled.drop(drop_cols, axis=1)\n",
    "logit_y = df_sampled['Recovered']\n",
    "\n",
    "# Perform the regression\n",
    "logit_result = perform_logistic_regression(logit_X, logit_y)\n",
    "\n",
    "plt.figure(figsize=(6, 5), dpi=400)\n",
    "plot_logit_coefficients(logit_result, title='Logistic regression Coefficients')\n",
    "\n",
    "# Save the coefficients, p-values and variable names to a file for plotting\n",
    "with open('plot_data/logit_results.csv', 'w') as f:\n",
    "    res = pd.DataFrame({'coef': logit_result.params, 'p-value': logit_result.pvalues}).reset_index()\n",
    "    res.columns = ['Variable', 'Coefficient', 'p-value']\n",
    "    res.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights:**\n",
    "\n",
    "* **Posted_more**:  Posting more videos during the decline period significantly increases the chances of recovery.\n",
    "\n",
    "* **Posted_shorter_videos** : Posting shorter videos has a marginally significant positive effect on recovery.\n",
    "\n",
    "* **Posted_less** : Posting fewer videos significantly decreases the chances of recovery.\n",
    "* \n",
    "\n",
    "**Actionable Advice:**\n",
    "\n",
    "* **Increase Video Uploads** : Consistently post more videos during the decline period to engage your audience and increase the chances of recovery.\n",
    "\n",
    "* **Avoid Reducing Uploads** : Avoid posting fewer videos, as this significantly decreases the chances of recovery.\n",
    "\n",
    "* **Consider Video Length** : Posting shorter videos may have a positive impact on recovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propensity score matching\n",
    "\n",
    "Seeing that the declines do not have the same distribution on their features, we perform propensity score matchings to balance the treatment and control groups looking at the effect that changing publication frequency,video duration and video category after the start of the decline have on the recovery.\n",
    "\n",
    "After the propensity score matching, we can observe the effect of the different treatments on the recovery :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variables to be used for the matching\n",
    "# Treatments in the same array will be plotted together\n",
    "TREATMENTS = [\n",
    "    ['Posted_more',\n",
    "    'Posted_less'],\n",
    "    ['Posted_longer_videos',\n",
    "    'Posted_shorter_videos'],\n",
    "    ['Topic_change'], \n",
    "]\n",
    "\n",
    "# The variables to be dropped for each treatment (to avoid multicollinearity or strong correlation)\n",
    "# Use the same order as the TREATMENTS array\n",
    "to_drop = [\n",
    "    [['Mean_frequency_difference', 'Posted_less'],\n",
    "    ['Mean_frequency_difference', 'Posted_more']],\n",
    "    [['Mean_duration_difference', 'Posted_shorter_videos'],\n",
    "    ['Mean_duration_difference', 'Posted_longer_videos']],\n",
    "    [[]]\n",
    "]\n",
    "\n",
    "plot_df = pd.DataFrame(columns=['Strategy', 'Adopted the strategy', 'Did not adopt'])\n",
    "\n",
    "matched_dfs = {}\n",
    "for plot_treatments, plot_dropped in zip(TREATMENTS, to_drop):\n",
    "    fig, axes = plt.subplots(1, len(plot_treatments), figsize=(5*len(plot_treatments), 4))\n",
    "    for subplot_id, (treatment, dropped) in enumerate(zip(plot_treatments, plot_dropped)):\n",
    "\n",
    "        # Try to load the matches from the file, otherwise compute them\n",
    "        matches = get_matches(treatment=treatment, declines=df_sampled.drop(dropped, axis=1), verbose=False)\n",
    "\n",
    "        print(f\"{treatment} matches :\", matches)\n",
    "\n",
    "        # Flatten\n",
    "        matches = [index for match in matches for index in match]\n",
    "\n",
    "        # Get the matched declines\n",
    "        matched_dfs[treatment] = df_sampled.loc[matches]\n",
    "\n",
    "        counts = matched_dfs[treatment].groupby(treatment)['Recovered'].mean() * 100\n",
    "        plot_df.loc[len(plot_df)] = [treatment, counts[True], counts[False]]\n",
    "\n",
    "        plot_treatment_effect(matched_dfs[treatment], treatment, ax=axes[subplot_id] if len(plot_treatments) > 1 else axes)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_df.to_csv('plot_data/matches_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for reporting\n",
    "t_test_results = []\n",
    "\n",
    "for treatment, df in matched_dfs.items():\n",
    "    # Separate groups and convert 'Recovered' to numeric\n",
    "    group_adopted = df[df[treatment] == True]['Recovered'].astype(float)\n",
    "    group_not_adopted = df[df[treatment] == False]['Recovered'].astype(float)\n",
    "    \n",
    "    # Perform t-test\n",
    "    t_stat, p_value = ttest_ind(group_adopted, group_not_adopted, equal_var=False)  # Welch's t-test for unequal variances\n",
    "\n",
    "    # Calculate means for reporting\n",
    "    mean_adopted = group_adopted.mean()\n",
    "    mean_not_adopted = group_not_adopted.mean()\n",
    "    \n",
    "    # Determine significance\n",
    "    significant = p_value < 0.05\n",
    "\n",
    "    # Store results\n",
    "    t_test_results.append({\n",
    "        \"Treatment\": treatment,\n",
    "        \"Mean Adopted\": mean_adopted,\n",
    "        \"Mean Not Adopted\": mean_not_adopted,\n",
    "        \"T-Statistic\": t_stat,\n",
    "        \"P-Value\": p_value,\n",
    "        \"Significant\": significant\n",
    "    })\n",
    "\n",
    "# Display results as a DataFrame\n",
    "t_test_results_df = pd.DataFrame(t_test_results)\n",
    "print(t_test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at how we could answer to Youtuber's questions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 : How often should I post new videos?\n",
    "We saw that **uploading more videos** should help to maximize the chances of recovery. Hence we should look at how many videos we should advise him to post each week "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `Posted_more` has a positive impact on recovery, we want to look into what situations benefit the most from posting videos more often.\n",
    "\n",
    "We therefore look at the characteristics of decline which reacted that way, and when it worked best :\n",
    "\n",
    "- Did channels who increased publication frequency already post often, or did they post few videos before the\n",
    "- Is posting more often associated with a lower average video duration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the reaction dataframe\n",
    "df_reactions = build_reaction_dataframe(df_sampled, df_videos_per_week, df_video_duration)\n",
    "df_reactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_by_frequency_reaction(df_reactions, 'Videos_per_week_before', 'Distribution of videos per week, before the decline')\n",
    "plot_distribution_by_frequency_reaction(df_reactions, 'Videos_per_week_after', 'Distribution of videos per week, after the decline')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot description :**\n",
    "\n",
    "* In red are the channels that during their decline didn't change the frequency of their publications, we plot the ditribution of videos published before and after their decline.\n",
    "\n",
    "* In light blue are the channels that during they declined decreased the frequency of their publications, we plot the distribution of videos published before and after their decline.\n",
    "\n",
    "* In dark blue are the channels that during they declined increased the frequency of their publications, we plot the distribution of videos published before and after their decline.\n",
    "\n",
    "* The red line represents the average number of videos published before/after the decline.\n",
    "\n",
    "It is interesting to note that the channels that increased video frequency after the start of the decline used to post less than average before the decline, while the ones that reduced video frequency used to post approximately as much as the average. The two groups almost switch places in terms of video frequency.\n",
    "\n",
    "These observations are good because their decline and recovery can be caused because they posted less videos, and they could have recovered by posting more videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next :** \n",
    "\n",
    "It would be interesting to give an indicator of the **number of videos to post per week** in order to have more chance to recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reaction_posted_more = df_reactions[df_reactions['Frequency_reaction'] == 'Posted_more']\n",
    "\n",
    "stats_before = df_reaction_posted_more[df_reaction_posted_more['Recovered']==1]['Videos_per_week_before'].describe()\n",
    "stats_after = df_reaction_posted_more[df_reaction_posted_more['Recovered']==1]['Videos_per_week_after'].describe()\n",
    "\n",
    "print(\"Statistics for Videos per Week Before Decline:\")\n",
    "print(stats_before)\n",
    "\n",
    "print(\"\\nStatistics for Videos per Week After Decline:\")\n",
    "print(stats_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think that the mean is really meaningful because we can notice quite some outliers, we should therefore preferably look at the median. Here we can observe that before a decline 0.41 videos were posted and after a decline 1.2 videos were posted. Meaning that we could advice to post at least one video per week in order to increase the chance of recovering from the decline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-test for number of videos per week before the decline\n",
    "t_stat_before, p_value_before = ttest_ind(df_reaction_posted_more[df_reaction_posted_more['Recovered'] == 1]['Videos_per_week_before'],\n",
    "                                          df_reaction_posted_more[df_reaction_posted_more['Recovered'] == 0]['Videos_per_week_before'], equal_var=False)\n",
    "\n",
    "# Perform t-test for number of videos per week after the decline\n",
    "t_stat_after, p_value_after = ttest_ind(df_reaction_posted_more[df_reaction_posted_more['Recovered'] == 1]['Videos_per_week_after'],\n",
    "                                        df_reaction_posted_more[df_reaction_posted_more['Recovered'] == 0]['Videos_per_week_after'], equal_var=False)\n",
    "\n",
    "print(f'T-test for Videos per Week Before Decline: t-statistic = {t_stat_before}, p-value = {p_value_before}')\n",
    "print(f'T-test for Videos per Week After Decline: t-statistic = {t_stat_after}, p-value = {p_value_after}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both t-tests suggest that the number of videos per week (both before and after the decline) is significantly different between channels that recovered and those that did not. This implies that the frequency of video uploads may play a role in a channel's recovery from a decline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for logistic regression\n",
    "X = df_reaction_posted_more[['Mean_frequency_difference', 'Mean_duration_before', 'Mean_duration_after']]\n",
    "y = df_reaction_posted_more['Recovered'].astype(int)  # Ensure the target variable is integer\n",
    "\n",
    "# Add a constant to the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "print(X.dtypes)\n",
    "# Fit the logistic regression model\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "* **Intercept (const)**: `Statistically significant` (p-value = 0.022), indicating a baseline log-odds of recovery.\n",
    "* **Mean_frequency_difference**: `Statistically significant` (p-value = 0.016), suggesting that the mean frequency difference has a significant positive effect on the recovery rate.\n",
    "* **Mean_duration_before**: `Statistically significant` (p-value = 0.020), suggesting that the mean duration of videos before the decline has a significant positive effect on the recovery rate.\n",
    "* **Mean_duration_after**: `Not statistically significant` (p-value = 0.703), suggesting that the mean duration of videos after the decline does not have a significant effect on the recovery rate.\n",
    "\n",
    "The only variable that migth be interesting to look into is the mean frequency difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse relationship between mean frequency difference and recovery\n",
    "correlation = df_sampled['Mean_frequency_difference'].corr(df_sampled['Recovered'])\n",
    "print(f'Correlation between Mean Frequency Difference and Recovery: {correlation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to advise a creator of the number of videos to upload, we should look at the probability to recover of each different frequency of upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propensity Score Matching on each frequency bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels for upload frequency\n",
    "bins = [0, 0.5, 1, 2, 3, 4, 5, 10]\n",
    "labels = ['<0.5', '0.5-1', '1-2', '2-3', '3-4', '4-5', '>5']\n",
    "\n",
    "df = df_sampled.drop(columns=(['Mean_frequency_difference', 'Posted_more', 'Posted_less']))\n",
    "matched_dfs = {}\n",
    "\n",
    "df['Frequency_bin'] = pd.cut(df_videos_per_week['Videos_per_week_after'], bins=bins, labels=labels)\n",
    "\n",
    "plot_df = pd.DataFrame(columns=['Frequency_bin', 'Recovery_rate'])\n",
    "\n",
    "for bin_label in labels:\n",
    "    df['Is_in_bin'] = (df['Frequency_bin'] == bin_label)\n",
    "    print(f'Processing bin: {bin_label} ({df[\"Is_in_bin\"].sum() / len(df) * 100:.2f}% of declines)')\n",
    "\n",
    "    df_dropped = df.drop(columns = ['Frequency_bin'])\n",
    "    \n",
    "    # Perform PSM for the treatment of interest\n",
    "    matches = get_matches(treatment='Is_in_bin', declines=df_dropped, verbose=False)\n",
    "\n",
    "    matched_indices_flat = [index for match in matches for index in match]\n",
    "    matched_df = df.loc[matched_indices_flat]\n",
    "\n",
    "    # Calculate recovery rate for \"in bin\" (True)\n",
    "    recovery_rate = matched_df.groupby('Is_in_bin')['Recovered'].mean() * 100\n",
    "\n",
    "    matched_dfs[bin_label] = matched_df\n",
    "    if True in recovery_rate.index: # Append recovery rate for the current bin\n",
    "        plot_df.loc[len(plot_df)] = [bin_label, recovery_rate[True]]\n",
    "    else:\n",
    "        plot_df.loc[len(plot_df)] = [bin_label, 0]\n",
    "print(plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=plot_df, x='Frequency_bin', y='Recovery_rate', errorbar=None)\n",
    "plt.title(f'Recovery by Upload Frequency')\n",
    "plt.xlabel('Upoad Frequency (per week)')\n",
    "plt.ylabel('Recovery Rate (%)')\n",
    "plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see on this histogram what upload frequencies have the best average recovery rates.\n",
    "\n",
    "## Q2 : Should I focus on shorter or longer videos? \n",
    "\n",
    "Now I know that I should upload more frequently, but what about the duration of my videos \n",
    "\n",
    "From the regression, we observed that changing the duration of a video doesn't change much. But let's still be a bit curious and see if we can extract something out of it \n",
    "\n",
    "Here we want to visualize if changing the duration of the videos uploaded change the rate of recovery. We observe that if there is no change in the duration of the videos uploaded then the creator has 50% chance of recovery. If the creator posts shorter videos, he has 50% chance of recovery as well and if he posts longer videos then he has 47% chance of recovery. \n",
    "\n",
    "So by just visualizing the data, we may want to conclude that during a decline changing the duration of the videos uploaded doesn't change the recovery. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation\n",
    "correlation = df['Mean_video_duration'].corr(df['Recovered'])\n",
    "print(f'Correlation between mean video duration and recovery: {correlation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Propensity Score Matching on each duration bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels for upload frequency\n",
    "duration_bins = [0*60, 5*60, 10*60, 15*60, 20*60, 30*60, 60*60, 120*60]\n",
    "duration_labels = ['<5', '5-10', '10-15', '15-20', '20-30', '30-60', '>60']\n",
    "\n",
    "df = df_sampled.copy()\n",
    "df = df.dropna()\n",
    "\n",
    "matched_dfs = {}\n",
    "\n",
    "# Bin the video durations\n",
    "df['Duration_bin'] = pd.cut(df_video_duration['Mean_duration_after'], bins=duration_bins, labels=duration_labels)\n",
    "print(df['Duration_bin'])\n",
    "plot_df = pd.DataFrame(columns=['Duration_bin', 'Recovery_rate'])\n",
    "\n",
    "bin_counts = df['Duration_bin'].value_counts()\n",
    "print(\"Number of samples in each bin:\")\n",
    "print(bin_counts)\n",
    "\n",
    "for bin_label in duration_labels:\n",
    "    df['Is_in_bin_duration'] = (df['Duration_bin'] == bin_label)\n",
    "    df = df.dropna()\n",
    "    print(f'Processing bin: {bin_label}')\n",
    "\n",
    "    df_dropped = df.drop(columns = ['Duration_bin']).dropna()\n",
    "    \n",
    "\n",
    "    # Scale the data if necessary (especially for numerical columns)\n",
    "    if (df_dropped == np.inf).any().any() or (df_dropped.isna()).any().any():\n",
    "        print(f\"Data contains NaN or Infinite values for {bin_label}.\")\n",
    "        continue  # Skip this bin if data is problematic\n",
    "\n",
    "    df_dropped = df_dropped.loc[:, df_dropped.nunique() > 1]\n",
    "\n",
    "\n",
    "    # Perform PSM for the treatment of interest\n",
    "    matches = get_matches(treatment='Is_in_bin_duration', declines=df_dropped, verbose=False)\n",
    "\n",
    "    matched_indices_flat = [index for match in matches for index in match]\n",
    "    matched_df = df.loc[matched_indices_flat]\n",
    "\n",
    "    # Calculate recovery rate for \"in bin\" (True)\n",
    "    recovery_rate = matched_df.groupby('Is_in_bin_duration')['Recovered'].mean() * 100\n",
    "\n",
    "    matched_dfs[bin_label] = matched_df\n",
    "    if True in recovery_rate.index: # Append recovery rate for the current bin\n",
    "        plot_df.loc[len(plot_df)] = [bin_label, recovery_rate[True]]\n",
    "    else:\n",
    "        plot_df.loc[len(plot_df)] = [bin_label, 0]\n",
    "print(plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=plot_df, x='Duration_bin', y='Recovery_rate', errorbar=None)\n",
    "plt.title(f'Recovery by Video Duration')\n",
    "plt.xlabel('Mean Video Duration (minutes)')\n",
    "plt.ylabel('Recovery Rate (%)')\n",
    "plt.ylim(35, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 : What type of content should i focus on ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sampled.copy()\n",
    "\n",
    "# Calculate the average recovery rate for each content category\n",
    "recovery_by_category = df.groupby('Category')['Recovered'].mean().reset_index()\n",
    "\n",
    "# Sort the categories by recovery rate\n",
    "recovery_by_category = recovery_by_category.sort_values(by='Recovered', ascending=False)\n",
    "\n",
    "# Display the recovery rates by category\n",
    "print(recovery_by_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_horizontal_barplot('Content Category', recovery_by_category, 'Recovered', 'Category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is naive and does not lead to any conclusions regarding what reactions are best. \n",
    "\n",
    "We will therefore explore topic changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example recommendation based on the barplot\n",
    "high_recovery_categories = recovery_by_category[recovery_by_category['Recovered'] > 0.45]['Category'].tolist()\n",
    "low_recovery_categories = recovery_by_category[recovery_by_category['Recovered'] <= 0.45]['Category'].tolist()\n",
    "\n",
    "recommendation = f\"To maximize your chances of recovery, focus on creating content in the following categories: {', '.join(high_recovery_categories)}. These categories have shown higher recovery rates. Consider avoiding or minimizing content in the following categories: {', '.join(low_recovery_categories)}, as they have shown lower recovery rates.\"\n",
    "\n",
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q? : Should I change the topic of my videos? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does a change of topic influence the recovery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_change_data = pd.read_csv('data/df_topic_change_20_15w.csv')\n",
    "topic_change_data.columns = ['Decline', 'Topic_change', 'Topic_before', 'Topic_after']\n",
    "\n",
    "df_reactions_topics = pd.merge(df_reactions, topic_change_data, left_index=True, right_on='Decline', how='left')\n",
    "df_reactions_topics = df_reactions_topics.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very small variation, not statistically significant \\\n",
    "BUT maybe different topic transitions have different correlations with the recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of different topic changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions_changed_topic = df_reactions_topics[df_reactions_topics['Topic_change'] == True]\n",
    "df_reactions_changed_topic = df_reactions_changed_topic.drop(columns=['Decline', 'Topic_change'])\n",
    "df_reactions_changed_topic = df_reactions_changed_topic.dropna()\n",
    "df_reactions_changed_topic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the LLM-generated topic themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reactions_changed_topic = map_topics_to_llm_themes('data/LLM_topics.json', df_reactions_changed_topic)\n",
    "df_reactions_changed_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_transitions = filter_topic_transitions(df_reactions_changed_topic)\n",
    "topic_transitions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barplot_topics_plotly(topic_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sankey_diagram(topic_transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range for the color bar\n",
    "x, y = 0.31, 0.57  # Replace with your desired range\n",
    "\n",
    "# Normalize the color bar values\n",
    "norm = Normalize(vmin=x, vmax=y)\n",
    "\n",
    "# Create a colormap object using the coolwarm palette\n",
    "cmap = cm.coolwarm\n",
    "\n",
    "# Create a ScalarMappable to map normalized values to colors\n",
    "sm = ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])  # Required to set the array for ScalarMappable\n",
    "\n",
    "# Create the color bar\n",
    "plt.figure(figsize=(0.5, 6))\n",
    "cbar = plt.colorbar(sm, cax=plt.gca(), orientation='vertical')  # Vertical color bar\n",
    "cbar.set_label('Recovery Rate', fontsize=12)\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
